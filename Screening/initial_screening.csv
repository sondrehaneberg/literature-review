ID,Title,DOI,Abstract,Keywords,Origin
2-s2.0-105007166117,"12th International Conference on HCI in Business, Government and Organizations, held as part of the 27th HCI International Conference, HCII 2025",,"The proceedings contain 41 papers. The special focus in this conference is on HCI in Business, Government and Organizations. The topics include: Apology or Gratitude: The Impact of Verbal Recovery Strategies of AI-Powered Virtual Tourism Assistants on Tourists’ Post-Recovery Satisfaction; enhancing the Omnichannel Retailing Customer Experience in the Pre-purchase Phase: Evaluation and Improvement of a Digital Grocery Brochure; investigating Users’ Responses to Blurred Boundary Advertisements in Short Video Websites; RoBuddy – An Innovative Research Project on AI in Office Environments; a Method of Assembly Guidance Information Delivery in Augmented Reality Considering Users’ Proficiency Levels; Once More with (the Right) Feeling: How Historical Fiction Writing Processes of Character Design, Plot Outline, and Context Checking Are Affected by Co-Writing with ChatGPT; Using CNN Models to Predict the Future Trends of Listed Stocks on the Taiwan Stock Exchange; Integrating AI-Driven Personas and Procedural Visualization for Complete Communities Design and Urban Planning for Large-Scale Urban Development; Generative AI and Changing Work: Systematic Review of Practitioner-Led Work Transformations Through the Lens of Job Crafting; Follow My Logic: Generative AI Workflows in Designing for Serious Table-Top Games; AI Integration in ERP Systems: Optimizing Knowledge Management and Business Process Re-engineering for Strategic Outcomes; rethink the Way of Conducting Research: It is Time to Change to the Artificial Intelligence Era for Reliable Outcomes; augmenting Student Startups’ Customer Validation Efforts Through Adaptive Coaching Using Large Language Models; skilled Labor Shortage and Artificial Intelligence: Challenges and Opportunities for the Regional Labor Market; LLM-Assisted Collaborative Change Specification of Industrial Control Software; ChatGPT and Financial Investing: The Advantages, the Disadvantages, and the Perils. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-105013137151,"14th Symposium on Languages, Applications and Technologies, SLATE 2025",,"The proceedings contain 13 papers. The topics discussed include: from prediction to precision: leveraging LLMs for equitable and data-driven writing placement in developmental education; a DSL for swarm intelligence algorithms; elements for weighted answer-set programming; beyond the score: exploring the intersection between sociodemographics and linguistic features in English (L1) writing placement; a chatbot to help promoting financial literacy; an architecture for composite combinatorial optimization solvers; semantic representation of adverbs in the lexicalized meaning representation (LMR) framework; stepwise source, a supporting tool for source code demonstration; bridging language barriers: a comparative review and empirical evaluation of source-to-source transpilers; and mining GitHub software repositories to look for programming language cocktails. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-105009402610,"18th International Conference on Information Technology and Applications, ICITA 2024",,"The proceedings contain 59 papers. The special focus in this conference is on Information Technology and Applications. The topics include: Comparison of Machine Learning Models for Early Prediction of Diabetes with LIME Interpretability; pattern Recognition in Disaster Response: Leveraging Machine Learning for Twitter Analysis; TTL: Transformer and Transfer Learning Approach to Detect Sunflower Disease; The Moderating Role of Risk Aversion in an Extended UTAUT Cryptocurrency Adoption Model; NLP on Text Messages Using Sentimentality Investigation; audio-Visual Features-Based Framework for Advertisement Detection from Sports Videos; Context-Aware Medical Question-Answering: An Extended Transformers-Based Approach with BioBERT Encoding for Restricted Domain Queries; Enhancing Seabass Detection in Aquaculture: A Step Toward Automated Behavioral Analysis Using AI; information Technology at the University of Turin: A Disruptive Method for Exams with the Safe Exam Browser; usability Assessment of Virtual Reality Applications to Support the Care of Older Adults: A Scoping Review; predicting Commodity Prices in Futures Market Using Machine Learning; A Hybrid Approach to Music Recommendations for Improving ADHD Productivity; hyper-chaotic Nonlinear Artificial Hummingbird Algorithm; an Efficient Voice Replay Antispoofing Method; deep Spatiotemporal Network-Based Spontaneous Macro- and Micro-facial Expression Recognition; Face Sketch Image Generation from Facial Attributes Using StyleGAN2; building a Usability and Accessibility Evaluation Method for Small Software Development Companies; Forex Price Prediction: A Multi-model Approach Integrating Sentiment Analysis Using LLMs with LSTM, XGBoost, Transformer Models; predicting New Zealand’s Stock Market Trends: Combining Sentiment Analysis and Deep Learning; netFlow-Based Network Intrusion Prevention System Using Machine Learning; data-Driven Demand Forecasting in Fast Fashion Using Integrated Deep Learning Models; early Detection of Cardiovascular Diseases. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85190369483,"2023 IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises, WETICE 2023",,"The proceedings contain 36 papers. The topics discussed include: an approach for software development effort estimation using ChatGPT; bitcoin transactions types and their impact on storage scalability; security-based multipath route switching protocol for quality-of-service enhancement in VANETs using Wiedemann car-following model; performance evaluation of container management tasks in OS-level Virtualization Platforms; Virtual Collaborative Assembly System Based on Unity; DeepChain: a deep learning and blockchain based framework for detecting risky transactions on hie system; a hybrid-DLT based trustworthy AI framework; a smart mining strategy for blockchain-enabled cyber-physical systems; and collisions-resistant hash function based on a logistics map. © 2024 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85216976361,"2024 6th International Conference on Machine Learning, Big Data and Business Intelligence, MLBDBI 2024",,"The proceedings contain 57 papers. The topics discussed include: research on aspect-level emotion categorization based on hybrid attention mechanism; research on financial transaction data protection and intelligent risk assessment based on differential privacy; fine-tuned large language model for autonomous vehicles accident report; research on optimization strategy of production line in aviation manufacturing enterprises based on machine learning; research on factor interaction effects and nonlinear relationships in quantitative models; exploring emotion recognition in children with autism spectrum disorder using ChatGPT; and predicting user purchase behavior on JD.com: a sequential interaction and engagement depth model. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85201236297,"2024 7th International Conference on Artificial Intelligence and Big Data, ICAIBD 2024",,"The proceedings contain 98 papers. The topics discussed include: patient clustering to improve process mining for disease trajectory analysis using Indonesia health insurance dataset; a comprehensive review of transformer-based models: ChatGPT and bard in focus; pneumonia image classification: deep learning and machine learning fusion; exploring the pathways to optimize immersive imaging experiences using AIGC technology; sentiment analysis of song dynasty classical poetry using fine-tuned large language models: a study with LLMs; a random forest stock prediction model based on Bayesian optimization; multi-level generative pretrained transformer for improving malware detection performance; and research on building a competency model for managers in private express enterprises based on recruitment big data. © 2024 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-105004809261,"2025 IEEE Symposium on Computational Intelligence for Financial Engineering and Economics, CiFer 2025",,"The proceedings contain 11 papers. The topics discussed include: novel financial network models using neuro correlations and applications; the superiority of direct neuro volatility forecasts over GARCH and machine learning forecasts for financial assets; innovative pattern extraction and synthetic high-frequency data generation in European carbon emission markets using GAN networks; robust European call option pricing via linear regression; simulating illiquid markets: insights from fractional ownership trading and agent-based models; enhancing forecasting with a 2D time series approach for cohort-based data; stock prediction by signal decomposition-driven multivariate feature extractor and executor-based mixture of experts; a deep ensemble learning approach for imbalanced data in bankruptcy prediction; and leveraging large language models and retrieval-augmented generation for enhanced multi-asset portfolio construction. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85212494941,"26th International Conference on Information Integration and Web Intelligence, iiWAS 2024",,"The proceedings contain 52 papers. The special focus in this conference is on Information Integration and Web Intelligence. The topics include: Financial News Classification Using Language Learning Models and Reinforcement Learning; ExtractGPT: Exploring the Potential of Large Language Models for Product Attribute Value Extraction; Feature Extraction for Claim Check-Worthiness Prediction Tasks Using LLM; training Data for Dialogue Generation Considering Philosophies; Finding Adequate Additional Layer of Auxiliary Task in BERT-Based Multi-task Learning; ponzi Scheme Detection and Prevention in Blockchain Platforms Using Machine Learning: A Systematic Literature Review; Cross-Chain Personal Data Exchange on EVM Platforms: Enhancing Transparency, and Equity; Incentivize Peer Review Without Rewarding: Using OSS-Like Citation Pull Request; towards Website X-Ray for Europe’s Municipalities: Unveiling Digital Transformation with Multimodal Embeddings; evolving Applications of Conversational Agents in Healthcare: A Literature Review; hybrid Edge-Cloud Federated Learning: The Case of Lightweight Smoking Detection; anonymization of Unstructured Health Data in Spanish; when Good Enough is the Best Option: Use of Digital Sufficiency to Fight Climate Change; multi-target Feature Selection Method for Predicting User-Level Psychological Status from Text; FIEAP: A Machine Learning Approach for Fair and Interpretable Employee Attrition Prediction; HOCON34k: A Corpus of Hate Speech in Online Comments from German Newspapers; SISIS: Sequence Indexing for SImilarity Search; top-k on Sequences: A New Approach to Enhanced Similarity Search; exploratory Data Analysis of Time Series Using Pre-segmented Clustering; a Data Science Approach for Predicting Soccer Passes Using Positional Data; a Method for Integrating Heterogeneous Data into a Knowledge Graph; predicting Knowledge Graph Updates from Edit Histories; Automatic Extraction of RML-star Mappings from Property Graphs; Exploring the Role of UML in Data Modelling for NoSQL Databases: Position Paper; railway Systems’ Ontologies: A Literature Review and an Alignment Proposal; Combining GraphSAGE and Label Propagation for Node Classification in Graphs. © 2024 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-105007848472,"7th International Conference on Adaptive Instructional Systems, AIS 2025, held as part of the 27th HCI International Conference, HCII 2025",,"The proceedings contain 40 papers. The special focus in this conference is on Adaptive Instructional Systems. The topics include: Evaluating Adaptive Training for Nautical Rules of the Road; from Standardization to Personalization: Leveraging Learner Profiles to Tailor Education; an Adaptive Simulated Startup Financial Modeling Mentor Using a Large Language Model to Address Shortages in Skilled Advisors: Architecture and Design Considerations; integrating Adaptive Interventions into Learning Engineering Workflows; leveraging Deterministic Algorithms to Personalize Education and Enhance Student Success: The Story of an Engineered Learning Experience; curriculum Sequencing as a Generalised Travelling Salesperson Problem: A Novel Perspective on Learning Path Generation; evaluation of Difficulty-Based Adaptive Training Strategies on Simulator Flight Training Performance; designing an Adaptive Mobile Application for Learning Programming Among Computing Students from Marginalized Backgrounds; architecture for a Large Scale Learning Ecosystem; automated Response Generation Using Language Models: An Approach to Enhancing User Interaction; Metacognition in HCI: Designing Systems for Planning and Flexibility; stress and Performance: Understanding the Effect of Appraisal and Coping on Performance in Complex Tasks; exploring the Potential of Large Language Models for Estimating the Reading Comprehension Question Difficulty; macrocognition in Simulation Based Training: A Practical Application of Learning Engineering in a Complex Training Environment; public Safety Personnel Readiness Prediction: A Hybrid Model of Neurophysiological and Psychometric Data; understanding the Complexity of Music Improvisation: Leveraging Cognitive Models to Inform Adaptive Instruction Design; advancing Cognitive State Monitoring: Diagnosing Cognitive Control States Under Varying Automation Reliability Level; the Weaponization of Critical Thinking: Understanding the Dynamics of Human Memory. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85176944442,"8th China Conference on Knowledge Graph and Semantic Computing, CCKS 2023",,"The proceedings contain 28 papers. The special focus in this conference is on Knowledge Graph and Semantic Computing. The topics include: A Generalized Strategy of Chinese Grammatical Error Diagnosis Based on Task Decomposition and Transformation; conversational Search Based on Utterance-Mask-Passage Post-training; financial Fraud Detection Based on Deep Learning: Towards Large-Scale Pre-training Transformer Models; GERNS: A Graph Embedding with Repeat-Free Neighborhood Structure for Subgraph Matching Optimization; feature Enhanced Structured Reasoning for Question Answering; conditional Knowledge Graph: Design, Dataset and a Preliminary Model; ODKG: An Official Document Knowledge Graph for the Effective Management; CCD-ASQP: A Chinese Cross-Domain Aspect Sentiment Quadruple Prediction Dataset; move Structure Recognition in Scientific Papers with Saliency Attribution; causE: Towards Causal Knowledge Graph Embedding; Moral Essential Elements: MEE-A Dataset for Moral Judgement; improving Adaptive Knowledge Graph Construction via Large Language Models with Multiple Views; single Source Path-Based Graph Neural Network for Inductive Knowledge Graph Reasoning; a Graph Learning Based Method for Inductive Knowledge Graph Relation Prediction; LLM-Based SPARQL Generation with Selected Schema from Large Scale Knowledge Base; Robust NL-to-Cypher Translation for KBQA: Harnessing Large Language Model with Chain of Prompts; in-Context Learning for Knowledge Base Question Answering for Unmanned Systems Based on Large Language Models; a Military Domain Knowledge-Based Question Answering Method Based on Large Language Model Enhancement; Advanced PromptCBLUE Performance: A Novel Approach Leveraging Large Language Models; exploring the Logical Expressiveness of Graph Neural Networks by Establishing a Connection with C<inf>2</inf> ; research on Joint Representation Learning Methods for Entity Neighborhood Information and Description Information; harvesting Event Schemas from Large Language Models; NTDA: Noise-Tolerant Data Augmentation for Document-Level Event Argument Extraction; Event-Centric Opinion Mining via In-Context Learning with ChatGPT; relation Repository Based Adaptive Clustering for Open Relation Extraction. © 2023 Elsevier B.V., All rights reserved.",,Scopus
10.1109/ISCMI63661.2024.10851487,A Comparative Study of Sequential Deep Learning Models in Financial Time Series Forecasting,10.1109/iscmi63661.2024.10851487,"Time series analysis of daily stock prices is challenging due to the inherent complexity, nonlinearity, and nonstationarity of financial data. In this paper, we compare three sequential deep learning models - LSTM, Transformer, and Large Language Models (LLMs) - for stock price prediction. By transforming the regression problem of predicting daily log returns into a classification task, we evaluate the models' classification accuracies, with the Transformer achieving the highest accuracy of 22%, followed by LSTM (15.6%) and LLM (15.3%). Regression metrics showed LSTM initially performing better, with a lower RMSE (180.92) than LLM (1739.61). However, outlier predictions in the LLM, caused by incomplete number outputs, inflated its error. After removing these outliers, LLM's RMSE improved significantly to 33.85, surpassing LSTM. These results demonstrate the potential of Transformer and LLM models for financial time series prediction. Future work will explore incorporating self-reflection mechanisms in LLM predictions and extending the comparison to multivariate financial time series incorporating textual data and other features. © 2025 Elsevier B.V., All rights reserved.",Deep Learning; Finance; Large Language Model; Survey; Time Series Forecasting; Decentralized Finance; Deep Learning; Prediction Models; Time Series; Comparatives Studies; Financial Time Series Forecasting; Inherent Complexity; Language Model; Large Language Model; Learning Models; Stock Price; Time Series Forecasting; Time-series Analysis; Contrastive Learning,Scopus
10.32473/flairs.v35i.130668,A Comparison of House Price Classification with Structured and Unstructured Text Data,10.32473/flairs.v35i.130668,"Purchasing a home is one of the largest investments most people make. House price prediction allows individuals to be informed about their asset wealth. Transparent pricing on homes allows for a more efficient market and economy. We report the performance of machine learning models trained with structured tabular representations and unstructured text descriptions. We collected a dataset of 200 descriptions of houses which include meta-information, as well as text descriptions. We test logistic regression and multi-layer perceptron (MLP) classifiers on dividing these houses into binary buckets based on fixed price thresholds. We present an exploration into strategies to represent unstructured text descriptions of houses as inputs for machine learning models. This includes a comparison of term frequency-inverse document frequency (TF-IDF), bag-of-words (BoW), and zero-shot inference with large language models. We find the best predictive performance with TF-IDF representations of house descriptions. Readers will gain an understanding of how to use machine learning models optimized with structured and unstructured text data to predict house prices. © 2023 Elsevier B.V., All rights reserved.",Economics; Houses; Text Processing; House's Prices; Machine Learning Models; Meta Information; Performance; Price Prediction; Structured Text; Tabular Representations; Term Frequencyinverse Document Frequency (tf-idf); Text Data; Unstructured Texts; Machine Learning,Scopus
10.18653/v1/2023.findings-emnlp.490,A Comprehensive Evaluation of Large Language Models on Legal Judgment Prediction,10.18653/v1/2023.findings-emnlp.490,"Large language models (LLMs) have demonstrated great potential for domain-specific applications, such as the law domain. However, recent disputes over GPT-4's law evaluation raise questions concerning their performance in real-world legal tasks. To systematically investigate their competency in the law, we design practical baseline solutions based on LLMs and test on the task of legal judgment prediction. In our solutions, LLMs can work alone to answer open questions or coordinate with an information retrieval (IR) system to learn from similar cases or solve simplified multi-choice questions. We show that similar cases and multi-choice options, namely label candidates, included in prompts can help LLMs recall domain knowledge that is critical for expertise legal reasoning. We additionally present an intriguing paradox wherein an IR system surpasses the performance of LLM+IR due to limited gains acquired by weaker LLMs from powerful IR systems. In such cases, the role of LLMs becomes redundant. Our evaluation pipeline can be easily extended into other tasks to facilitate evaluations in other domains. Code is available at https://github.com/srhthu/LM-CompEval-Legal. © 2025 Elsevier B.V., All rights reserved.",Computational Linguistics; Domain Knowledge; Comprehensive Evaluation; Domain-specific Application; Information-retrieval Systems; Language Model; Law Evaluation; Legal Judgements; Multi Choices; Performance; Real-world; Similar Case; Search Engines,Scopus
10.3390/jrfm18020099,A First Look at Financial Data Analysis Using ChatGPT-4o,10.3390/jrfm18020099,"OpenAI’s new flagship model, ChatGPT-4o, released on 13 May 2024, offers enhanced natural language understanding and more coherent responses. This paper investigates ChatGPT-4o’s capabilities in financial data analysis, including zero-shot prompting, time series analysis, risk and return analysis, and ARMA-GARCH estimation. ChatGPT-4o’s performance is generally comparable to traditional statistical software like Stata, though some errors and discrepancies arise due to differences in implementation. Despite these issues, our findings indicate that ChatGPT-4o has significant potential for real-world financial analysis. Integrating ChatGPT-4o into financial research and practice may lead to more efficient data processing, improved analytical capabilities, and better-informed investment decisions. © 2025 Elsevier B.V., All rights reserved.",Academia; Artificial Intelligence (ai); Chatgpt; Data Analysis; Finance Research; Financial Analysis; Generative Ai (genai); Large Language Models; Stock Return,Scopus
10.1109/ACCESS.2024.3404862,A Framework for LLM-Assisted Smart Policing System,10.1109/access.2024.3404862,"In the face of rapidly increasing crime rates, the evolving complexity of crime data processing, and public safety challenges, the need for more advanced policing solutions has increased leading to the emergence of smart policing systems and predictive policing techniques. This urgency and shift toward smart policing incorporates artificial intelligence (AI), with a specific focus on machine learning (ML) as an essential tool for data analysis, pattern recognition, and proactive crime forecasting. Among these, the flexibility and power of AI techniques including large language models (LLMs), as a subset of generative AI, have increased the interest in applying them in real-world applications, such as financial, medical, legal, and agricultural applications. However, the abilities and possibilities of adopting LLMs in applications including crime prediction remain unexplored. This paper focuses on bridging this gap by developing a framework based on the transformative potential of BART, GPT-3, and GPT-4, three state-of-the-art LLMs, in the domain of smart policing, specifically, crime prediction. As a prototype, diverse methods such as zero-shot prompting, few-shot prompting, and fine-tuning are used to comprehensively assess the performance of these models in crime prediction based on state-of-the-art datasets from two major cities: San Francisco and Los Angeles. The main objective is to illuminate the adaptability of LLMs and their capacity to revolutionize crime analysis practices. Additionally, a comparative analysis of the aforementioned methods on the GPT series model and BART with ML techniques is provided which shows that the GPT models are more suitable than the traditional ML models for crime classification in most experimental scenarios. © 2024 Elsevier B.V., All rights reserved.",Crime Prediction; Few-shot Prompting; Fine-tuning; Large Language Models; Llm; Zero-shot Prompting; Computational Linguistics; Crime; Data Handling; Job Analysis; Learning Systems; Pattern Recognition; Recurrent Neural Networks; Adaptation Models; Crime Prediction; Few-shot Prompting; Fine Tuning; Language Model; Large Language Model; Predictive Models; Task Analysis; Zero-shot Prompting; Forecasting,Scopus
10.1145/3637528.3671440,A Tutorial on Multi-Armed Bandit Applications for Large Language Models,10.1145/3637528.3671440,"This tutorial offers a comprehensive guide on using multi-armed bandit (MAB) algorithms to improve Large Language Models (LLMs). As Natural Language Processing (NLP) tasks grow, efficient and adaptive language generation systems are increasingly needed. MAB algorithms, which balance exploration and exploitation under uncertainty, are promising for enhancing LLMs. The tutorial covers foundational MAB concepts, including the exploration-exploitation trade-off and strategies like epsilon-greedy, UCB (Upper Confidence Bound), and Thompson Sampling. It then explores integrating MAB with LLMs, focusing on designing architectures that treat text generation options as arms in a bandit problem. Practical aspects like reward design, exploration policies, and scalability are discussed. Real-world case studies demonstrate the benefits of MAB-augmented LLMs in content recommendation, dialogue generation, and personalized content creation, showing how these techniques improve relevance, diversity, and user engagement. © 2025 Elsevier B.V., All rights reserved.",Large Language Models; Multi-armed Bandit; Contrastive Learning; Speech Enhancement; Adaptive Language Generation System; Balance Exploration; Exploration And Exploitation; Exploration/exploitation; Language Model; Language Processing; Large Language Model; Multiarmed Bandits (mabs); Natural Languages; Uncertainty; Natural Language Processing Systems,Scopus
10.1038/s41598-024-56740-9,A comparison of human and GPT-4 use of probabilistic phrases in a coordination game,10.1038/s41598-024-56740-9,"English speakers use probabilistic phrases such as likely to communicate information about the probability or likelihood of events. Communication is successful to the extent that the listener grasps what the speaker means to convey and, if communication is successful, individuals can potentially coordinate their actions based on shared knowledge about uncertainty. We first assessed human ability to estimate the probability and the ambiguity (imprecision) of twenty-three probabilistic phrases in a coordination game in two different contexts, investment advice and medical advice. We then had GPT-4 (OpenAI), a Large Language Model, complete the same tasks as the human participants. We found that GPT-4’s estimates of probability both in the Investment and Medical Contexts were as close or closer to that of the human participants as the human participants’ estimates were to one another. However, further analyses of residuals disclosed small but significant differences between human and GPT-4 performance. Human probability estimates were compressed relative to those of GPT-4. Estimates of probability for both the human participants and GPT-4 were little affected by context. We propose that evaluation methods based on coordination games provide a systematic way to assess what GPT-4 and similar programs can and cannot do. © 2024 Elsevier B.V., All rights reserved.",Ambiguity; Gpt-4; Llm; Pragmatics; Probabilistic Phrases; Probability; Human; Interpersonal Communication; Investment; Knowledge; Language; Probability; Communication; Humans; Investments; Knowledge; Language; Probability,Scopus
10.1016/j.ejor.2024.09.011,A fused large language model for predicting startup success,10.1016/j.ejor.2024.09.011,"Investors are continuously seeking profitable investment opportunities in startups and, hence, for effective decision-making, need to predict a startup's probability of success. Nowadays, investors can use not only various fundamental information about a startup (e.g., the age of the startup, the number of founders, and the business sector) but also textual description of a startup's innovation and business model, which is widely available through online venture capital (VC) platforms such as Crunchbase. To support the decision-making of investors, we develop a machine learning approach with the aim of locating successful startups on VC platforms. Specifically, we develop, train, and evaluate a tailored, fused large language model to predict startup success. Thereby, we assess to what extent self-descriptions on VC platforms are predictive of startup success. Using 20,172 online profiles from Crunchbase, we find that our fused large language model can predict startup success, with textual self-descriptions being responsible for a significant part of the predictive power. Our work provides a decision support tool for investors to find profitable investment opportunities. © 2025 Elsevier B.V., All rights reserved.",Deep Learning; Large Language Models; Machine Learning; Text Mining; Venture Capital; Decentralized Finance; Business Sector; Decisions Makings; Deep Learning; Investment Opportunities; Language Model; Large Language Model; Machine-learning; Probability Of Success; Text-mining; Venture Capital; Adversarial Machine Learning,Scopus
10.1016/j.artmed.2025.103203,A generalized LLMs framework to support public health financing through probabilistic predictions and uncertainty quantification,10.1016/j.artmed.2025.103203,"As a systemic problem, public health cannot be addressed without considering other policy dimensions. Hence, a holistic approach across public policy areas is necessary to incorporate Health-for-All values into decision-making. However, such multisectoral interventions require public budgets that are effectively mapped into public health outcomes and indicators of their wider determinants. This budget-tagging procedure is high-cost, given that it is often done manually by domain experts. In this paper, we propose Categorical Perplexity-based Uncertainty Quantification (CPUQ), a novel, cost-effective Large Language Models (LLMs) framework that can be leveraged by policymakers to build budget-to-indicator and indicator-to-indicator mappings. This model-agnostic method employs categorical-style prompts to generate interpretable Bernoulli and categorical distributions for edges in a Text-attributed Graph, which is associated with the descriptions of the budget items and indicators. The prompting strategy proposed provides a novel way to incorporate models’ uncertainty within the final outputs, enhancing accuracy and safety, We find that the budget-to-indicator mapping predicted by the framework aligns effectively with expert annotations, while when prompted to infer indicator-to-indicator networks, CPUQ estimates more nuanced relationships compared to alternative LLMs-based methods. Through our work, we hope to provide valuable insights into the strengths and weaknesses of leveraging LLMs to support public health budget planning, with the aim of promoting the implementation of the Health-for-All agenda across diverse governments and institutions. © 2025 Elsevier B.V., All rights reserved.",Health-for-all; Large Language Models; Prompt Engineering; Public Health Financing; Uncertainty Quantification; Wider Determinants Of Health; Budget Control; Decision Making; Public Health; Public Policy; Uncertainty Analysis; Determinants Of Healths; Health-for-all; Language Model; Large Language Model; Modelling Framework; Probabilistic Prediction; Prompt Engineering; Public Health Financing; Uncertainty Quantifications; Wide Determinant Of Health; Cost Effectiveness; Article; Artificial Intelligence; Budget; Conceptual Framework; Decision Making; Financial Management; Government; Health Outcome; Human; Large Language Model; Methodology; Policy; Prediction; Probability; Prompt Engineering; Public Health; Uncertainty; Validation Study; Cost Benefit Analysis; Economics; Budgets; Cost-benefit Analysis; Humans; Public Health; Uncertainty,Scopus
WOS:001225271800001,"A memory bank of the future: Stiegler, education and the gesture of care",10.1080/00131857.2024.2354463,"In contemporary societies, the processes of transindividuation by which knowledges are transformed into cycles and rhythms of metastability have been dramatically short-circuited. In turn, this has provoked the spiritual misery and pseudo-fabulations so prevalent all around us, including our educational contexts. For Stiegler, this is nothing short of a noetic reticulation that deprives us from ways of thinking ourselves beyond or outside of our digital experience. But digitality has not only intensified the commodification of knowledges (savoirs), it has also rendered even knowledge production automated, recursive and probabilistic, the uncritical implementation of ChatGPT being a prime example. What this means is that knowledge and knowledge production have been subsumed under the rubric of recursive optimization for predictive performance. To understand this transformation, I discuss the implications of the widespread use of Bayesian statistics in machine learning. My argument is that we need to develop new speculative tools aimed at what is known as priors in Bayesian models, which is to say the probability of an occurrence before the collection of new data. What this means for education is that we need to address not only the effects of automation, but also the very conditions that give rise to these.",Bayesian statistics; care; instauration; Stiegler; automated knowledge production; recursive optimization,WoS
WOS:001421563000001,A novel content-based approach to measuring monetary policy uncertainty using fine-tuned LLMs,10.1016/j.frl.2025.106832,"Policy uncertainty is a potential source for reducing policy effectiveness. Existing studies have measured policy uncertainty by tracking the frequency of specific keywords in newspaper articles. However, this keyword-based approach fails to account for the context of articles and differentiate the types of uncertainty that such contexts indicate. This study introduces a new method for measuring different types of policy uncertainty in news content using large language models (LLMs). We fine-tune the LLMs to identify different types of uncertainty expressed in newspaper articles based on their context, even if they do not contain specific keywords indicating uncertainty. By applying this method to Japan's monetary policy from 2015 to 2016, we demonstrate that our approach successfully captures the dynamics of monetary policy uncertainty, which vary significantly depending on the type of uncertainty examined.",Bank of Japan; Central bank communication; Generative pre-trained transformer; Large language model; Monetary policy; Policy uncertainty; Text data,WoS
10.1063/5.0274031,A survey on the application and research progress of large language models in financial forecasting,10.1063/5.0274031,"Large language models (LLMs) are reshaping the technical paradigms of financial forecasting through their robust representation learning and reasoning capabilities. This paper systematically reviews the application pathways of architectures such as transformers and graph neural networks in scenarios like stock prediction and risk management, highlighting key technologies for enhancing prediction accuracy through knowledge injection and temporal modeling improvements. The study reveals that LLMs demonstrate significant advantages in unstructured data processing and cross-market correlation analysis but face challenges related to economic logic interpretability and data non-stationarity. Future research should focus on advancing causal reasoning augmentation and federated learning collaboration to achieve secure and trustworthy evolution of financial forecasting systems. © 2025 Elsevier B.V., All rights reserved.",Data Accuracy; Data Handling; Finance; Forecasting; Prediction Models; Risk Perception; Financial Forecasting; Graph Neural Networks; Injection Modeling; Key Technologies; Language Model; Learning Capabilities; Prediction Accuracy; Reasoning Capabilities; Risks Management; Stock Predictions; Risk Management,Scopus
10.1007/978-3-031-55536-7,AI and Chatbots in FinTech: Revolutionizing Digital Experiences and Predictive Analytics,10.1007/978-3-031-55536-7,"This book is a comprehensive guide to the use of Artificial Intelligence (AI) in the Financial Technology (FinTech) industry. It is comprised of ten chapters, each addressing a specific aspect of AI in FinTech. The reader is introduced to AI in FinTech, including its history and current state and the role of chatbots in FinTech and how they are used to improve customer service. Furthermore, the book explores the business framework of AI-based ChatGPT in FinTech, including the technology behind ChatGPT and how it can be applied to various financial sectors. The book examines the use of predictive analytics and machine learning in FinTech, highlighting how these tools are used to predict customer behavior and improve decision-making. The author delves into how ChatGPT is used to determine buying behavior and discusses the use of machine learning to reshape the digital experience in FinTech. Additionally, the book provides best practices for retaining customers in FinTech, including how to use AI to create personalized experiences that keep customers coming back, and explores the different applications of predictive models in FinTech, including how they are used to improve risk management and fraud detection. Lastly, the book discusses the use of ChatGPT for stock price prediction and the detection of financial fraud and examines the role of ChatGPT in the world of cryptocurrency, including how it can be used to make informed investment decisions. Overall, this book provides a comprehensive overview of the different ways AI is being used in FinTech and the potential it holds for improving customer experiences and driving innovation in the financial industry. © 2024 Elsevier B.V., All rights reserved.",Ai; Blockchain; Business Framework; Capital Markets; Chatgpt; Cryptocurrencies; Financial Services; Fintech; Machine Learning; Predictive Models; Stock Price,Scopus
10.1145/3677052.3698694,AI in Investment Analysis: LLMs for Equity Stock Ratings,10.1145/3677052.3698694,"Investment Analysis is a cornerstone of the Financial Services industry. The rapid integration of advanced machine learning techniques, particularly Large Language Models (LLMs), offers opportunities to enhance the equity stock rating process. This paper explores the application of LLMs to predict stock performance and generate stock ratings by ingesting diverse datasets. Traditional stock rating methods rely heavily on the expertise of financial analysts, and face several challenges such as data overload, inconsistencies in filings, and delayed reactions to market events. Our study addresses these issues by leveraging LLMs to improve the accuracy and consistency of stock ratings. Additionally, we assess the efficacy of using different data modalities with LLMs for the financial domain. We utilize varied datasets comprising fundamental financial, market, and news data from January 2022 to June 2024, along with GPT-4-32k (v0613) (with a training cutoff in Sep. 2021 to prevent information leakage). Our results show that our benchmark method outperforms traditional stock rating methods when assessed by forward returns. Specifically, incorporating financial fundamentals enhances ratings accuracy. While integrating news data improves short-term performance, substituting detailed news summaries with sentiment scores reduces token use without loss of performance. In many cases, omitting news data entirely enhances performance by reducing bias. Our research shows that LLMs can be leveraged to effectively utilize large amounts of multimodal financial data, as showcased by their effectiveness at the stock rating prediction task. Our work provides a reproducible framework for generating consistent and accurate stock ratings, offering a cost-effective and efficient alternative to traditional methods. Future work will extend the analysis to longer time horizons, incorporating more diverse data, and utilizing newer models to enhance detailed investment analysis and reports. © 2025 Elsevier B.V., All rights reserved.",Cost Effectiveness; Data Accuracy; Financial Markets; Data Overload; Financial Analysts; Financial Domains; Financial News; Financial Services Industries; Investment Analysis; Language Model; Machine Learning Techniques; Performance; Stock Performance,Scopus
10.1145/3707649,AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy,10.1145/3707649,"Large language models (LLMs) match and sometimes exceed human performance in many domains. This study explores the potential of LLMs to augment human judgment in a forecasting task. We evaluate the effect on human forecasters of two LLM assistants: one designed to provide high-quality (""superforecasting"") advice, and the other designed to be overconfident and base-rate neglecting, thus providing noisy forecasting advice. We compare participants using these assistants to a control group that received a less advanced model that did not provide numerical predictions or engage in explicit discussion of predictions. Participants (N 991) answered a set of six forecasting questions and had the option to consult their assigned LLM assistant throughout. Our preregistered analyses show that interacting with each of our frontier LLM assistants significantly enhances prediction accuracy by between 24% and 28% compared to the control group. Exploratory analyses showed a pronounced outlier effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 41%, compared with 29% for the noisy assistant. We further examine whether LLM forecasting augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-The-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our data do not consistently support these hypotheses. Our results suggest that access to a frontier LLM assistant, even a noisy one, can be a helpful decision aid in cognitively demanding tasks compared to a less powerful model that does not provide specific forecasting advice. However, the effects of outliers suggest that further research into the robustness of this pattern is needed. © 2025 Elsevier B.V., All rights reserved.",Ai Augmentation; Forecasting; Human-ai Interaction; Large Language Models; Digital Elevation Model; Ai Augmentation; Base Rate; Control Groups; Forecasting Accuracy; High Quality; Human Judgments; Human Performance; Human-ai Interaction; Language Model; Large Language Model; Prediction Models,Scopus
10.1016/j.frl.2025.106967,AT-FinGPT: Financial risk prediction via an audio-text large language model,10.1016/j.frl.2025.106967,"Financial risk prediction is crucial for investment decision-making. Traditional machine learning methods are limited by their structures and parameter size, which hinders their generalizability and effectiveness. Large language models (LLMs), which are pretrained with very large dataset and many GPUs have recently shown promising improvements in financial risk prediction. Despite this progress, most existing financial LLMs mainly rely on textual data for training and prediction, overlooking audio data and limiting analysis to text summarization. However, natural language processing studies have shown that audio from CEOs’ quarterly earnings calls is crucial for financial risk prediction. In this work, we introduce an audio–text LLM named AT-FinGPT, which fuses financial audio data and summarization texts for financial risk prediction. The empirical experimental results show that AT-FinGPT is superior to most advanced methods. Through an ablation study, we demonstrate that different data sources can facilitate financial risk assessment and discuss the effectiveness of each part in the AT-FinGPT model. © 2025 Elsevier B.V., All rights reserved.",Fingpt; Large Language Model; Multi-sources Data Fusion; Quantitative Finance,Scopus
WOS:001540458000194,AdaRAG: Adaptive Optimization for Retrieval Augmented Generation with Multilevel Retrievers at the Edge,10.1109/infocom55648.2025.11044685,"Considering privacy concerns and real-time demands of popular large language models (LLMs), a shift towards edge-based LLM inference leverages edge clusters in proximity to provide low latency and secure responsiveness. To enhance the generation quality of LLMs, retrieval-augmented generation (RAG) can seamlessly integrate relevant external knowledge from local databases into LLMs without dedicated fine-tuning. However, this retrieval process can significantly contribute to overall latency, particularly in resource-constrained edge environments. To address this challenge, we introduce AdaRAG, tailored for edge-based RAG, leveraging multilevel (i.e., light and heavy) retrievers to facilitate adaptive retrieval granularity and efficient pipeline parallelism for retrieval and inference processes by fully exploiting heterogeneous edge resources (i.e., CPU and GPU). AdaRAG adaptively manages the heavy retrieval proportion and selected documents in augmented prompts, aiming to balance the long-term trade-off between overall generation quality and latency for dynamic user queries. Due to the inherent randomness of probabilistic LLM inference and highly dynamic queries at the edge, the underlying relations between the above decisions and performance feedback (i.e., end-to-end latency and accuracy) are difficult to obtain accurately a priori. Thus, we adopt bandit convex optimization to design a lightweight online algorithm, which utilizes real-time performance feedback to estimate the gradient information and optimize the retrieval and prompt decisions on the fly. Our rigorous theoretical analysis and extensive evaluations show our AdaRAG's superior performance. These promising results can boost the adoption of AdaRAG in future edge-based LLM applications.",,WoS
WOS:001545134000011,Adaptive Probabilistic Operational Testing for Large Language Models Evaluation,10.1109/ast66626.2025.00017,"Large Language Models (LLM) empower many modern software systems, and are required to be highly accurate and reliable. Evaluating LLM poses challenges due to the high costs of manual labeling and of validation of labeled data. This study investigates the suitability of probabilistic operational testing for effective and efficient evaluation of LLM, focusing on a case study with DistilBERT. To this aim, we adopt an existing framework (DeepSample) for Deep Neural Network (DNN) testing and adapt it to the LLM domain by introducing auxiliary variables tailored to LLM and classification tasks. Through a comprehensive evaluation, we demonstrate how sampling-based operational testing can yield reliable LLM accuracy estimates and effectively expose failures, or, under testing budget constraints, it can find a trade off between accuracy estimation and failure exposure. The experimental results, using DistilBERT on three sentiment analysis datasets, show that sampling-based methods can provide cost effective and reliable operational accuracy assessment for LLM. These findings offer practical insights for testers and help address critical gaps in current LLM evaluation practices.",Software testing; Large Language Models; Sampling; LLM evaluation,WoS
10.1145/3677052.3698681,Adaptive and Explainable Margin Trading via Large Language Models on Portfolio Management,10.1145/3677052.3698681,"Recent strategies for portfolio management often lack flexibility to adjust funds between long and short positions throughout trading periods. This prevents adapting portfolios to the market, which mitigates risks and seizes opportunities. To address these gaps, we propose an adaptive and explainable framework that integrates Large Language Models (LLMs) with Reinforcement Learning (RL) for dynamic long-short position adjustment in response to evolving market conditions. This approach leverages the recent advancements in LLMs for processing unstructured data and their capacity for explainable reasoning. The framework includes two stages: an Explainable Market Forecasting/Reasoning Pipeline, and a Position Reallocation stage. The Market Forecasting/Reasoning Pipeline allows various LLMs to learn market trends from diverse external data sources and determine optimal adjustment ratios with a clear reasoning path. The Portfolio Reallocation stage interacts with the sequential trading process from a pre-trained RL model to enhance decision-making and transparency. Our framework is flexible to accommodate various external data sources from microeconomics to macroeconomics data, diverse data types including time series and news text, along with multiple LLMs. Experiments demonstrate that our framework effectively achieves three times the return and doubles the Sharpe ratio compared to benchmarks. All the data and code are publicly available under NJIT FinTech Lab's GitHub1. © 2025 Elsevier B.V., All rights reserved.",Explainable Ai; Large Language Model; Market Trend Forecasting; Portfolio Management; Reinforcement Learning; Decision Making; Financial Data Processing; Financial Markets; Fintech; Investments; Marketplaces; Pipeline Codes; Reinforcement Learning; Explainable Ai; Language Model; Large Language Model; Market Forecasting; Market Trend Forecasting; Market Trends; Portfolio Managements; Reinforcement Learnings; Short Position; Trend Forecasting,Scopus
10.1007/978-3-031-84371-6_3,Advancing Interpretability in Sequential Models Through Generative AI Rationalization Using GPT-4,10.1007/978-3-031-84371-6_3,"In this study, we investigate the role of Generative Pre-trained Transformer 4 (GPT-4) in enhancing interpretability of sequential predictions in Natural Language Processing (NLP). Our study introduces a hybrid model that integrates traditional sequential prediction models with GPT-4, aiming to generate detailed, context-sensitive explanations for model outputs. This approach is rooted in the use of advanced transformer architectures and a specialized tokenization method that maintains semantic coherence, allowing for deep contextual analysis by GPT-4. Additionally, we devise a rationale generation algorithm that achieves a balance between succinctness and informativeness. Our experimental validation spans across various high-dimensional datasets, including financial time-series and multilingual texts, employing both qualitative and quantitative metrics to evaluate the model’s performance. These metrics focus on the plausibility and consistency of the rationales, as well as the model’s predictive accuracy. Preliminary results demonstrate that our approach not only enhances the accuracy of sequential predictions but also significantly improves their interpretability. This finding highlights the potential of generative AI to bridge the gap between complex AI decision-making processes. This research underscores the viability of employing generative AI to elucidate the underlying mechanisms of sequential prediction models, paving the way for more transparent AI systems. © 2025 Elsevier B.V., All rights reserved.",Explainable Ai (xai); Generative Ai; Sequential Predictions; Explainable Ai (xai); Generative Ai; Hybrid Model; Interpretability; Language Processing; Natural Languages; Prediction Modelling; Rationalisation; Sequential Modeling; Sequential Prediction; Natural Language Processing Systems,Scopus
10.1109/I2CT61223.2024.10543356,Agents are All you need: Elevating Trading Dynamics with Advanced Generative AI-Driven Conversational LLM Agents and Tools,10.1109/i2ct61223.2024.10543356,"This paper presents the development of a groundbreaking LLM multi-agent system designed to optimize the Energy Exchange (EX)'s electricity trading. The system integrates cutting-edge, Generative AI, embedding-based deep learning models and LLM Agents to forecast electricity prices with heightened accuracy and facilitate interactive reporting. Our first agent performs advanced deep learning , tapping into IEX's rich databases for day-ahead and intraday market prices, alongside additional data streams such as weather and economic indicators. We eschew traditional predictive models in favor of sophisticated embedding-based models adept at discerning complex temporal patterns, enabling precise forecasts up to seven days ahead. Rigorous validation methods, including k-fold cross-validation, are applied, with accuracy gauged by metrics like Root Mean Squared Error (RMSE). The second agent is founded on a robust GenAI tools framework, translating intricate model predictions into intelligible reports and extract insights through another LLM based Agents. This interface adeptly handles energy market specifics, ensuring contextually relevant interactions. This tool's integration aims to enhance decision-making for market participants and to inject unprecedented predictive transparency into market dynamics. Our initiative heralds a transformative step toward realizing a data-centric, efficient, and customer-focused energy market in India, with potential expansion throughout the South Asian region powered by LLM and generative AI. © 2024 Elsevier B.V., All rights reserved.",Agents Tools; Energy Price Prediction; Forecasting; Generative Ai; Gpt; Large Language Model; Costs; Decision Making; Embeddings; Long Short-term Memory; Mean Square Error; Multi Agent Systems; Power Markets; Agent Tool; Day-ahead; Energy Price Prediction; Energy Prices; Generative Ai; Gpt; Language Model; Large Language Model; Price Prediction; Forecasting,Scopus
10.1016/j.jer.2023.09.019,An Artificial Intelligence based automated case-based reasoning (CBR) system for severity investigation and root-cause analysis of road accidents – Comparative analysis with the predictions of ChatGPT,10.1016/j.jer.2023.09.019,"Road accidents have been progressively causing havoc in our society and certain preventive measures must be taken to reduce or possibly eliminate road accidents. The derivative of a road accident ranges from a mild injury to casualty. This research work mainly focusses on developing a novel case-based reasoning system to investigate and troubleshoot the cause of road accidents on a war-foot basis. First, the dominant attributes contributing to the cause of road accidents are identified and finalized as 28. A unique road accident dataset is developed which comprises of 1028 data collected from web resources, popular news magazines and extended further to large scale database of one-million cases by biased random number simulation. Each attribute is given a severity weightage of 1,2 and 3 for computing the net weighted score for a case in the database. Also, non-weighted scores are computed by introduction of a primary number dataset to maintain the uniqueness of the score which is further used for similarity analytics. Now, an accident news is randomly selected, and Rapid automatic keyword extraction (RAKE) schema is used as Natural language processor (NLP) for extracting the dominant keywords from the news articles. The extracted keywords are compared and further mapped into a factor-matrix comprising 28 attributes causing road accidents. Further, similarity analytics is performed to evaluate the severity scores and comparison of new cases. The system demonstrated high retrieval accuracy with all road accident cases collected from real world scenarios. This research has great prospects on troubleshooting road accident cases effectively and provides instant promising troubleshooting measures to prevent such accidents in the future. Also, the proposed framework might be useful for intelligent decision-making systems and automated driving systems. Based on the final outlook, a comprehensive framework for national road safety could be developed and passed as a valid law for implementation. Finally, the forecasted results of the proposed algorithm are compared with the predictions of Chat GPT program. © 2024 Elsevier B.V., All rights reserved.",Accident Analysis; Artificial Intelligence; Case-based Reasoning; Chat Gpt; Natural Language Processing; Root Cause Analysis,Scopus
10.1109/ICSCNA58489.2023.10370253,An Imperial Analysis of Large Language Models for Automated Tweet Sentiment Prediction,10.1109/icscna58489.2023.10370253,"Company' s brand perception majorly depends on customer experience and the reviews which follow that. A customer is capable of influencing many more people just on the basis of reviews given by him/her. Google released Pathways Language Model (PaLM) which is a major advancement in Artificial Intelligence (AI). It has been trained with the Pathways System, which allows it to generalize tasks in various domains. In this work, a trustworthy platform is provided for the examination of millions of people's continually moving and changing perspectives. Twitter data is captured, and effective sentiment and data analysis is used to generate trustworthy and helpful info graphics reflecting public opinion. Product sales, stock returns, election outcomes, and other commercial and social events may all be predicted and explained using the information found in tweets. Brands, product manufacturers, and other companies may utilise the information derived from data analysis to better understand their brand image, expand their market share by targeting the relevant demographics at the right moments, and enhance their offerings in terms of both quality and customer service. © 2024 Elsevier B.V., All rights reserved.",Attention Units; Data Visualization; Masks; Sentiment Analysis; Tokenization; Competition; Data Handling; Data Visualization; Image Enhancement; Investments; Quality Control; Sales; Social Aspects; Attention Unit; Customer Experience; Google+; Language Model; Product Sales; Public Opinions; Sentiment Analysis; Social Events; Stock Returns; Tokenization,Scopus
10.3389/frai.2025.1609097,"An overview of model uncertainty and variability in LLM-based sentiment analysis: challenges, mitigation strategies, and the role of explainability",10.3389/frai.2025.1609097,"Large Language Models (LLMs) have significantly advanced sentiment analysis, yet their inherent uncertainty and variability pose critical challenges to achieving reliable and consistent outcomes. This paper systematically explores the Model Variability Problem (MVP) in LLM-based sentiment analysis, characterized by inconsistent sentiment classification, polarization, and uncertainty arising from stochastic inference mechanisms, prompt sensitivity, and biases in training data. We present illustrative examples and two case studies to highlight its impact and analyze the core causes of MVP, discussing a dozen fundamental reasons for model variability. We pay especial atenttion to explainabily, with an analysis of its importance in LLMs from the MVP perspective. In addition, we investigate key challenges and mitigation strategies, paying particular attention to the role of temperature as a driver of output randomness and highlighting the crucial role of explainability in improving transparency and user trust. By providing a structured perspective on stability, reproducibility, and trustworthiness, this study helps develop more reliable, explainable, and robust sentiment analysis models, facilitating their deployment in high-risk domains such as finance, healthcare and policy making, among others. © 2025 Elsevier B.V., All rights reserved.",Large Language Models; Llm-based Sentiment Analysis; Model Variability Problem; Sentiment Analysis; Uncertainty,Scopus
10.1001/jamasurg.2025.2154,Applying Large Language Models for Surgical Case Length Prediction,10.1001/jamasurg.2025.2154,"Importance: Accurate prediction of surgical case duration is critical for operating room (OR) management, as inefficient scheduling can lead to reduced patient and surgeon satisfaction while incurring considerable financial costs. Objective: To evaluate the feasibility and accuracy of large language models (LLMs) in predicting surgical case length using unstructured clinical data compared to existing estimation methods. Design, Setting, and Participants: This was a retrospective study analyzing elective surgical cases performed between January 2017 and December 2023 at a single academic medical center and affiliated community hospital ORs. Analysis included 125 493 eligible surgical cases, with 1950 used for LLM fine-tuning and 2500 for evaluation. An additional 500 cases from a community site were used for external validation. Cases were randomly sampled using strata to ensure representation across surgical specialties. Exposures: Eleven LLMs, including base models (GPT-4, GPT-3.5, Mistral, Llama-3, Phi-3) and 2 fine-tuned variants (GPT-4 fine-tuned, GPT-3.5 fine-tuned), were used to predict surgical case length based on clinical notes. Main Outcomes and Measures: The primary outcome was average error between predicted and actual surgical case length (wheels-in to wheels-out time). The secondary outcome was prediction accuracy, defined as predicted length within 20% of actual duration. Results: Fine-tuned GPT-4 achieved the best performance with a mean absolute error (MAE) of 47.64 minutes (95% CI, 45.71-49.56) and R2 of 0.61, matching the performance of current OR scheduling (MAE, 49.34 minutes; 95% CI, 47.60-51.09; R2, 0.63; P = .10). Both GPT-4 fine-tuned and GPT-3.5 fine-tuned significantly outperformed current scheduling methods in accuracy (46.12% and 46.08% vs 40.92%, respectively; P < .001). GPT-4 fine-tuned outperformed all other models during external validation with similar performance metrics (MAE, 48.66 minutes; 95% CI, 45.31-52.00; accuracy, 46.0%). Base models demonstrated variable performance, with GPT-4 showing the highest performance among non-fine-tuned models (MAE, 59.20 minutes; 95% CI, 56.88 - 61.52). Conclusion and Relevance: The findings in this study suggest that fine-tuned LLMs can predict surgical case length with accuracy comparable to or exceeding current institutional scheduling methods. This indicates potential for LLMs to enhance operating room efficiency through improved case length prediction using existing clinical documentation. This record is sourced from MEDLINE/PubMed, a database of the U.S. National Library of Medicine",Elective Surgery; Feasibility Study; Female; Human; Large Language Model; Male; Middle Aged; Operating Room; Operation Duration; Organization And Management; Retrospective Study; Elective Surgical Procedures; Feasibility Studies; Female; Humans; Large Language Models; Male; Middle Aged; Operating Rooms; Operative Time; Retrospective Studies,Scopus
10.3390/e27060550,"Artificial Intelligence Models for Predicting Stock Returns Using Fundamental, Technical, and Entropy-Based Strategies: A Semantic-Augmented Hybrid Approach",10.3390/e27060550,"This study examines the effectiveness of combining semantic intelligence drawn from large language models (LLMs) such as ChatGPT-4o with traditional machine-learning (ML) algorithms to develop predictive portfolio strategies for NASDAQ-100 stocks over the 2020–2025 period. Three different predictive frameworks––fundamental, technical, and entropy-based––are tested through examination of novel combinations of ML- and LLM-derived semantic metrics. The empirical results reveal a considerable divergence in optimal blending methods across the methodologies; namely, the technical methodology exhibits the best performance when using only ML predictions, with around 1978% cumulative returns with monthly rebalancing. In contrast, the fundamental methodology achieves its full potential when it is based primarily on LLM-derived semantic insights. The Entropy methodology is improved by a balanced combination of both semantic and ML signals, thus highlighting the potential of LLMs to improve predictive power by offering interpretative context for complex market interactions. These findings highlight the strategic importance of tailoring the semantic–algorithmic fusion to suit the nature of the predictive data and the investment horizon, with significant implications for portfolio management and future research in financial modeling. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence; Fundamental; Fuzzy Logic; Technical; Trading,Scopus
WOS:001343416300025,Assessing the Potentials of LLMs and GANs as State-of-the-Art Tabular Synthetic Data Generation Methods,10.1007/978-3-031-69651-0_25,"The abundance of tabular microdata constitutes a valuable resource for research, policymaking, and innovation. However, due to stringent privacy regulations, a significant portion of this data remains inaccessible. To address this, synthetic data generation methods have emerged as a promising solution. Here, we assess the potentials of two state-of-the-art GAN and LLM tabular synthetic data generators using different utility & risk measures and propose a robust risk estimation for individual records based on shared nearest neighbors. LLMs outperform CTGAN by generating synthetic data that more closely matches real data distributions, as evidenced by lower Wasserstein distances. LLMs also generally provide better predictive performance compared to CTGAN, with higher F-1 and R-2 scores. Interestingly, this does not necessarily mean that LLMs better capture correlations. Our proposed risk measure, Shared Neighbor Identifiability (SNI), proves effective in accurately assessing identification risk, offering a robust tool for navigating the risk-utility trade-off. Furthermore, we identify the challenges posed by mixed feature types in distance calculation. Ultimately, the choice between LLMs and GANs depends on factors such as data complexity, computational resources, and the desired level of model interpretability, emphasizing the importance of informed decision-making in selecting the appropriate generative model for specific applications.",Tabular Data; Synthetic Data Generation; LLM; GAN,WoS
WOS:001374817300001,Assessment of ChatGPT-3.5's Knowledge in Oncology: Comparative Study with ASCO-SEP Benchmarks,10.2196/50442,"Background: ChatGPT (Open AI) is a state-of-the-art large language model that uses artificial intelligence (AI) to address questions across diversetopics. TheAmerican Society of Clinical Oncology Self-Evaluation Program (ASCO-SEP) created a comprehensive educational program to help physicians keep up to date with the many rapid advances in the field. The question bank consists of multiple choice questions addressing the many facets of cancer care, including diagnosis, treatment, and supportive care. As ChatGPT applications rapidly expand, it becomesvital to ascertain if the knowledge of ChatGPT-3.5 matches the established standards that oncologists are recommended to follow. Objective: This studyaimstoevaluatewhetherChatGPT-3.5'sknowledgealigns with theestablished benchmarksthat oncologists are expected to adhere to. This will furnish us with a deeper understanding of the potential applications of this tool as a support forclinical decision-making. Methods: We conducted a systematic assessment of the performance of ChatGPT-3.5 on theASCO-SEP, the leading educational and assessment tool for medical oncologists in training and practice. Over 1000 multiple choice questions covering the spectrum of cancer care were extracted. Questions were categorized by cancer type or discipline, with subcategorization as treatment, diagnosis, or other. Answers were scored as correct if ChatGPT-3.5 selected the answer as defined by ASCO-SEP. Results: Overall, ChatGPT-3.5 achieved a score of 56.1% (583/1040) for the correct answers provided. The program demonstrated varying levels of accuracy across cancer types or disciplines. The highest accuracy was observed in questions related to developmental therapeutics (8/10; 80% correct), while the lowest accuracy was observed in questions related to gastrointestinal cancer (102/209; 48.8% correct). There was no significant difference in the program's performance across the predefined subcategoriesof diagnosis, treatment, and other (P=.16, which isgreaterthan .05). Conclusions:This study evaluated ChatGPT-3.5's oncology knowledge using the ASCO-SEP, aiming to address uncertainties regarding AI tools like ChatGPT in clinical decision-making. Our findings suggest that while ChatGPT-3.5 offers a hopeful outlook for AI in oncology, its present performance in ASCO-SEP tests necessitates further refinement to reach the requisite competency levels. Future assessments could explore ChatGPT's clinical decision support capabilities with real-world clinical scenarios, its ease of integration into medical workflows, and its potentialto foster interdisciplinary collaboration and patient engagement in health care settings.",artificial intelligence; ChatGPT-3.5; language model; medical oncology,WoS
10.1145/3652037.3652047,Assessment of the Applicability of Large Language Models for Quantitative Stock Price Prediction,10.1145/3652037.3652047,"In accordance with the findings presented in [34], this study examines the applicability of Machine Learning (ML) models and training strategies from the Natural Language Processing (NLP) domain in addressing time series problems, emphasizing the structural and operational aspects of these models and strategies. Recognizing the structural congruence within the data, we opt for Stock Price Prediction (SPP) as the designated domain to assess the transferability of NLP models and strategies. Building upon initial positive outcomes derived from quantitative SPP models in our ongoing research endeavors, we provide a rationale for exploring a range of additional methods and conducting subsequent research experiments. The outlined research aims to elucidate the efficacy of leveraging NLP models and techniques for addressing time series problems exemplified as SPP. © 2024 Elsevier B.V., All rights reserved.",Big Data; Large Language Models; Natural Language Processing; Quantitative Analysis; Stock Embeddings; Stock Price Prediction; Big Data; Computational Linguistics; Financial Markets; Forecasting; Natural Language Processing Systems; Embeddings; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Processing Model; Stock Embedding; Stock Price Prediction; Times Series; Time Series,Scopus
10.1145/3651655.3651658,Automated Smart Contract Vulnerability Detection using Fine-Tuned Large Language Models,10.1145/3651655.3651658,"As decentralized finance (DeFi) built on blockchain grows rapidly, the security of smart contracts underpinning DeFi has become a major concern due to exploits leading to billions in damages. Although tools exist for automated vulnerability detection in smart contracts, studies show that most vulnerabilities remain undetected. In this work, we propose using fine-Tuned large language models (LLMs) for enhanced automated detection of vulnerabilities in smart contracts. We collected over 26,727 labeled smart contract vulnerabilities and fine-Tuned the 13B parameter Llama-2 model. Evaluation of 1,000 unseen functions shows promising precision of 31-36% in predicting vulnerability categories. The fine-Tuned LLM demonstrates potential as an auxiliary tool to identify vulnerable code and assist auditors. Future work is outlined for improving performance via larger models, higher-quality data, and specialized binary detection models. We present promising preliminary results on integrating LLMs into smart contract analysis and motivate further research at the intersection of LLMs and blockchain security. © 2024 Elsevier B.V., All rights reserved.",Large Language Model; Security; Smart Contract; Vulnerability Detection; Automation; Blockchain; Computational Linguistics; Automated Detection; Block-chain; Decentralised; High Quality Data; Improving Performance; Language Model; Large Language Model; Large Models; Security; Vulnerability Detection; Smart Contract,Scopus
10.16097/j.cnki.1009-6744.2025.04.014,Autonomous Driving Decision-making Method Based on Cooperative Reinforcement Learning of Large Language Model; 大语言模型协同强化学习的自动驾驶决策方法,10.16097/j.cnki.1009-6744.2025.04.014,"Aiming at the problems that the high- level decision- making of the current autonomous driving system lacks specific execution details and continuous learning ability, this paper focuses on applying the Large Language Model (LLM) in refining the decision-making process of autonomous driving. Based on the powerful reasoning ability of the LLM and the exploration ability of Reinforcement Learning (RL), this paper proposes a method of combining the LLM and RL to refine the vehicle decision-making process. First, based on the high-level actions output of the RL, the reasoning ability of the LLM is used to predict the future trajectory points of the host vehicle. Then, the output of the RL model is combined with the current state information to make a safe, collision-free and interpretable prediction of the next state. At last, the above driving decision-making process is vectorized and stored in the memory module as driving experience, and the driving experience is updated regularly to achieve sustainable learning. The trajectory points predicted by the LLM provide a detailed motion path for the Proportional-Integral-Derivative (PID) controller, providing a basis for adjusting the vehicle's acceleration and speed to ensure that the vehicle travels along the predetermined path. In addition, the trajectory prediction can also evaluate and avoid potential collision risks, and create a safe path by analyzing the traffic state and historical data. The results of the closed-loop experiment show that the proposed decision-making method outperforms other models in all evaluation indicators. Compared to the RL, the decision-making method based solely on the LLM, and the LLM-based car-following model, the driving scores are increased by 35.12, 14.33 and 12.28 respectively. The method with the memory module increases the driving score by 25.59 compared to the method without the memory module. © 2025 Elsevier B.V., All rights reserved.",Autonomous Driving; Continual Learning; Intelligent Traffic; Large Language Model; Reinforcement Learning; Trajectory Prediction; Autonomous Vehicles; Computational Linguistics; Decision Making; Forecasting; High Level Languages; Trajectories; Two Term Control Systems; Autonomous Driving; Continual Learning; Decision-making Method; Decision-making Process; Intelligent Traffics; Language Model; Large Language Model; Memory Modules; Reinforcement Learnings; Trajectory Prediction; Reinforcement Learning,Scopus
10.1145/3677052.3698627,"Bankruptcy Prediction: Data Augmentation, LLMs and the Need for Auditor's Opinion",10.1145/3677052.3698627,"Predicting bankruptcy is crucial for managing financial risk in corporations. This study emphasizes incorporating the auditor's opinion text into prediction models to improve their ability to assess financial health. These opinions provide essential insights as they offer an independent assessment, complementing other predictive inputs like the management's discussion and analysis. However, the rarity of bankruptcy cases in the data introduces a challenging issue due to severe class imbalance. To address this, we propose a method to generate synthetic positive samples using a variational autoencoder and integrate the multi-source input in a late fusion setting. We showcase that both data augmentation and using multiple textual sources improve the performance of existing models on a related benchmark dataset. Additionally, we evaluate LLMs when used for data augmentation in the proposed method and in a zero-shot prediction setting, discussing important aspects to consider when incorporating them in a predictive pipeline. © 2025 Elsevier B.V., All rights reserved.",Auditor's Opinion; Bankruptcy Prediction; Data Augmentation; Llms; Nlp; Auditor's Opinions; Auto Encoders; Bankruptcy Prediction; Class Imbalance; Data Augmentation; Financial Health; Financial Risks; Independent Assessment; Llm; Prediction Modelling; Prediction Models,Scopus
10.1145/3626772.3657882,Beyond Accuracy: Investigating Error Types in GPT-4 Responses to USMLE Questions,10.1145/3626772.3657882,"GPT-4 demonstrates high accuracy in medical QA tasks, leading with an accuracy of 86.70%, followed by Med-PaLM 2 at 86.50%. However, around 14% of errors remain. Additionally, current works use GPT-4 to only predict the correct option without providing any explanation and thus do not provide any insight into the thinking process and reasoning used by GPT-4 or other LLMs. Therefore, we introduce a new domain-specific error taxonomy derived from collaboration with medical students. Our GPT-4 USMLE Error (G4UE) dataset comprises 4153 GPT-4 correct responses and 919 incorrect responses to the United States Medical Licensing Examination (USMLE) respectively. These responses are quite long (258 words on average), containing detailed explanations from GPT-4 justifying the selected option. We then launch a large-scale annotation study using the Potato annotation platform and recruit 44 medical experts through Prolific, a well-known crowdsourcing platform. We annotated 300 out of these 919 incorrect data points at a granular level for different classes and created a multi-label span to identify the reasons behind the error. In our annotated dataset, a substantial portion of GPT-4's incorrect responses is categorized as a ""Reasonable response by GPT-4,""by annotators. This sheds light on the challenge of discerning explanations that may lead to incorrect options, even among trained medical professionals. We also provide medical concepts and medical semantic predications extracted using the SemRep tool for every data point. We believe that it will aid in evaluating the ability of LLMs to answer complex medical questions. We make the resources available at https://github.com/roysoumya/usmle-gpt4-error-taxonomy. © 2025 Elsevier B.V., All rights reserved.",Gpt-4; Medical Qa; Multi-label Dataset; Usmle Error Taxonomy; Http; Semantics; Taxonomies; 'current; Datapoints; Error Taxonomy; Error Types; Gpt-4; High-accuracy; Medical Qa; Multi-label Dataset; Multi-labels; Usmle Error Taxonomy; Errors,Scopus
10.1109/CogSIMA61085.2024.10553755,Beyond Probabilities: Unveiling the Delicate Dance of Large Language Models (LLMs) and AI-Hallucination,10.1109/cogsima61085.2024.10553755,"Large language models (LLMs), like OpenAI’s ChatGPT and Google’s Gemini, operate as probabilistic models, leveraging their ability to generalise and discern intricate patterns within data. By assigning probabilities to different tokens based on patterns learned during extensive training on large datasets, these models can generate a wide range of contextually appropriate responses, spanning from textual scripts to auditory and visual outputs (both static and moving images). However, the inherent probabilistic nature of LLMs introduces a notable challenge, leading to the phenomenon known in the field of artificial intelligence as ‘AI-hallucination, ’ where the model may produce responses that sound plausible but are factually incorrect or nonsensical. Despite being perceived as a drawback, we posit in this paper that AI-hallucinations can be reframed as a distinctive feature of LLMs rather than a mere limitation. Our argument stems from the understanding that attempts to mitigate the harms caused by AI-hallucinations might inadvertently lead to increased model rigidity. This delicate balance between minimising harm and preserving the model’s flexibility is a central theme in our discussion. Furthermore, we revisit the concept of ‘context, ’ contending that a complete definition goes beyond the mere description of circumstances, environment, or surrounding facts. We assert that context is enriched by a conscious embodiment, involving the choice or refusal of action (considering all associate ethical implications) among a set of available options. © 2024 Elsevier B.V., All rights reserved.",Ai-hallucination; Artificial Intelligence (ai); Consciousness; Context; Gemini; Generalisation-hallucination Dilemma; Gpt; Large Language Models (llms); Artificial Intelligence; Computational Linguistics; Artificial Intelligence-hallucination; Consciousness; Context; Geminus; Generalisation; Generalization-hallucination Dilemma; Gpt; Language Model; Large Language Model; Large Datasets,Scopus
2-s2.0-105000518469,Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency,,"We present a novel statistical framework for analyzing stereotypes in large language models (LLMs) by systematically estimating the bias and variation in their generation. Current evaluation metrics in the alignment literature often overlook the randomness of stereotypes caused by the inconsistent generative behavior of LLMs. For example, this inconsistency can result in LLMs displaying contradictory stereotypes, including those related to gender or race, for identical professions across varied contexts. Neglecting such inconsistency could lead to misleading conclusions in alignment evaluations and hinder the accurate assessment of the risk of LLM applications perpetuating or amplifying social stereotypes and unfairness. This work proposes a Bias-Volatility Framework (BVF) that estimates the probability distribution function of LLM stereotypes. Specifically, since the stereotype distribution fully captures an LLM's generation variation, BVF enables the assessment of both the likelihood and extent to which its outputs are against vulnerable groups, thereby allowing for the quantification of the LLM's aggregated discrimination risk. Furthermore, we introduce a mathematical framework to decompose an LLM's aggregated discrimination risk into two components: bias risk and volatility risk, originating from the mean and variation of LLM's stereotype distribution, respectively. We apply BVF to assess 12 commonly adopted LLMs and compare their risk levels. Our findings reveal that: i) Bias risk is the primary cause of discrimination risk in LLMs; ii) Most LLMs exhibit significant pro-male stereotypes for nearly all careers; iii) Alignment with reinforcement learning from human feedback lowers discrimination by reducing bias, but increases volatility; iv) Discrimination risk in LLMs correlates with key sociol-economic factors like professional salaries. Finally, we emphasize that BVF can also be used to assess other dimensions of generation inconsistency's impact on LLM behavior beyond stereotypes, such as knowledge mastery. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.3390/nu17071176,Bridging Gaps in Cancer Care: Utilizing Large Language Models for Accessible Dietary Recommendations,10.3390/nu17071176,"Background/Objectives: Weight management is directly linked to cancer recurrence and survival, but unfortunately, nutritional oncology counseling is not typically covered by insurance, creating a disparity for patients without nutritional education and food access. Novel ways of imparting personalized nutrition advice are needed to address this issue. Large language models (LLMs) offer a promising path toward tailoring dietary advice to individual patients. This study aimed to assess the capacity of LLMs to offer personalized dietary advice to patients with breast cancer. Methods: Thirty-one prompt templates were designed to evaluate dietary recommendations generated by ChatGPT and Gemini with variations within eight categorical variables: cancer stage, comorbidity, location, culture, age, dietary guideline, budget, and store. Seven prompts were selected for four board-certified oncology dietitians to also respond to. Responses were evaluated based on nutritional content and qualitative observations. A quantitative comparison of the calories and macronutrients of the LLM- and dietitian-generated meal plans via the Acceptable Macronutrient Distribution Ranges and United States Department of Agriculture’s estimated calorie needs was performed. Conclusions: The LLMs generated personalized grocery lists and meal plans adapting to location, culture, and budget but not age, disease stage, comorbidities, or dietary guidelines. Gemini provided more comprehensive responses, including visuals and specific prices. While the dietitian-generated diets offered more adherent total daily calorie contents to the United States Department of Agriculture’s estimated calorie needs, ChatGPT and Gemini offered more adherent macronutrient ratios to the Acceptable Macronutrient Distribution Range. Overall, the meal plans were not significantly different between the LLMs and dietitians. LLMs can provide personalized dietary advice to cancer patients who may lack access to this care. Grocery lists and meal plans generated by LLMs are applicable to patients with variable food access, socioeconomic means, and cultural preferences and can be a tool to increase health equity. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence; Breast Cancer; Cancer; Diet; Weight Management; Adult; Article; Body Weight Gain; Breast Cancer; Calorie; Cancer Patient; Cancer Staging; Chatgpt; Cohort Analysis; Communication Barrier; Comorbidity; Controlled Study; Cultural Background; Diet Therapy; Dietary Intake; Dietitian; Female; Financial Stress; Government; Human; Insurance; Large Language Model; Macronutrient; Major Clinical Study; Male; Nutrition Education; Nutritional Counseling; Personalized Nutrition; Prevalence; Zero Shot Prompting; Breast Tumor; Counseling; Diet; Language; Nutrition Policy; United States; Breast Neoplasms; Counseling; Diet; Female; Humans; Language; Large Language Models; Nutrition Policy; Nutritionists,Scopus
10.22495/rgcv15i2p13,CAN CHATGPT PREDICT STOCK PRICES? EVALUATING ARTIFICIAL INTELLIGENCE-DRIVEN FINANCIAL FORECASTING AND RISK MANAGEMENT,10.22495/rgcv15i2p13,"The use of artificial intelligence (AI) in financial forecasting has become increasingly significant in finance and accounting, offering improved precision in predicting key financial indicators such as revenue and net income. The purpose of this study is to explore the relationship between AI models’ benchmark scores and their predictive accuracy, addressing a gap in the literature regarding comprehensive evaluations of AI performance across financial metrics. Recent research highlights AI’s potential to outperform traditional statistical methods, with deep learning and ensemble models demonstrating notable accuracy in predicting stock prices and financial ratios (Khattak et al., 2023; Cao, 2021). By analyzing the 2020–2022 financial records of ten publicly listed corporations this research implements zero-shot prompt approaches for forecasting 2023 revenue and net income. Research findings demonstrate AI models can effectively boost financial prediction accuracy and such accuracy remains essential for business choices and risk protocols. Practical steps for AI reliability enhancement focus on using top-quality data with transparency and methods to control algorithmic biases. The research is relevant because it adds to AI finance understanding in academia while generating practical applications that guide industry professionals toward future exploration of financial AI applications. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence; Benchmarking; Financial Forecasting; Net Income Prediction; Predictive Analytics; Revenue Prediction,Scopus
10.1093/rof/rfaf015,"CEO turnover, sequential disclosure, and stock returns",10.1093/rof/rfaf015,"We document that firms experience large negative stock returns during, and positive returns following, the first informational events after forced CEO turnovers. This V-shaped return pattern is driven by the strategic sequential disclosure of bad news and good news, aligned with incoming CEOs’ incentives to manage expectations. The pattern is more pronounced when these incentives are stronger, such as when firms earn higher stock returns and have higher valuation uncertainty leading up to the informational events. Evidence from firms’ earnings surprises, analysts’ forecast revisions, and large language model-based measures of disclosure behavior indicates that incoming CEOs often initially release bad news about realized and short-term earnings, projecting a broadly pessimistic outlook for the firm’s future performance, and subsequently disclose favorable news about longer-term earnings prospects. Our findings suggest that investors make the costly mistake of failing to discern the incentives behind managers’ disclosure. © 2025 Elsevier B.V., All rights reserved.",Ceo Turnover; Expectation Management; Stock Returns,Scopus
2-s2.0-85144347623,CEPOC: The Cambridge Exams Publishing Open Cloze dataset,,"Open cloze tests are a standard type of exercise where examinees must complete a text by filling in the gaps without any given options to choose from. This paper presents the Cambridge Exams Publishing Open Cloze (CEPOC) dataset, a collection of open cloze tests from world-renowned English language proficiency examinations. The tests in CEPOC have been expertly designed and validated using standard principles in language research and assessment. They are prepared for language learners at different proficiency levels and hence classified into different CEFR levels (A2, B1, B2, C1, C2). This resource can be a valuable testbed for various NLP tasks. We perform a complete set of experiments on three tasks: gap filling, gap prediction, and CEFR text classification. We implement transformer-based systems based on pre-trained language models to model each task and use our dataset as a test set, providing promising benchmark results. © 2022 Elsevier B.V., All rights reserved.",Blank-filling; Cambridge Examinations; Language Learning; Open Cloze; Second Language Testing; Classification (of Information); Natural Language Processing Systems; Statistical Tests; Text Processing; Blank-filling; Cambridge; Cambridge Examination; Filling In; Language Learning; Language Testing; Open Cloze; Second Language; Second Language Testing; Standard Type; Filling,Scopus
10.3778/j.issn.1673-9418.2406055,CFB：Financial Large Models Evaluation Methods; CFB：金融领域大模型评估方法,10.3778/j.issn.1673-9418.2406055,"As the potential applications of large language models (LLMs) in the financial sector continue to emerge, evaluating the performance of financial LLMs becomes increasingly important. However, current financial evaluation methods face limitations such as singular evaluation tasks, insufficient coverage of evaluation datasets, and contamination of benchmark data. Consequently, the potential of LLMs in the financial domain has not been fully explored. To address these issues, this paper proposes the Chinese financial benchmark (CFB) for evaluating financial LLMs. The CFB encompasses 36 datasets, covers 24 financial tasks, and involves 7 evaluation tasks: question answering, terminology explanation, text generation, text translation, classification task, voice recognition, and predictive decision. It also establishes corresponding benchmarks. The new approach of the CFB includes a broader range of tasks and data, the introduction of a benchmark decontamination method based on LLMs, and three evaluation methods: instruction fine- tuning, knowledge retrieval enhancement, and prompt engineering. The evaluation of 12 LLMs, including GPT- 4o, ChatGPT, and Gemini, reveals that though LLMs excel in information extraction and text analysis, they struggle with advanced reasoning and complex tasks. GPT- 4o performs exceptionally in information extraction and stock trading, whereas Gemini excels in text generation and prediction. Instruction fine- tuning improves LLMs’performance in text analysis but offers limited benefits for complex tasks. © 2024 Elsevier B.V., All rights reserved.",Evaluation Benchmark; Financial Large Models; Instruction Fine-tuning; Knowledge Retrieval Enhancement; Prompt Engineering,Scopus
10.1109/ICSME58944.2024.00024,CPLS: Optimizing the Assignment of LLM Queries,10.1109/icsme58944.2024.00024,"Large Language Models (LLMs) like ChatGPT have gained significant attention because of their impressive capabilities, leading to a dramatic increase in their integration into intelligent software engineering. However, their usage as a service with varying performance and price options presents a challenging trade-off between desired performance and the associated cost. To address this challenge, we propose CPLS, a framework that utilizes transfer learning and local search techniques for assigning intelligent software engineering jobs to LLM-based services. CPLS aims to minimize the total cost of LLM invocations while maximizing the overall accuracy. The framework first leverages knowledge from historical data across different projects to predict the probability of an LLM processing a query correctly. Then, CPLS incorporates problem-specific rules into a local search algorithm to effectively generate Pareto optimal solutions based on the predicted accuracy and cost. To evaluate the proposed approach, we conduct extensive experiments on LLM-based log parsing, a typical software maintenance task. Our experimental results demonstrate that CPLS outperforms the baseline methods, providing solutions with the highest accuracy in 14 out of 16 instances. Compared to the baselines, CPLS achieves an accuracy improvement ranging from 1.24% to 485.54%, or reduces costs by 15.21% to 89.09% while maintaining the highest accuracy achieved by the baselines. © 2025 Elsevier B.V., All rights reserved.",Cross-project Prediction; Large Language Models; Local Search; Log Parsing; Query Assignment; Computer Software Maintenance; Computer Software Selection And Evaluation; Cost Engineering; Cost Reduction; Data Accuracy; Query Languages; Query Processing; Search Engines; Transfer Learning; Cross-project Prediction; High-accuracy; Intelligent Software; Language Model; Large Language Model; Local Search; Log Parsing; Model-based Opc; Performance; Query Assignment; Structured Query Language,Scopus
10.1016/j.knee.2024.08.015,Can ChatGPT make surgical decisions with confidence similar to experienced knee surgeons?,10.1016/j.knee.2024.08.015,"Background: Unicompartmental knee replacements (UKRs) have become an increasingly attractive option for end-stage single-compartment knee osteoarthritis (OA). However, there remains controversy in patient selection. Natural language processing (NLP) is a form of artificial intelligence (AI). We aimed to determine whether general-purpose open-source natural language programs can make decisions regarding a patient's suitability for a total knee replacement (TKR) or a UKR and how confident AI NLP programs are in surgical decision making. Methods: We conducted a case-based cohort study using data from a separate study, where participants (73 surgeons and AI NLP programs) were presented with 32 fictitious clinical case scenarios that simulated patients with predominantly medial knee OA who would require surgery. Using the overall UKR/TKR judgments of the 73 experienced knee surgeons as the gold standard reference, we calculated the sensitivity, specificity, and positive predictive value of AI NLP programs to identify whether a patient should undergo UKR. Results: There was disagreement between the surgeons and ChatGPT in only five scenarios (15.6%). With the 73 surgeons’ decision as the gold standard, the sensitivity of ChatGPT in determining whether a patient should undergo UKR was 0.91 (95% confidence interval (CI): 0.71 to 0.98). The positive predictive value for ChatGPT was 0.87 (95% CI: 0.72 to 0.94). ChatGPT was more confident in its UKR decision making (surgeon mean confidence = 1.7, ChatGPT mean confidence = 2.4). Conclusions: It has been demonstrated that ChatGPT can make surgical decisions, and exceeded the confidence of experienced knee surgeons with substantial inter-rater agreement when deciding whether a patient was most appropriate for a UKR. © 2024 Elsevier B.V., All rights reserved.","Artificial Intelligence; Decision Making; Knee Arthroplasty; Natural Language Processing; Adult; Article; Calculation; Chatgpt; Clinical Article; Clinical Decision Making; Cohort Analysis; Confidence Interval; Controlled Study; Diagnostic Test Accuracy Study; Gold Standard; Human; Interrater Reliability; Knee Osteoarthritis; Medial Meniscus; Middle Aged; Natural Language Processing; Orthopedic Surgeon; Patient Selection; Predictive Value; Sensitivity And Specificity; Total Knee Arthroplasty; Artificial Intelligence; Clinical Competence; Female; Knee Replacement; Male; Procedures; Surgeon; Surgery; Arthroplasty, Replacement, Knee; Artificial Intelligence; Clinical Competence; Clinical Decision-making; Female; Humans; Male; Middle Aged; Natural Language Processing; Osteoarthritis, Knee; Patient Selection; Surgeons",Scopus
10.1016/j.frl.2024.105631,Can ChatGPT predict Chinese equity premiums?,10.1016/j.frl.2024.105631,"Leveraging over 1.86 million news headlines, we examine the capability of ChatGPT-3.5, a large language model (LLM), to predict equity risk premiums in the Chinese market. The predictive scores from ChatGPT not only positively and significantly forecast equity premiums but also markedly outperform the bag-of-words (BoW) method, demonstrating its superior capability to discern intricate market sentiments from extensive datasets. The consistent and reliable performance in both in-sample and out-of-sample tests underscores the effectiveness of ChatGPT and its potential to revolutionize financial forecasting. This study highlights the substantial value and innovative contribution of LLMs, such as ChatGPT, in enriching the precision and depth of financial market analysis. © 2024 Elsevier B.V., All rights reserved.",Bag-of-words; Chatgpt; Chinese Equity Premium; Large Language Model,Scopus
10.1080/20954816.2023.2276965,Can ChatGPT reduce human financial analysts’ optimistic biases?,10.1080/20954816.2023.2276965,"This paper examines the potential of ChatGPT, a large language model, as a financial advisor for listed firm performance forecasts. We focus on the constituent stocks of the China Securities Index 300 and compare ChatGPT’s forecasts for major financial performance measures with human analysts’ forecasts and the realised values. Our findings suggest that ChatGPT can correct the optimistic biases of human analysts. This study contributes to the literature by exploring the potential of ChatGPT as a financial advisor and demonstrating its role in reducing human biases in financial decision-making. © 2024 Elsevier B.V., All rights reserved.",Analyst Forecast; Chatgpt; Human–machine Interaction; Large Language Models; Optimistic Biases,Scopus
10.1080/00128775.2025.2534144,Can Large Language Models Forecast Time Series of Earnings per Share? Case from Poland,10.1080/00128775.2025.2534144,"This research evaluates the predictive accuracy of the cutting-edge LAG-LLaMA Large Language Model for earnings forecasts of Warsaw Stock Exchange-listed firms, comparing it with a seasonal random walk benchmark. The study uses two methods: zero-shot generalization, where the model leverages extensive pre-trained data, and fine-tuning, where historical EPS data specifically train the model. While the seasonal random walk yielded the lowest error rates, fine-tuning the LAG-LLaMA model produced comparable results in terms of MAAPE metric. The fine-tuned LAG-LLaMA model achieves the lowest RMSE and MAE errors but performs statistically equivalent to the simpler seasonal random walk model. This conclusion is specific to the Polish market and the studied period. The findings suggest LAG-LLaMA’s adaptability for longer datasets, while the simpler random walk remains effective for shorter timeframes, especially in emerging markets like Poland. © 2025 Elsevier B.V., All rights reserved.",Earnings Per Share; Lag-llama; Large Language Model; Large Language Model Meta Ai; Llama; Llm; Seasonal Random Walk; Warsaw Stock Exchange,Scopus
10.1007/s00521-024-10613-4,Can Large Language Models beat wall street? Evaluating GPT-4’s impact on financial decision-making with MarketSenseAI,10.1007/s00521-024-10613-4,"This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4’s advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability, and acceptance. Through empirical testing on the competitive S&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10–30% and achieving a cumulative return of up to 72% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies. © 2024 Elsevier B.V., All rights reserved.",Artificial Intelligence; Financial Markets; Gpt-4; Large Language Models; Marketsenseai; Stock Selection; Decentralized Finance; Decision Making; Financial Markets; Investments; Context Learning; Decisions Makings; Financial Decisions; Gpt-4; In Contexts; Language Model; Large Language Model; Marketsenseai; Stock Selections; Wall Streets; Acceptance Tests,Scopus
10.1016/j.ribaf.2025.102951,Can Large Language Models forecast carbon price movements? Evidence from Chinese carbon markets,10.1016/j.ribaf.2025.102951,"This paper investigates the impact of Large Language Models (LLMs) on forecasting Chinese carbon prices. We introduce a novel two-stage forecasting framework integrating a Time-Series Model (TSM) and Large Language Models. Initially, we use historical data on Chinese Emission Allowance prices to train the TSM for preliminary predictions. LLMs then refine these predictions, which process a sequence of past and corresponding future prices as a chain of thought. Additionally, we utilize the LLM to analyze and categorize the sentiment of news headlines, generating market sentiment labels that enhance the LLM's predictive accuracy. Our findings indicate that LLMs can improve TSM forecasts by 28–38 % across different regional markets. Furthermore, incorporating news sentiment labels into the LLM contributes an additional reduction in forecasting deviations, ranging from 3–4 %. © 2025 Elsevier B.V., All rights reserved.",Carbon Price Forecasting; Financial Sentiment Analysis; Large Language Models; Machine Learning,Scopus
10.1016/j.finmar.2025.101002,Can news predict firm bankruptcy?,10.1016/j.finmar.2025.101002,"We examine whether real-time business news predicts firm bankruptcy. Using full-text daily articles from the Dow Jones Newswires database, we generate firm-level predictors with ChatGPT and benchmark against FinBERT and dictionary-based models. ChatGPT-based variables outperform alternatives, with sentiment scores showing predictive power across horizons. Full-text news significantly enhance predictive accuracy over headlines. News-based measures add explanatory power beyond financial variables. Finally, we show that news captures timely information on macroeconomic conditions relevant to bankruptcy prediction, such as VIX, real GDP growth, and recession probability. © 2025 Elsevier B.V., All rights reserved.",Bankruptcy Prediction; Chatgpt; Generative Ai; News Data; Sentiment,Scopus
WOS:001098823200001,Capabilities of GPT-4 in ophthalmology: an analysis of model entropy and progress towards human-level medical question answering,10.1136/bjo-2023-324438,"Background Evidence on the performance of Generative Pre-trained Transformer 4 (GPT-4), a large language model (LLM), in the ophthalmology question-answering domain is needed. Methods We tested GPT-4 on two 260-question multiple choice question sets from the Basic and Clinical Science Course (BCSC) Self-Assessment Program and the OphthoQuestions question banks. We compared the accuracy of GPT-4 models with varying temperatures (creativity setting) and evaluated their responses in a subset of questions. We also compared the best-performing GPT-4 model to GPT-3.5 and to historical human performance. Results GPT-4- 0.3 (GPT-4 with a temperature of 0.3) achieved the highest accuracy among GPT-4 models, with 75.8% on the BCSC set and 70.0% on the OphthoQuestions set. The combined accuracy was 72.9%, which represents an 18.3% raw improvement in accuracy compared with GPT-3.5 (p<0.001). Human graders preferred responses from models with a temperature higher than 0 (more creative). Exam section, question difficulty and cognitive level were all predictive of GPT-4- 0.3 answer accuracy. GPT-4- 0.3' s performance was numerically superior to human performance on the BCSC (75.8% vs 73.3%) and OphthoQuestions (70.0% vs 63.0%), but the difference was not statistically significant (p=0.55 and p=0.09). Conclusion GPT-4, an LLM trained on non-ophthalmology-specific data, performs significantly better than its predecessor on simulated ophthalmology board-style exams. Remarkably, its performance tended to be superior to historical human performance, but that difference was not statistically significant in our study.",Medical Education,WoS
10.1007/978-3-031-72356-8_10,Carbon Price Forecasting with LLM-Based Refinement and Transfer-Learning,10.1007/978-3-031-72356-8_10,"We propose a unified forecasting framework for accurately predicting carbon markets of EU Emission Trading Scheme (EU ETS) and Chinese Emission Allowance (CEA). Our framework utilizes a Time-Series Model (TSM) for initial prediction followed by applying a Large Language Model (LLM) to refine the forecasts. We prompt the LLM to refine the TSM forecasts by demonstrating an example pair of past TSM predictions and their corresponding true future prices to the LLM as a chain-of-thought. The in-context learning capacity of the LLM allows the LLM to rectify inaccurate predictions to reflect on TSM predictions and refine the forecasts. To further reduce the prompting delays and expenses involving LLMs, we innovate a post-finetuning approach to train a Gated Linear Unit (GLU) model to condense the LLM’s in-context learning capability. This enables direct fine-tuning of TSM outputs without the need for explicit prompting LLM during inference. Experimental results show that our method can refine the TSM prediction by 10% to 40% in various zones, as well as enhance transfer learning by 10% to 21% through the inclusion of market context of the source zone when predicting the target zone. Remarkably, our GLU model achieves comparable, and in some cases superior, performance compared to LLM prompting. It effectively combines the short-term forecasting capability of classical Time Series Models with the long-term trend prediction ability typically associated with the LLMs. © 2024 Elsevier B.V., All rights reserved.",Carbon Future Market; Gated Linear Unit; Large Language Models; Post-finetuning; Price Forecasts; Time-series Prediction; Transfer Learning; Low Emission; Time Series; Transfer Learning; Carbon Future Market; Gated Linear Unit; Language Model; Large Language Model; Linear Units; Post-finetuning; Price Forecasts; Time Series Prediction; Times Series Models; Prediction Models,Scopus
10.1016/j.heliyon.2024.e31750,ChatGPT achieves comparable accuracy to specialist physicians in predicting the efficacy of high-flow oxygen therapy,10.1016/j.heliyon.2024.e31750,"Background: The failure of high-flow nasal cannula (HFNC) oxygen therapy can necessitate endotracheal intubation in patients, making timely prediction of the intubation risk following HFNC therapy crucial for reducing mortality due to delays in intubation. Objectives: To investigate the accuracy of ChatGPT in predicting the endotracheal intubation risk within 48 h following HFNC therapy and compare it with the predictive accuracy of specialist and non-specialist physicians. Methods: We conducted a prospective multicenter cohort study based on the data of 71 adult patients who received HFNC therapy. For each patient, their baseline data and physiological parameters after 6-h HFNC therapy were recorded to create a 6-alternative-forced-choice questionnaire that asked participants to predict the 48-h endotracheal intubation risk using scale options ranging from 1 to 6, with higher scores indicating a greater risk. GPT-3.5, GPT-4.0, respiratory and critical care specialist physicians and non-specialist physicians completed the same questionnaires (N = 71) respectively. We then determined the optimal diagnostic cutoff point, using the Youden index, for each predictor and 6-h ROX index, and compared their predictive performance using receiver operating characteristic (ROC) analysis. Results: The optimal diagnostic cutoff points were determined to be ≥ 4 for both GPT-4.0 and specialist physicians. GPT-4.0 demonstrated a precision of 76.1 %, with a specificity of 78.6 % (95%CI = 52.4–92.4 %) and sensitivity of 75.4 % (95%CI = 62.9–84.8 %). In comparison, the precision of specialist physicians was 80.3 %, with a specificity of 71.4 % (95%CI = 45.4–88.3 %) and sensitivity of 82.5 % (95%CI = 70.6–90.2 %). For GPT-3.5 and non-specialist physicians, the optimal diagnostic cutoff points were ≥5, with precisions of 73.2 % and 64.8 %, respectively. The area under the curve (AUC) in ROC analysis for GPT-4.0 was 0.821 (95%CI = 0.698–0.943), which was the highest among the predictors and significantly higher than that of non-specialist physicians [0.662 (95%CI = 0.518–0.805), P = 0.011]. Conclusion: GPT-4.0 achieves an accuracy level comparable to specialist physicians in predicting the 48-h endotracheal intubation risk following HFNC therapy, based on patient baseline data and physiological parameters after 6-h HFNC therapy. © 2024 Elsevier B.V., All rights reserved.",Artificial Intelligence; Chatgpt; High-flow Nasal Cannula Oxygen; Rox Index,Scopus
10.1002/fut.22568,ChatGPT and Commodity Return,10.1002/fut.22568,"This paper investigates the ability of a ChatGPT-based indicator to forecast excess returns of the commodity futures index. Using ChatGPT to extract information from over 2.5 million articles from nine international newspapers, we demonstrate that our constructed commodity news ratio index significantly predicts future commodity returns, both in-sample and out-of-sample. Furthermore, it outperforms traditional textual analysis methods, including Bidirectional Encoder Representations from Transformers (BERT) and Bag-of-Words (BoW), while indicating economic significance within an asset allocation framework. The results highlight the critical role of ChatGPT in forecasting commodity market dynamics and provide valuable insights for both financial market participants and researchers. © 2025 Elsevier B.V., All rights reserved.",Chatgpt; Commodity Return Analysis; Textual Analysis,Scopus
10.1007/s42488-025-00143-6,ChatGPT based credit rating and default forecasting,10.1007/s42488-025-00143-6,"Credit rating is a key element to reflect the level of credit risk in order to maintain the stability of financial markets. For enterprises, it supports the estimation of default risk to generate the credit rating and the associated funding costs. With the increase in credit risks over the past few years, for example, the falling of Silicon Valley Bank (SVB) and the Credit Suisse (CS), the demand for a more responsive and effective rating system is increasing. Although traditional models use structured data such as financial metrics, they are generally considered to have a time lag and incomprehensive, certain key elements that reflect the overall market environment, which might be generated from the unstructured data such as the changes in laws and policies as well as those that discussed in social media. Developing a more comprehensive and responsive credit rating system is a challenge to both practitioners and researchers. This study uses ChatGPT, a powerful LLM (Large Language Model), to collect a large set of multi-dimensional unstructured data and integrate such unstructured data with structured data to develop a comprehensive corporate rating model. Though the set of testing samples is small, the research results indicated that such an approach provides satisfactory accuracy and predictability. Even in the absence of some structured data from non-public financial reports, reasonable rating can be generated to provide the investors or business partners a reference, so this study shed the light to develop more suitable credit rating methodology for small and medium-sized enterprises, which were difficult to be rated with those traditional rating models. © 2025 Elsevier B.V., All rights reserved.",Chatgpt; Credit Rating; Default Forecast; Risk Management,Scopus
10.1016/j.procs.2024.08.258,ChatGPT-based Sentiment Analysis and Risk Prediction in the Bitcoin Market,10.1016/j.procs.2024.08.258,"The risk prediction of financial markets is of paramount importance, with investor sentiment playing a critical role. However, current research appears to be lacking in-depth exploration of this particular aspect within the Bitcoin market. This study aims to explore the impact of market participants' sentiment on risk prediction in the bitcoin market. We first applied ChatGPT to analyze the sentiment of crawled Bitcoin-related news headlines. Meanwhile, Monte Carlo simulation was employed to calculate value at risk (VaR). And we selected five conventional factors, including Bitcoin price, transaction volume, market share, hash rate, and average difficulty of mining. Finally, K-Nearest Neighbors (KNN) regression model was used to construct the model for predicting the risk of bitcoin market. We made a comparison between the accuracy outcomes when considering and not considering sentiment as factors. The results show that market participant's sentiment is significantly associated with market risk, and the inclusion of sentiment can significantly improve the accuracy of the risk prediction model. © 2024 Elsevier B.V., All rights reserved.",Bitcoin Market; Chatgpt; Investor Sentiment; Risk Prediction; Text Mining; 'current; Bitcoin Market; Chatgpt; Investor's Sentiments; Market Participants; Monte Carlo's Simulation; Risk Predictions; Sentiment Analysis; Text-mining; Value At Risk; Prediction Models,Scopus
WOS:001025183100001,"ChatGPT: Jack of all trades, master of none",10.1016/j.inffus.2023.101861,"OpenAI has released the Chat Generative Pre-trained Transformer (ChatGPT) and revolutionized the approach in artificial intelligence to human-model interaction. The first contact with the chatbot reveals its ability to provide detailed and precise answers in various areas. Several publications on ChatGPT evaluation test its effectiveness on well-known natural language processing (NLP) tasks. However, the existing studies are mostly non-automated and tested on a very limited scale. In this work, we examined ChatGPT's capabilities on 25 diverse analytical NLP tasks, most of them subjective even to humans, such as sentiment analysis, emotion recognition, offensiveness, and stance detection. In contrast, the other tasks require more objective reasoning like word sense disambiguation, linguistic acceptability, and question answering. We also evaluated GPT-4 model on five selected subsets of NLP tasks. We automated ChatGPT and GPT-4 prompting process and analyzed more than 49k responses. Our comparison of its results with available State-of-the-Art (SOTA) solutions showed that the average loss in quality of the ChatGPT model was about 25% for zero-shot and few -shot evaluation. For GPT-4 model, a loss for semantic tasks is significantly lower than for ChatGPT. We showed that the more difficult the task (lower SOTA performance), the higher the ChatGPT loss. It especially refers to pragmatic NLP problems like emotion recognition. We also tested the ability to personalize ChatGPT responses for selected subjective tasks via Random Contextual Few-Shot Personalization, and we obtained significantly better user-based predictions. Additional qualitative analysis revealed a ChatGPT bias, most likely due to the rules imposed on human trainers by OpenAI. Our results provide the basis for a fundamental discussion of whether the high quality of recent predictive NLP models can indicate a tool's usefulness to society and how the learning and validation procedures for such systems should be established.",ChatGPT; GPT-4; Natural language processing (NLP); Semantic NLP tasks; Pragmatic NLP tasks; Subjective NLP tasks; Natural language inference (NLI); Sentiment analysis; Offensive content; Emotion recognition; Humor detection; Stance detection; Word sense disambiguation (WSD); Question answering (QA); Model personalization; Text classification; SOTA analysis; Large language model; Prompting,WoS
2-s2.0-85216933378,Claim Check-Worthiness Detection: How Well do LLMs Grasp Annotation Guidelines?,,"The rising threat of disinformation underscores the need to fully or partially automate the fact-checking process. Identifying text segments requiring fact-checking is known as claim detection (CD) and claim check-worthiness detection (CW), the latter incorporating complex domain-specific criteria of worthiness and often framed as a ranking task. Zero- and few-shot LLM prompting is an attractive option for both tasks, as it bypasses the need for labeled datasets and allows verbalized claim and worthiness criteria to be directly used for prompting. We evaluate the LLMs’ predictive accuracy on five CD/CW datasets from diverse domains, using corresponding annotation guidelines in prompts. We examine two key aspects: (1) how to best distill factuality and worthiness criteria into a prompt, and (2) how much context to provide for each claim. To this end, we experiment with different levels of prompt verbosity and varying amounts of contextual information given to the model. We additionally evaluate the top-performing models with ranking metrics, resembling prioritization done by fact-checkers. Our results show that optimal prompt verbosity varies, meta-data alone adds more performance boost than co-text, and confidence scores can be directly used to produce reliable check-worthiness rankings. © 2025 Elsevier B.V., All rights reserved.",Labeled Data; Complex Domains; Contextual Information; Diverse Domains; Domain Specific; Labeled Dataset; Meta-data; Performance; Predictive Accuracy; Prioritization; Text Segments; Computational Linguistics,Scopus
10.1109/BigData62323.2024.10825449,Combining Financial Data and News Articles for Stock Price Movement Prediction Using Large Language Models,10.1109/bigdata62323.2024.10825449,"Predicting financial markets and stock price movements requires analyzing a company's performance, historic price movements, industry-specific events alongside the influence of human factors such as social media and press coverage. We assume that financial reports (such as income statements, balance sheets, and cash flow statements), historical price data, and recent news articles can collectively represent aforementioned factors.We combine financial data in tabular format with textual news articles and employ pre-trained Large Language Models (LLMs) to predict market movements. Recent research in LLMs has demonstrated that they are able to perform both tabular and text classification tasks, making them our primary model to classify the multi-modal data. We utilize retrieval augmentation techniques to retrieve and attach relevant chunks of news articles to financial metrics related to a company and prompt the LLMs in zero, two, and four-shot settings. Our dataset contains news articles collected from different sources, historic stock price, and financial report data for 20 companies with the highest trading volume across different industries in the stock market. We utilized recently released language models for our LLM-based classifier, including GPT- 3 and 4, and LLaMA- 2 and 3 models.We introduce an LLM-based classifier capable of performing classification tasks using combination of tabular (structured) and textual (unstructured) data. By using this model, we predicted the movement of a given stock's price in our dataset with a weighted F1-score of 58.5% and 59.1% and Matthews Correlation Coefficient of 0.175 for both 3-month and 6-month periods.The dataset and codes for this paper can be found on Github. https://github.com/aliielahi/FinedFMP 1 © 2025 Elsevier B.V., All rights reserved.",Financial Stock Price Movement Prediction; Information Retrieval; Large Language Models; Retrieval Augmented Generation; Commerce; Financial Data Processing; Financial Markets; Network Security; Prediction Models; Classification Tasks; Financial Data; Financial Reports; Financial Stock Price Movement Prediction; Language Model; Large Language Model; News Articles; Retrieval Augmented Generation; Stock Price; Stock Price Movement Predictions; Costs,Scopus
10.1007/978-3-031-96235-6_21,Comparative Analysis and Evaluation of SLMs and LLMs for Stock Price Movement Prediction,10.1007/978-3-031-96235-6_21,"This paper investigates the performance disparities between Small Language Models (SLMs) and Large Language Models (LLMs) in predicting stock price movements using data from two different datasets containing news articles and tweets. The study emphasizes the potential of SLMs as a more accessible and resource-efficient alternative to LLMs, enabling local and in-house deployment. Critical gaps are addressed, including the lack of direct price movement predictions, the utilization and comparison of State-of-the-Art (SotA) models, and the integration of diverse data sources. The research employed a fundamental trading strategy based on predicted stock price movement as the sole trading signal. The Phi-2 model, fine-tuned with Quantized Low-Rank Adaptation (QLoRA) on consumer-grade hardware, was compared with GPT-4, serving as a SotA benchmark. Performance was evaluated using accuracy, precision, recall, and F1-score. The results indicate that the fine-tuned SML (Phi-2) outperformed the LLM (GPT-4), albeit by a small margin, demonstrating the potential of a trained SML over a general LLM. © 2025 Elsevier B.V., All rights reserved.",Algorithmic Trading; Artificial Intelligence; Large Language Models; Small Language Models; Stock Price Prediction; Benchmarking; Commerce; Computation Theory; Costs; Electronic Trading; Financial Markets; Forecasting; Information Management; Information Systems; Information Use; Large Datasets; Prediction Models; Algorithmic Trading; Comparative Analyzes; Language Model; Large Language Model; Performance; Small Language Model; State Of The Art; Stock Price Movements; Stock Price Prediction; Artificial Intelligence,Scopus
10.1109/DSIT61374.2024.10881868,Comparative Analysis of LLM-based Market Prediction and Human Expertise with Sentiment Analysis and Machine Learning Integration,10.1109/dsit61374.2024.10881868,"This study conducts a comparative analysis of market prediction accuracy between Large Language Model (LLM)-based systems and human expertise within the financial analysis domain. Leveraging Quantum, an advanced LLM specialized for financial forecasting, we evaluate its predictive performance against human analysts and general-purpose LLMs, including GPT-3, GPT-4, FinGPT, and FinBERT. Employing a dataset of historical financial data, news headlines, and social media sentiment, we systematically assess predictive accuracy, response efficiency, and interpretability across models. The integration of sentiment analysis and machine learning further strengthens prediction reliability. Results reveal that Quantum's specialized model demonstrates superior accuracy and speed in financial forecasting compared to human predictions and generalized LLMs, particularly in fast-moving, data-rich contexts. Nevertheless, limitations in nuanced contextual understanding and adaptability persist, highlighting the enduring value of human expertise. This research reinforces the potential of LLMs as robust tools for financial decision-making while identifying key areas for refinement to enhance synergy with human analytical insights. https://chatgpt.com/g/g-bS4Q76v0I-quantum © 2025 Elsevier B.V., All rights reserved.",Financial Prediction; Large Language Models; Machine Learning; Market Forecasting; Quantum Ai; Sentiment Analysis; Adversarial Machine Learning; Contrastive Learning; Machine Learning; Predictive Analytics; Sentiment Analysis; Comparative Analyzes; Financial Prediction; Human Expertise; Language Model; Large Language Model; Machine-learning; Market Forecasting; Market Prediction; Quantum Ai; Prediction Models,Scopus
10.14569/IJACSA.2025.0160402,"Comparing Vision-Instruct LLMs, Vision-Based Deep Learning, and Numeric Models for Stock Movement Prediction",10.14569/ijacsa.2025.0160402,"This research conducts a comparative study of several stock movement prediction approaches, evaluating large language models (LLMs) and vision-based deep learning models with stock image as input, as well as models that utilize numerical data. Specifically, the study investigates a prompt-based LLM framework that processes candlestick charts, comparing its performance with image-based models such as MobileNetV2, Vision Transformer, and Convolutional Neural Network (CNN), as well as models with numerical inputs including Support Vector Machine (SVM), Random Forest, LSTM, and CNN-LSTM. Although LLMs have demonstrated promising results in stock prediction, directly applying them to stock images poses challenges compared to numerical approaches. To address this, this study further improves LLM performance with post-hoc calibration, reducing prediction biases. Experimental results demonstrate that post-hoc calibrated LLMs with visual input achieve competitive performance compared to other models, highlighting their potential as a viable alternative to traditional stock prediction methods while simplifying the prediction process. © 2025 Elsevier B.V., All rights reserved.",Convolutional Neural Network (cnn); Large Language Model (llm); Mobilenetv2; Stock Price Prediction; Time Series Forecasting; Vision Transformer; Cellular Neural Networks; Deep Learning; Financial Markets; Motion Estimation; Motion Tracking; Convolutional Neural Network; Language Model; Large Language Model; Learning Models; Mobilenetv2; Stock Price Prediction; Time Series Forecasting; Vision Based; Vision Transformer; Convolutional Neural Networks,Scopus
10.1109/EI264398.2024.10990385,Comprehensive Approaches to a Low-Carbon Transition of the Power Grid,10.1109/ei264398.2024.10990385,"As the largest source of carbon emissions, the power grid has garnered significant attention for its decarbonization from academia, government, and industry. This transition is complex due to the involvement of various stakeholders, including power generation, transmission networks, and active customers. The purpose of this paper is to present a framework aimed at facilitating the low-carbon transition, focusing on emission reduction strategies across four components of the power system. On the generation side, retiring coal-fired generators and deploying renewable energy sources coupled with energy storage systems can significantly lower overall system emissions. Correspondingly, on the consumer side, the electrification of transportation offers a viable alternative to internal combustion engine vehicles, thereby reducing societal emissions. Furthermore, in terms of system control, planning methodologies such as gas-electric coupling optimization can expedite emission reduction efforts. For market design, effective policies like carbon taxes and carbon emission allowance trading incentivize power companies to actively lower emissions. Besides the above, advanced AI-based prediction methods, such as generative adversarial networks (GANs) and large language models (LLMs), enhance the accuracy of forecasting uncertainty including electricity prices and loads, improving the efficiency of transition. These evolving policies and grid technologies will enable us to decarbonize the grid and better mitigate the impacts of climate change driven by electricity consumption. © 2025 Elsevier B.V., All rights reserved.",Decarbonization; Gas-electric Coupling Optimization; Generative Adversarial Networks (gans); Large Language Models (llms); Transportation Electrification; Electric Power Plant Loads; Electric Vehicles; Electrification; Emission Control; Global Warming; Smart Power Grids; Solar Fuels; Adversarial Networks; Coupling Optimization; Decarbonisation; Electric Coupling; Gas-electric Coupling Optimization; Generative Adversarial Network; Language Model; Large Language Model; Low-carbon Transitions; Transportation Electrifications; Electric Power Transmission Networks,Scopus
2-s2.0-85217737988,Concept-Based RAG Models: A High-Accuracy Fact Retrieval Approach,,"This study introduces a concept-based methodology to optimize Retrieval-Augmented Generation (RAG) tasks by assessing dataset certainty using entropy-based metrics and concept extraction techniques. Unlike traditional methods focused on reducing LLM hallucinations or modifying data structures, this approach evaluates inherent knowledge uncertainty from an LLM perspective. By pre-processing documents with LLMs, the concept-based method significantly enhances precision in tasks demanding high accuracy, such as legal, finance, or formal document responses. © 2025 Elsevier B.V., All rights reserved.",Content Based Retrieval; Data Structures; Concept Extraction; Concept-based; Concept-based Retrieval; Entropy Based Metrics; Extraction Techniques; High-accuracy; Pre-processing; Uncertainty; Computational Linguistics,Scopus
10.1109/CSCE60160.2023.00302,Content Analysis of Items in Newspaper Data Using Table Arrangement Technology and ChatGPT for Stock Price Prediction,10.1109/csce60160.2023.00302,"In this study, we employed table arrangement techniques and ChatGPT to analyze newspaper content relevant to stock prices. Using table arrangement techniques, we effectively organized sentences from articles into tables, extracting 22 key content elements. Additionally, we discovered that ChatGPT possesses the ability to extract and present newspaper data in tabular form. Factors were found to influence stock price movements. Drops in stock prices were impacted by factors such as crude oil prices, and the COVID-19 pandemic. Conversely, rising stock prices were supported by global trends, and vac-cine effectiveness. Furthermore, we propose a highly effective, large-scale method for constructing tables by combining table arrangement techniques and ChatGPT. The proposed method achieves an accuracy rate of 0.95 under a lenient criterion. © 2024 Elsevier B.V., All rights reserved.",Chatgpt; Content Analysis; Newspaper Data; Regular Research Paper; Stock Prices; Table Arrangement; Covid-19; Financial Markets; Newsprint; Chatgpt; Content Analysis; Crude Oil Prices; Newspaper Data; Regular Research Paper; Research Papers; Stock Price; Stock Price Movements; Stock Price Prediction; Table Arrangement; Costs,Scopus
WOS:001563374700001,Contract sentence-level evaluation (Con-SEN): A sentence-level semantic engine for accurate recognition of financial contract clauses,10.1142/s2424786325500161,"Financial contracts under regulatory review are often characterized by excessive length, intricate clause nesting, implicit negations, and cross-sentence legal dependencies - posing significant challenges for automated compliance systems. To address these issues, we propose Contract Sentence-level Evaluation (Con-SEN), a sentence-level semantic framework designed for fine-grained clause recognition and risk-oriented interpretation in financial documents. This work introduces three core innovations. First, we construct Con-SEN-Corpus, a domain-specific dataset spanning over 30 financial sub-industries from 2010 to 2024. Each sentence is annotated along three legal dimensions - clause type, legality status, and risk level - enabling multi-dimensional supervision. Second, we develop a structure-aware encoder based on an enhanced Longformer, incorporating a sliding-window mechanism and a novel clause-anchor global attention module to capture long-range dependencies and structural hierarchies across chapters. Third, we introduce a negation polarity and regulatory keyword injection module, which improves the model's ability to resolve adversarial logic, implicit obligations, and exemption clauses - often overlooked by general-purpose LLMs. To ensure coherent predictions across the three dimensions, we propose a multi-task consistency learning strategy that jointly optimizes clause classification, legality assessment, and risk estimation. Extensive experiments on real-world contract datasets show that Con-SEN significantly outperforms leading LLMs such as ChatGPT and Claude, achieving 18-25 percentage points higher document-level accuracy, up to 23% greater sentence coverage, and up to 6% improvements in clause-level classification. Moreover, it reduces volatility across evaluation metrics by an order of magnitude. These results position Con-SEN as a precise, regulation-aware framework for contract analysis, capable of handling the linguistic and structural complexity inherent in financial compliance tasks.",Sentence-level semantic modeling; structure-aware encoding; multi-task learning; long-document processing; financial contracts,WoS
10.3390/math13091538,Correctness Coverage Evaluation for Medical Multiple-Choice Question Answering Based on the Enhanced Conformal Prediction Framework,10.3390/math13091538,"Large language models (LLMs) are increasingly adopted in medical question answering (QA) scenarios. However, LLMs have been proven to generate hallucinations and nonfactual information, undermining their trustworthiness in high-stakes medical tasks. Conformal Prediction (CP) is now recognized as a robust framework within the broader domain of machine learning, offering statistically rigorous guarantees of marginal (average) coverage for prediction sets. However, the applicability of CP in medical QA remains to be explored. To address this limitation, this study proposes an enhanced CP framework for medical multiple-choice question answering (MCQA) tasks. The enhanced CP framework associates the non-conformance score with the frequency score of the correct option. The framework generates multiple outputs for the same medical query by leveraging self-consistency theory. The proposed framework calculates the frequency score of each option to address the issue of limited access to the model’s internal information. Furthermore, a risk control framework is incorporated into the enhanced CP framework to manage task-specific metrics through a monotonically decreasing loss function. The enhanced CP framework is evaluated on three popular MCQA datasets using off-the-shelf LLMs. Empirical results demonstrate that the enhanced CP framework achieves user-specified average (or marginal) error rates on the test set. Moreover, the results show that the test set’s average prediction set size (APSS) decreases as the risk level increases. It is concluded that it is a promising evaluation metric for the uncertainty of LLMs. © 2025 Elsevier B.V., All rights reserved.",Average Prediction Set Size; Conformal Prediction; Large Language Models; Medical Multiple-choice Question Answering,Scopus
10.18653/v1/2023.conll-1.38,Cross-Document Event Coreference Resolution: Instruct Humans or Instruct GPT?,10.18653/v1/2023.conll-1.38,"This paper explores utilizing Large Language Models (LLMs) to perform Cross-Document Event Coreference Resolution (CDEC) annotations and evaluates how they fare against human annotators with different levels of training. Specifically, we formulate CDEC as a multi-class classification problem on pairs of events that are represented as decontextualized sentences, and compare the predictions of GPT-4 with the judgment of fully trained annotators and crowdworkers on the same dataset. Our study indicates that GPT-4 with zero-shot learning outperformed crowd-workers by a large margin and exhibits a level of performance comparable to trained annotators. Upon closer analysis, GPT-4 also exhibits tendencies of being overly confident, and forcing annotation decisions even when such decisions are not warranted due to insufficient information. Our results have implications on how to perform complicated annotations such as CDEC in the age of LLMs, and show that the best way to acquire such annotations might be to combine the strengths of LLMs and trained human annotators in the annotation process, and using untrained or undertrained crowdworkers is no longer a viable option to acquire high-quality data to advance the state of the art for such problems. We make our source and data publicly available.1 © 2024 Elsevier B.V., All rights reserved.",Classification (of Information); Natural Language Processing Systems; Coreference Resolution; Cross Documents; Forcings; High Quality Data; Language Model; Large Margins; Multiclass Classification Problems; Performance; State Of The Art; Workers'; Zero-shot Learning,Scopus
10.1162/TACL.a.20,Data Contamination Quiz: A Tool to Detect and Estimate Contamination in Large Language Models,10.1162/tacl.a.20,"We propose the Data Contamination Quiz (DCQ), a simple and effective approach to detect data contamination in large language models (LLMs) and estimate the amount of it. Specifically, we frame data contamination detection as a series of multiple-choice questions, devising a quiz format wherein three perturbed versions of each instance, subsampled from a specific dataset partition, are created. These changes only include word-level perturbations. The generated perturbations, along with the original dataset instance, form the options in the DCQ, with an extra option accommodating the selection of none of the provided options. Given that the only distinguishing signal among the options is the exact wording with respect to the original dataset instance, an LLM, when tasked with identifying the original dataset instance, gravitates towards selecting the original one if it has been exposed to it. While accounting for positional biases in LLMs, the quiz performance reveals the contamination level for the tested model with the dataset partition to which the quiz pertains. Applied to various datasets and LLMs, under controlled and uncontrolled contamination, our findings—while fully lacking access to training data and model parameters—suggest that DCQ achieves state-of-the-art results and uncovers greater contamination levels through memorization compared to existing methods. Also, it proficiently bypasses more safety filters, especially those set to avoid generating copyrighted content.1 © 2025 Elsevier B.V., All rights reserved.",Computational Linguistics; Contamination; Natural Language Processing Systems; Contamination Detection; Contamination Levels; Dataset Partitions; Effective Approaches; Exposed To; Frame Data; Language Model; Multiple-choice Questions; Simple Approach; Word Level; Pollution Detection,Scopus
WOS:001437443700004,Distractor Generation for Multiple-Choice Questions with Predictive Prompting and Large Language Models,10.1007/978-3-031-74627-7_4,"Large Language Models (LLMs) such as ChatGPT have demonstrated remarkable performance across various tasks and have garnered significant attention from both researchers and practitioners. However, in an educational context, we still observe a performance gap in generating distractors-i.e., plausible yet incorrect answers-with LLMs for multiple-choice questions (MCQs). In this study, we propose a strategy for guiding LLMs such as ChatGPT, in generating relevant distractors by prompting them with question items automatically retrieved from a question bank as well-chosen in-context examples. We evaluate our LLM-based solutions using a quantitative assessment on an existing test set, as well as through quality annotations by human experts, i.e., teachers. We found that on average 53% of the generated distractors presented to the teachers were rated as high-quality, i.e., suitable for immediate use as is, outperforming the state-of-the-art model. We also show the gains of our approach (https://github.com/semerekiros/distractGPT/) in generating high-quality distractors by comparing it with a zero-shot ChatGPT and a few-shot ChatGPT prompted with static examples.",Distractor generation; natural language processing; large language models; predictive prompting; language learning; neural networks,WoS
10.1117/12.3060936,Distributed photovoltaic power prediction based on Solar-LLM,10.1117/12.3060936,"Distributed photovoltaic power generation has volatility and intermittently, and its power generation is usually difficult to predict accurately. Previous studies have mostly focused on physical or mathematical modeling methods, and it is difficult to grasp the complexity and variability of historical data, and the prediction accuracy is limited. This paper presents a distributed PV power prediction method Solar-LLM, firstly, introduced the technical principles of the model, and then introduced the Solar-LLM model architecture and multistep photovoltaic power prediction method, the large language model, by reprogramming the input and output layer, can effectively consider the complexity and versatility of historical data. Finally, the performance of the model is demonstrated by example analysis and compared with existing methods. The experiment is based on the datasets of five photovoltaic power stations in Hebei Province, China, and the results show that the performance of Solar-LLM is better than that of GRU, LSTM, TCN, Transformer and Informer in different prediction periods, which is a feasible and effective method for power generation prediction. © 2025 Elsevier B.V., All rights reserved.",Large Language Model; Photovoltaic Power Prediction; Time Series Prediction; Historical Data; Language Model; Large Language Model; Performance; Photovoltaic Power; Photovoltaic Power Prediction; Power Predictions; Power- Generations; Prediction Methods; Time Series Prediction; Problem Oriented Languages,Scopus
10.1016/j.iref.2025.104642,Dual-model synergy for audit opinion prediction: A collaborative LLM agent framework approach,10.1016/j.iref.2025.104642,"By combining the capabilities of Moonshot and DeepSeek-R1, which respectively evaluate risk scores based on MD&A text information and financial data, we exploit the complementary strengths of long-context and reasoning large language models (LLMs) to help evaluate material misstatement risks in audit opinions. Our results suggest that both MD&A-based and financial-based risk evaluations effectively distinguish qualified and unqualified audit opinions, but combining them yields the best performance. In addition to the interpretative analysis text output, the combined LLM evaluation consistently outperforms logistic regression prediction that incorporates all indicators, and it achieves comparable performance compared to the sophisticated machine learning methods like gradient boosting regression and random forests. Further analysis reveals that LLMs excel in high-risk scenarios: in firms with 1) high financial constraints, 2) low internal controls, 3) low audit quality, 4) low readability, or 5) negative tone of MD&A texts. A topic model analysis has shown clear difference in the MD&A emphasis for firms the qualified and unqualified opinions given by our framework. These findings shed light on the potential role of the collaborative LLM agent framework as a tool to help auditors and investors detect financial fraud. © 2025 Elsevier B.V., All rights reserved.",Audit Opinion Prediction; Financial Fraud; Generative Ai; Large Language Model; Long-context Model; Material Misstatement Risk Assessment; Reasoning Model,Scopus
10.1145/3677052.3698689,ECC Analyzer: Extracting Trading Signal from Earnings Conference Calls using Large Language Model for Stock Volatility Prediction,10.1145/3677052.3698689,"In the realm of financial analytics, leveraging unstructured data, such as earnings conference calls (ECCs), to forecast stock volatility is a critical challenge that has attracted both academics and investors. While previous studies have used multimodal deep learning-based models to obtain a general view of ECCs for volatility predicting, they often fail to capture detailed, complex information. Our research introduces a novel framework: ECC Analyzer, which utilizes large language models (LLMs) to extract richer, more predictive content from ECCs to aid the model's prediction performance. We use the pre-trained large models to extract textual and audio features from ECCs and implement a hierarchical information extraction strategy to extract more fine-grained information. This strategy first extracts paragraph-level general information by summarizing the text and then extracts fine-grained focus sentences using Retrieval-Augmented Generation (RAG). These features are then fused through multimodal feature fusion to perform volatility prediction. Experimental results demonstrate that our model outperforms traditional analytical benchmarks, confirming the effectiveness of advanced LLM techniques in financial analysis. © 2025 Elsevier B.V., All rights reserved.",Earnings Conference Call Analysis; Large Language Model; Retrieval-augmented Generation; Volatility Forecasting; Prediction Models; Critical Challenges; Earning Conference Call Analyze; Fine Grained; Language Model; Large Language Model; Learning Based Models; Multi-modal; Retrieval-augmented Generation; Unstructured Data; Volatility Forecasting; Deep Learning,Scopus
10.1117/12.3064969,Electricity load forecasting using large language models,10.1117/12.3064969,"With the increasing global energy demand and the widespread deployment of renewable energy sources, the operation and management of power systems face unprecedented challenges. Electricity load forecasting is a critical component of power system planning and operation, playing a significant role in ensuring the reliability of power supply, optimizing resource allocation, and enhancing economic benefits. This study proposes a deep learning-based electricity load forecasting framework that integrates two large language models, Qwen2.5-14B and Lag-Llama, to improve the accuracy and robustness of predictive models through meticulously designed prompt engineering, LoRA fine-tuning strategies, and data fusion tactics. The experiment utilized hourly electricity consumption data from May 1, 2004, to April 30, 2009, to forecast hourly electricity consumption from May 1 to October 31, 2009, and included the Shanghai Composite Index, CSI 300, CSI 500, and CSI 1000 as auxiliary financial data for prediction. The results demonstrate that our approach shows certain advantages in predictive accuracy, especially in handling multi-source data and considering spatiotemporal correlations. This provides valuable insights for power operators to optimize power generation plans and scheduling decisions, thereby improving economic efficiency and reducing environmental impact. © 2025 Elsevier B.V., All rights reserved.",Data Fusion; Deep Learning; Electricity Load Forecasting; Large Language Models; Power Systems; Energy Supply; Energy Utilization; Deep Learning; Electricity Load Forecasting; Electricity-consumption; Global Energy Demand; Language Model; Large Language Model; Operation And Management; Power; Power System; Renewable Energy Source; Power Management,Scopus
10.1108/JHTT-07-2023-0203,Embracing the ChatGPT revolution: unlocking new horizons for tourism,10.1108/jhtt-07-2023-0203,"Purpose: This study aims to investigate tourists’ attitudes and intentions regarding the usage of Chat Generative Pre-trained Transformer (ChatGPT) for accessing tourism information. Furthermore, by integrating the perceived risks associated with ChatGPT and the theory of planned behavior (TPB), this research examines the impact of three types of perceived risks, such as privacy risk, accuracy risk and overreliance risk, on tourists’ behavioral intention. Design/methodology/approach: Data were gathered for this study by using two online survey platforms, thus resulting in a sample of 536 respondents. The online survey questionnaire assessed tourists’ perceived risks, attitude, subjective norm, perceived behavioral control, behavioral intention and demographic information related to their usage of ChatGPT. Findings: The structural equation modeling analysis revealed that tourists express concerns about the associated risks of using ChatGPT to search for tourism information, specifically privacy risk, accuracy risk and overreliance risk. It was found that perceived risks significantly influence tourists’ attitude and intention toward the usage of ChatGPT, which is consistent with the hypotheses proposed in previous literature regarding tourists’ perceived risks of ChatGPT. Research limitations/implications: This work is a preliminary empirical study that assesses tourists’ behavioral intention toward the use of ChatGPT in the field of tourism. Previous research has remained at the hypothetical level, speculating about the impact of ChatGPT on the tourism industry. This study investigates the behavioral intention of tourists who have used ChatGPT to search for travel information. Furthermore, this study provides evidence based on the outcome of this research and offers theoretical foundations for the sustainable development of generative AI in the tourism domain. This study has limitations in that it primarily focused on exploring the risks associated with ChatGPT and did not extensively investigate its range of benefits. Practical implications: First, to address privacy concerns that pose significant challenges for chatbots various measures, such as data encryption, secure storage and obtaining user consent, are crucial. Second, despite concerns and uncertainties, the introduction of ChatGPT holds promising prospects for the tourism industry. By offering personalized recommendations and enhancing operational efficiency, ChatGPT has the potential to revolutionize travel experiences. Finally, recognizing the potential of ChatGPT in enhancing customer service and operational efficiency is crucial for tourism enterprises. Social implications: Recognizing the potential of ChatGPT in enhancing customer service and operational efficiency is crucial for tourism enterprises. As their interest in adopting ChatGPT grows, increased investments and resources will be dedicated to developing and implementing ChatGPT solutions. This enhancement may involve creating customized ChatGPT solutions and actively engaging in training and development programs to empower employees in effectively using ChatGPT’s capabilities. Such initiatives can contribute to improved customer service and overall operations within the tourism industry. Originality/value: This study integrates TPB with perceived risks in ChatGPT, thus providing empirical evidence. It highlights the importance of considering perceived risks in tourists’ intentions and contributes to the sustainable development of generative AI in tourism. As such, it provides valuable insights for practitioners and policymakers. © 2024 Elsevier B.V., All rights reserved.",Chatgpt; Generative Ai; Information; Perceived Risk; Theory Of Planned Behavior (tpb),Scopus
10.1007/978-3-031-78554-2_8,Empowering Airline Route Decisions with LLM-Generated Pseudo-labels and Zero-Shot Review Prediction,10.1007/978-3-031-78554-2_8,"The airline industry has suffered a severe impact due to the COVID-19 pandemic. It resulted in significant financial losses. Strategic route planning is now an urgent need to mitigate the ongoing crisis. Motivated by the importance of customer sentiment in informing airline route decisions, this paper presents EAGLE (Enhancing Airline Groundtruth Labels and rEview rating prediction), a novel two-stage framework that leverages the power of Large Language Models (LLMs) to address the limitations of current works, which often rely on manual labeling and traditional machine learning models. In the first phase, EAGLE introduces a pseudo-labeling approach using LLMs to automatically label customer reviews to reduce the need for manual annotation and mitigate potential biases that exist in human labeling. The second phase employs a zero-shot LLM-based text classification method to predict customer sentiment and preferences from online reviews to provide a more accurate and context-aware analysis of customer feedback. Through extensive experiments, we demonstrate the effectiveness and robustness of EAGLE to demonstrate its superior performance compared to existing techniques. The proposed framework empowers airline companies to make data-driven decisions about route expansions, considering customer preferences and sentiments. Our contribution fibs in enhancing the objectivity of sentiment analysis and providing a comprehensive and scalable solution for airline route planning in the post-pandemic era, eventually leading to improved customer satisfaction and optimized operations. © 2025 Elsevier B.V., All rights reserved.",Airline Industry; Large Language Model; Social Media Data Mining; Text Generation; Air Transportation; Customer Satisfaction; Labeled Data; Sentiment Analysis; Strategic Planning; Transportation Routes; Zero-shot Learning; 'current; Airline Industry; Airline Routes; Financial Loss; Language Model; Large Language Model; Power; Route Planning; Social Media Data Minings; Text Generations; Sales,Scopus
10.4108/airo.6117,Empowering financial futures: Large language models in the modern financial landscape,10.4108/airo.6117,"In this paper, we delve into the transformative influence of Large Language Models (LLMs) in the financial sector. Through meticulous exploration, we uncover the multifaceted applications of LLMs, ranging from elevating customer support and fortifying fraud detection to reshaping market analysis and prediction. LLMs, with their unparalleled ability to process extensive textual data, bring forth innovative solutions and insights. However, we also address critical challenges such as user trust and ethical considerations, emphasizing the need for responsible integration. Collaborative efforts between industry stakeholders and researchers are essential prerequisites for making a pivotal stride towards a future where LLMs redefine financial practices, with efficiency, accuracy, and ethical precision shaping the industry’s evolution. © 2024 Elsevier B.V., All rights reserved.",Customer Support; Financial Sector; Fraud Detection; Large Language Models,Scopus
WOS:001540681300080,"Energy-Efficient Inference on RNN and LLM networks: A Quantized Evaluation on RISC-V, ARM, and x86 Devices",10.1145/3679240.3735101,"This work explores the energy efficiency and performance of quantized artificial intelligence models deployed on low-power devices across multiple hardware architectures, including RISC-V, x86, ARM 64, and ARM 32. We examine two distinct model types: recurrent neural networks (RNNs) for meteorological forecasting and large language models (LLMs) for conversational applications. Quantization techniques are employed to reduce model size and computational overhead while maintaining acceptable accuracy, enabling the execution of large-scale models on resource-constrained platforms. We evaluate how these models perform in practical edge scenarios, measuring accuracy, execution time, and energy consumption across diverse hardware configurations. The results show that the benefits of quantization vary with both the model type and the underlying architecture, revealing important trade-offs between efficiency and predictive performance. These findings demonstrate the viability of running advanced AI models on edge devices, and offer guidance for optimizing deployments in energy-sensitive environments.",Artificial Intelligence; Quantization; Model optimization; Energy Efficiency,WoS
WOS:001380651800001,Enhancing Alzheimer's Detection: Leveraging ADNI Data and Large Language Models for High-Accuracy Diagnosis,,"-Alzheimer's disease (AD), the most common type of dementia, is expected to affect 152 million people by 2050, emphasizing the importance of early diagnosis. This study uses the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset, combining cognitive tests, biomarkers, demographic details, and genetic data to build predictive models. Using large language models (LLMs), specifically ChatGPT 3.5, we achieved high classification accuracy, with ROC AUC values of 0.98 for cognitively normal (CN) individuals, 0.99 for dementia, and 0.98 for mild cognitive impairment (MCI). These findings show that LLMs can handle complex data quickly and accurately. By focusing on numerical and text-based data instead of just imaging, this method provides a cost-effective and accessible option for diagnosing AD. Adding genetic information improves the predictions, reflecting the important role of genetics in AD risk. This study highlights the potential of combining different types of data with advanced machine learning and LSTM to improve early AD diagnosis. Future research should explore more ways to combine data and test different machine learning models to further enhance diagnostic tools.",-Alzheimer; dementia; LLMs; ChatGPT; LSTM,WoS
10.14569/IJACSA.2024.01511134,Enhancing Alzheimer’s Detection: Leveraging ADNI Data and Large Language Models for High-Accuracy Diagnosis,10.14569/ijacsa.2024.01511134,"Alzheimer’s disease (AD), the most common type of dementia, is expected to affect 152 million people by 2050, emphasizing the importance of early diagnosis. This study uses the Alzheimer’s Disease Neuroimaging Initiative (ADNI) dataset, combining cognitive tests, biomarkers, demographic details, and genetic data to build predictive models. Using large language models (LLMs), specifically ChatGPT 3.5, we achieved high classification accuracy, with ROC AUC values of 0.98 for cognitively normal (CN) individuals, 0.99 for dementia, and 0.98 for mild cognitive impairment (MCI). These findings show that LLMs can handle complex data quickly and accurately. By focusing on numerical and text-based data instead of just imaging, this method provides a cost-effective and accessible option for diagnosing AD. Adding genetic information improves the predictions, reflecting the important role of genetics in AD risk. This study highlights the potential of combining different types of data with advanced machine learning and LSTM to improve early AD diagnosis. Future research should explore more ways to combine data and test different machine learning models to further enhance diagnostic tools. © 2025 Elsevier B.V., All rights reserved.",Alzheimer; Chatgpt; Dementia; Llms; Lstm; Diagnosis; Neuroimaging; Alzheimer; Chatgpt; Cognitive Tests; Dementia; Early Diagnosis; Genetic Data; High-accuracy; Language Model; Large Language Model; Lstm; Neurodegenerative Diseases,Scopus
10.1145/3604237.3626866,Enhancing Financial Sentiment Analysis via Retrieval Augmented Large Language Models,10.1145/3604237.3626866,"Financial sentiment analysis is critical for valuation and investment decision-making. Traditional NLP models, however, are limited by their parameter size and the scope of their training datasets, which hampers their generalization capabilities and effectiveness in this field. Recently, Large Language Models (LLMs) pre-trained on extensive corpora have demonstrated superior performance across various NLP tasks due to their commendable zero-shot abilities. Yet, directly applying LLMs to financial sentiment analysis presents challenges: The discrepancy between the pre-training objective of LLMs and predicting the sentiment label can compromise their predictive performance. Furthermore, the succinct nature of financial news, often devoid of sufficient context, can significantly diminish the reliability of LLMs' sentiment analysis. To address these challenges, we introduce a retrieval-augmented LLMs framework for financial sentiment analysis. This framework includes an instruction-tuned LLMs module, which ensures LLMs behave as predictors of sentiment labels, and a retrieval-augmentation module which retrieves additional context from reliable external sources. Benchmarked against traditional models and LLMs like ChatGPT and LLaMA, our approach achieves 15% to 48% performance gain in accuracy and F1 score. © 2023 Elsevier B.V., All rights reserved.",Instruction Tuning; Large Language Models; Retrieval Augmented Generation; Sentiment Analysis; Computational Linguistics; Decision Making; Investments; Reliability Analysis; Generalization Capability; Instruction Tuning; Investment Decision Making; Language Model; Large Language Model; Performance; Pre-training; Retrieval Augmented Generation; Sentiment Analysis; Training Dataset,Scopus
10.1109/ICCA62237.2024.10927923,Enhancing Stock Price Prediction: A Hybrid Approach Leveraging Large Language Models and Deep Learning,10.1109/icca62237.2024.10927923,"Accurate stock price prediction is a challenging yet crucial goal in finance, with significant implications for investment decisions and risk management. This paper presents a comprehensive review of machine learning techniques for stock price prediction, examining traditional methods such as regression and ensemble models, as well as advanced approaches that integrate sentiment analysis and textual data sources. With the emergence of powerful Large Language Models (LLMs) such as ChatGPT, Llama and Gemini, we explore their potential for enhancing predictive accuracy using historical stock data. Key challenges are discussed, including data quality, model interpretability, and adapting to dynamic market conditions. Additionally, this paper proposes a trustworthy stock price prediction model based on LLMs enabling informed investment decision-making. Experimental results demonstrate that ChatGPT-4o model achieved a prediction accuracy of approximately 97%, which can be improved by tuning model parameters. Consequently, the paper highlights the potential of LLMs in improving stock price forecasting. © 2025 Elsevier B.V., All rights reserved.",Large Language Models (llms); Machine Learning; Stock Price Prediction; Adversarial Machine Learning; Decision Management; Hybrid Approach; Investment Decisions; Investment Risks; Language Model; Large Language Model; Machine Learning Techniques; Machine-learning; Risks Management; Stock Price Prediction; Prediction Models,Scopus
2-s2.0-85188699932,Enhancing Text Classification through LLM-Driven Active Learning and Human Annotation,,"In the context of text classification, the financial burden of annotation exercises for creating training data is a critical issue. Active learning techniques, particularly those rooted in uncertainty sampling, offer a cost-effective solution by pinpointing the most instructive samples for manual annotation. Similarly, Large Language Models (LLMs) such as GPT-3.5 provide an alternative for automated annotation but come with concerns regarding their reliability. This study introduces a novel methodology that integrates human annotators and LLMs within an Active Learning framework. We conducted evaluations on three public datasets. IMDB for sentiment analysis, a Fake News dataset for authenticity discernment, and a Movie Genres dataset for multi-label classification. The proposed framework integrates human annotation with the output of LLMs, depending on the model uncertainty levels. This strategy achieves an optimal balance between cost efficiency and classification performance. The empirical results show a substantial decrease in the costs associated with data annotation while either maintaining or improving model accuracy. © 2024 Elsevier B.V., All rights reserved.",Classification (of Information); Computational Linguistics; Cost Effectiveness; Fake Detection; Learning Systems; Uncertainty Analysis; Active Learning; Cost-effective Solutions; Critical Issues; Human Annotations; Language Model; Learning Techniques; Model-driven; Text Classification; Training Data; Uncertainty Samplings; Sentiment Analysis,Scopus
10.1016/j.knosys.2025.114449,Enhancing large language models for bitcoin time series forecasting,10.1016/j.knosys.2025.114449,"In the recent advancements in application of deep learning to time series forecasting, focus has shifted from training transformers end-to-end to efficiently leveraging the predictive capabilities of Large Language Models (LLMs). Models that encode the time series data to interact with a frozen LLM backbone have been shown to outperform transformers on all benchmark datasets. However, their efficiency on complex datasets, which do not show clear seasonality or trend, remains an open question. In this work, we seek to evaluate the performance of reprogrammed LLMs on the Bitcoin price chart, a financial time series known for its complexity and high volatility. We propose effective methods to improve the performance of Time-LLM, a State-of-the-art (SOTA) method, on such a time series. First, we propose structural improvements to Time-LLM. Second, we suggest an efficient way to handle the non-stationarity of the dataset. Finally, we propose an efficient method for passing additional financial information to the LLM. Our results demonstrate a 50 % improvement on the average percentage loss and a 5 % increase on accuracy of our adapted Time-LLM architecture on Bitcoin data when compared to SOTA models, including the original Time-LLM model. This highlights the impact on forecast accuracy of domain-specific decision making in data processing and feature selection. © 2025 Elsevier B.V., All rights reserved.",Financial Time Series; Language Models; Time Series Forecasting; Data Accuracy; Data Handling; Decision Making; Deep Learning; Finance; Forecasting; Learning Systems; Prediction Models; Time Series; Benchmark Datasets; Complex Datasets; End To End; Financial Time Series; Language Model; Performance; Predictive Capabilities; Seasonality; Time Series Forecasting; Time-series Data; Financial Data Processing,Scopus
10.1371/journal.pone.0326034,Enhancing stock timing predictions based on multimodal architecture: Leveraging large language models (LLMs) for text quality improvement,10.1371/journal.pone.0326034,"This study aims to enhance stock timing predictions by leveraging large language models (LLMs), specifically GPT-4, to filter and analyze online investor comment data. Recognizing challenges such as variable comment quality, redundancy, and authenticity issues, we propose a multimodal architecture that integrates filtered comment data with stock price dynamics and technical indicators. Using data from nine Chinese banks, we compare four filtering models and demonstrate that employing GPT-4 significantly improves financial metrics like profit-loss ratio, win rate, and excess return rate. The multimodal architecture outperforms baseline models by effectively preprocessing comment data and combining it with quantitative financial data. While focused on Chinese banks, the approach can be adapted to broader markets by modifying the prompts of large language models. Our findings highlight the potential of LLMs in financial forecasting and provide more reliable decision support for investors. © 2025 Elsevier B.V., All rights reserved.","Methadone; Methadone; Article; Artificial Intelligence; Artificial Neural Network; Decision Making; Decision Support System; Human; Language; Learning Algorithm; Machine Learning; Mathematical Analysis; Mathematical Model; Multimodal Architecture; Multimodal Imaging; Natural Language Processing; Total Quality Management; China; Economic Model; Economics; Investment; Large Language Model; Humans; Investments; Language; Large Language Models; Models, Economic; Quality Improvement",Scopus
10.1016/j.xops.2023.100324,Evaluating the Performance of ChatGPT in Ophthalmology: An Analysis of Its Successes and Shortcomings,10.1016/j.xops.2023.100324,"Purpose: Foundation models are a novel type of artificial intelligence algorithms, in which models are pretrained at scale on unannotated data and fine-tuned for a myriad of downstream tasks, such as generating text. This study assessed the accuracy of ChatGPT, a large language model (LLM), in the ophthalmology question-answering space. Design: Evaluation of diagnostic test or technology. Participants: ChatGPT is a publicly available LLM. Methods: We tested 2 versions of ChatGPT (January 9 “legacy” and ChatGPT Plus) on 2 popular multiple choice question banks commonly used to prepare for the high-stakes Ophthalmic Knowledge Assessment Program (OKAP) examination. We generated two 260-question simulated exams from the Basic and Clinical Science Course (BCSC) Self-Assessment Program and the OphthoQuestions online question bank. We carried out logistic regression to determine the effect of the examination section, cognitive level, and difficulty index on answer accuracy. We also performed a post hoc analysis using Tukey's test to decide if there were meaningful differences between the tested subspecialties. Main Outcome Measures: We reported the accuracy of ChatGPT for each examination section in percentage correct by comparing ChatGPT's outputs with the answer key provided by the question banks. We presented logistic regression results with a likelihood ratio (LR) chi-square. We considered differences between examination sections statistically significant at a P value of < 0.05. Results: The legacy model achieved 55.8% accuracy on the BCSC set and 42.7% on the OphthoQuestions set. With ChatGPT Plus, accuracy increased to 59.4% ± 0.6% and 49.2% ± 1.0%, respectively. Accuracy improved with easier questions when controlling for the examination section and cognitive level. Logistic regression analysis of the legacy model showed that the examination section (LR, 27.57; P = 0.006) followed by question difficulty (LR, 24.05; P < 0.001) were most predictive of ChatGPT's answer accuracy. Although the legacy model performed best in general medicine and worst in neuro-ophthalmology (P < 0.001) and ocular pathology (P = 0.029), similar post hoc findings were not seen with ChatGPT Plus, suggesting more consistent results across examination sections. Conclusion: ChatGPT has encouraging performance on a simulated OKAP examination. Specializing LLMs through domain-specific pretraining may be necessary to improve their performance in ophthalmic subspecialties. Financial Disclosure(s): Proprietary or commercial disclosure may be found after the references. © 2023 Elsevier B.V., All rights reserved.",Artificial Intelligence; Chatgpt; Generative Pretrained Transformer; Medical Education; Ophthalmology; Accreditation; Article; Artificial Intelligence; Cognition; Eye Disease; General Practice; Logistic Regression Analysis; Measurement Accuracy; Measurement Repeatability; Neuroophthalmology; Ophthalmology; Post Hoc Analysis,Scopus
10.1038/s41598-025-17188-7,Evaluating the ability of large Language models to predict human social decisions,10.1038/s41598-025-17188-7,"Recent advances in large language models (LLMs) have highlighted their potential to predict human decisions. In two studies, we compared predictions by GPT-3.5 and GPT-4 across 51 scenarios (9,600 responses) against published data from 2,104 human participants within an evolutionary-psychology framework. We further examined our findings with GPT-4o across eight social-group and kinship conditions (1,600 responses). Our results revealed behavioral differences between humans and LLMs’ predictions: Humans showed a greater sensitivity to kinship and group size than the LLMs when making life-death decisions. LLMs align closer with humans with a higher risk-seeking preference in financial domains. While human choices followed Prospect theory’s value function (risk-averse in gains, risk-seeking in losses), LLMs often predicted reversed patterns. GPT-3.5 matched the average level of human risk preference but showed reversed framing effects; GPT-4 was indiscriminately risk-averse across social contexts. While humans were more risk-seeking in small or kin groups than in large groups, GPT-4o made the opposite predictions. Our results suggest a set of criteria for a psychological version of the Turing Test reflected in framing effects and social context-dependent risk preference involving kinship, group size, social relations, sense of fairness, self-age awareness, public vs. personal properties, and social group-dependent aspiration levels. © 2025 Elsevier B.V., All rights reserved.",Framing Effects; Generative Ai; Group Size; Group-dependent Aspiration Levels; Kinship; Social Decision-making; Adult; Decision Making; Female; High Risk Behavior; Human; Language; Large Language Model; Male; Social Behavior; Young Adult; Adult; Decision Making; Female; Humans; Language; Large Language Models; Male; Risk-taking; Social Behavior; Young Adult,Scopus
WOS:001044510300001,Evaluating the performance of large language models: ChatGPT and Google Bard in generating differential diagnoses in clinicopathological conferences of neurodegenerative disorders,10.1111/bpa.13207,"This study explores the utility of the large language models (LLMs), specifically ChatGPT and Google Bard, in predicting neuropathologic diagnoses from clinical summaries. A total of 25 cases of neurodegenerative disorders presented at Mayo Clinic brain bank Clinico-Pathological Conferences were analyzed. The LLMs provided multiple pathologic diagnoses and their rationales, which were compared with the final clinical diagnoses made by physicians. ChatGPT-3.5, ChatGPT-4, and Google Bard correctly made primary diagnoses in 32%, 52%, and 40% of cases, respectively, while correct diagnoses were included in 76%, 84%, and 76% of cases, respectively. These findings highlight the potential of artificial intelligence tools like ChatGPT in neuropathology, suggesting they may facilitate more comprehensive discussions in clinicopathological conferences.",artificial intelligence; ChatGPT; clinicopathological conference; CPC; Google Bard; large language model; neuropathology; pathology,WoS
10.3389/fopht.2024.1387190,Evaluating the strengths and limitations of multimodal ChatGPT-4 in detecting glaucoma using fundus images,10.3389/fopht.2024.1387190,"Overview: This study evaluates the diagnostic accuracy of a multimodal large language model (LLM), ChatGPT-4, in recognizing glaucoma using color fundus photographs (CFPs) with a benchmark dataset and without prior training or fine tuning. Methods: The publicly accessible Retinal Fundus Glaucoma Challenge “REFUGE” dataset was utilized for analyses. The input data consisted of the entire 400 image testing set. The task involved classifying fundus images into either ‘Likely Glaucomatous’ or ‘Likely Non-Glaucomatous’. We constructed a confusion matrix to visualize the results of predictions from ChatGPT-4, focusing on accuracy of binary classifications (glaucoma vs non-glaucoma). Results: ChatGPT-4 demonstrated an accuracy of 90% with a 95% confidence interval (CI) of 87.06%-92.94%. The sensitivity was found to be 50% (95% CI: 34.51%-65.49%), while the specificity was 94.44% (95% CI: 92.08%-96.81%). The precision was recorded at 50% (95% CI: 34.51%-65.49%), and the F1 Score was 0.50. Conclusion: ChatGPT-4 achieved relatively high diagnostic accuracy without prior fine tuning on CFPs. Considering the scarcity of data in specialized medical fields, including ophthalmology, the use of advanced AI techniques, such as LLMs, might require less data for training compared to other forms of AI with potential savings in time and financial resources. It may also pave the way for the development of innovative tools to support specialized medical care, particularly those dependent on multimodal data for diagnosis and follow-up, irrespective of resource constraints. © 2024 Elsevier B.V., All rights reserved.",Artificial Intelligence; Chatgpt; Glaucoma; Gpt; Large Language Models,Scopus
WOS:001447403300001,Evaluation of ChatGPT Performance on Emergency Medicine Board Examination Questions: Observational Study,10.2196/67696,"Background: The ever-evolving field of medicine has highlighted the potential for ChatGPT as an assistive platform. However, its use in medical board examination preparation and completion remains unclear. Objective: This study aimed to evaluate the performance of a custom-modified version of ChatGPT-4, tailored with emergency medicine board examination preparatory materials (Anki flashcard deck), compared to its default version and previous iteration (3.5). The goal was to assess the accuracy of ChatGPT-4 answering board-style questions and its suitability as a tool to aid students and trainees in standardized examination preparation. Methods: A comparative analysis was conducted using a random selection of 598 questions from the Rosh In-Training Examination Question Bank. The subjects of the study included three versions of ChatGPT: the Default, a Custom, and ChatGPT-3.5. The accuracy, response length, medical discipline subgroups, and underlying causes of error were analyzed. Results: The Custom version did not demonstrate a significant improvement in accuracy over the Default version (P=.61), although both significantly outperformed ChatGPT-3.5 (P<.001). The Default version produced significantly longer responses than the Custom version, with the mean (SD) values being 1371 (444) and 929 (408), respectively (P<.001). Subgroup analysis revealed no significant difference in the performance across different medical subdisciplines between the versions (P>.05 in all cases). Both the versions of ChatGPT-4 had similar underlying error types (P>.05 in all cases) and had a 99% predicted probability of passing while ChatGPT-3.5 had an 85% probability. Conclusions: The findings suggest that while newer versions of ChatGPT exhibit improved performance in emergency medicine board examination preparation, specific enhancement with a comprehensive Anki flashcard deck on the topic does not significantly impact accuracy. The study highlights the potential of ChatGPT-4 as a tool for medical education, capable of providing accurate support across a wide range of topics in emergency medicine in its default form.",artificial intelligence; ChatGPT-4; medical education; emergency medicine; examination; examination preparation,WoS
10.18420/inf2024_123,Expanding Knowledge Graphs Through Text: Leveraging Large Language Models for Inductive Link Prediction,10.18420/inf2024_123,"Knowledge graphs (KG) play a crucial role for knowledge modelling in various domains such as web search, medical applications, or technical support, yet they are often incomplete. To mitigate this problem, knowledge graph completion (KGC) may be used to infer missing links of the graph. Taking it a step further, in an automated knowledge acquisition process, links for entirely new, unseen entities may be incorporated. This process is known as inductive link prediction (I-LP). Optionally, text as an external source of information is leveraged to infer the correct linkage of such entities. Depending on the context, this text either provides a comprehensive singular description of the entity or includes numerous incidental references to it. This paper presents a study that explores the application of LLAMA3 as a representative of the current generation of large language models (LLM) to I-LP. Through experimentation on popular benchmark datasets such as Wikidata5m, FB15k-237, WN18-RR, and IRT2, we evaluate the performance of LLMs for inserting new facts into a knowledge base, given textual references to the target object. These benchmarks, by design, exhibit significant variations in the quality of the associated text, as well as in the number of entities and links included. This paper explores several prompt formulations and studies whether pre-emptive retrieval of text helps. For automated link prediction, we implement the full cycle of prompt generation, answer processing, entity candidate lookup and finally link prediction. Our results show that LLM-based inductive link-prediction is outperformed by previously suggested models which fine-tune task-specific LM encoders. © 2025 Elsevier B.V., All rights reserved.",Inductive Link Prediction; Knowledge Graph Completion; Large Language Models; Prompting; Economic And Social Effects; Knowledge Acquisition; Modeling Languages; Prediction Models; Inductive Link; Inductive Link Prediction; Knowledge Graph Completion; Knowledge Graphs; Knowledge Model; Language Model; Large Language Model; Link Prediction; Prompting; Web Searches; Knowledge Graph,Scopus
10.1016/j.paid.2023.112307,Expanding the methodological toolbox: Machine-based item desirability ratings as an alternative to human-based ratings,10.1016/j.paid.2023.112307,"The accuracy of self-reported data in the social and behavioral sciences may be compromised by response biases such as socially desirable responding. Researchers and scale developers therefore obtain item desirability ratings, in order to maintain item neutrality, and parity with alternative options when creating forced-choice items. Gathering item desirability ratings from human judges can be time-consuming and costly, with no consistent guidelines with regard to required sample size and composition. However, recent advancements in natural language processing have yielded large language models (LLMs) with exceptional abilities to identify abstract semantic attributes in text. The presented research highlights the potential application of LLMs to estimate the desirability of items, as evidenced by the re-analysis of data from 14 distinct studies. Findings indicate a significant and strong correlation between human- and machine-rated item desirability of .80, across 521 items. Results furthermore showed that the proposed fine-tuning approach of LLMs results in predictions that explained 19 % more variance beyond that of sentiment analysis. These results demonstrate the feasibility of relying on machine-based item desirability ratings as a viable alternative to human-based ratings and contribute to the field of personality psychology by expanding the methodological toolbox available to researchers, scale developers, and practitioners. © 2023 Elsevier B.V., All rights reserved.",Artificial Intelligence; Item Desirability; Large Language Models; Natural Language Processing; Sentiment Analysis; Social Desirability Bias; Article; Artificial Intelligence; Feasibility Study; Human; Human Experiment; Natural Language Processing; Personality Psychology; Physician; Prediction; Sentiment Analysis; Social Desirability Bias,Scopus
2-s2.0-105010209602,FILTERED NOT MIXED: FILTERING-BASED ONLINE GATING FOR MIXTURE OF LARGE LANGUAGE MODELS,,"We propose MoE-F - a formalized mechanism for combining N pre-trained expert Large Language Models (LLMs) in online time-series prediction tasks. MoE-F adaptively forecasts the optimal weighting of LLM predictions at each time step by leveraging the conditional information in each expert's running performance, enabling the best combination of experts for the next step prediction. Diverging from static (learned) Mixture of Experts (MoE) methods, our approach employs time-adaptive stochastic filtering techniques to combine experts. By framing the expert selection problem as a finite state-space, continuous-time Hidden Markov model (HMM), we can leverage the Wonham-Shiryaev filter. Our approach first constructs N parallel filters corresponding to each N individual LLMs. Each filter proposes its best combination of LLMs, given the information that they have access to. Subsequently, the N filter outputs are optimally aggregated to maximize their robust predictive power, and this update is computed efficiently via a closed-form expression, thus generating our ensemble predictor. Our contributions are: (I) the MoE-F algorithm - deployable as a plug-and-play filtering harness over any heterogenous mixture of LLMs or specialized models, (II) theoretical optimality guarantees of the proposed filtering-based gating algorithm (via optimality guarantees for its parallel Bayesian filtering and its robust aggregation steps), and (III) empirical evaluation and ablative results using state of the art foundational and MoE LLMs on a real-world Financial Market Movement task based on streaming news where MoE-F attains a 17% absolute and 48.5% relative F1-score improvement over the best performing individual LLM expert. Further, we provide empirical evidence of substantial performance gains with MoE-F over specialized models in the long-horizon time-series forecasting domain using electricity-grid datasets. Supplementary materials available at: https://github.com/raeidsaqur/moe-f. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1109/CAIT64506.2024.10963060,FN-Agents: Analysis of Exchange Rate Volatility Prediction Based on Multi-Agent Systems,10.1109/cait64506.2024.10963060,"Exchange rate prediction is challenging. While large language models (LLMs) offer new approaches, their lack of domain-specific fine-tuning hinders effective assessment of factors influencing exchange rates. To address this, we propose FN-Agents - a framework integrating technologies such as vector databases, Retrieval-Augmented Generation(RAG), and self-reflection - to enable LLMs to better utilize external resources for exchange rate tasks. FN-Agents assists LLMs in acquiring expert knowledge, learning through self-reflection, building fine-tuned datasets, and ultimately generating interpretable core feature sets autonomously. Experiments demonstrate that FN-Agents excels in feature selection and enhances the predictive accuracy of mainstream models. © 2025 Elsevier B.V., All rights reserved.",Agents; Exchange Rate Forecasting; Experiential Learning; Privatized Knowledge; Exchange Rate Forecasting; Exchange Rate Volatilities; Exchange Rates; Exchange Rates Prediction; Experiential Learning; Language Model; Multiagent Systems (mass); Prediction-based; Privatized Knowledge; Self Reflection,Scopus
10.3390/app15158282,FairRAG: A Privacy-Preserving Framework for Fair Financial Decision-Making,10.3390/app15158282,"Customer churn prediction has become crucial for businesses, yet it poses significant challenges regarding privacy preservation and prediction accuracy. In this paper, we address two fundamental questions: (1) How can customer churn be effectively predicted while ensuring robust privacy protection of sensitive data? (2) How can large language models enhance churn prediction accuracy while maintaining data privacy? To address these questions, we propose FairRAG, a robust architecture that combines differential privacy, retrieval-augmented generation, and LLMs. Our approach leverages OPT-125M as the core language model along with a sentence transformer for semantic similarity matching while incorporating differential privacy mechanisms to generate synthetic training data. We evaluate FairRAG on two diverse datasets: Bank Churn and Telco Churn. The results demonstrate significant improvements over both traditional machine learning approaches and standalone LLMs, achieving accuracy improvements of up to 11% on the Bank Churn dataset and 12% on the Telco Churn dataset. These improvements were maintained when using differentially private synthetic data, thus indicating robust privacy and accuracy trade-offs. © 2025 Elsevier B.V., All rights reserved.",Algorithmic Fairness; Differential Privacy; Privacy-preserving Machine Learning; Retrieval-augmented Generation; Computational Linguistics; Decision Making; Differential Privacy; Forecasting; Learning Systems; Machine Learning; Privacy-preserving Techniques; Semantics; Algorithmic Fairness; Algorithmics; Differential Privacies; Financial Decisions; Language Model; Machine-learning; Prediction Accuracy; Privacy Preserving; Privacy-preserving Machine Learning; Retrieval-augmented Generation; Economic And Social Effects,Scopus
10.1016/j.jns.2024.123360,Faster and better than a physician?: Assessing diagnostic proficiency of ChatGPT in misdiagnosed individuals with neuromyelitis optica spectrum disorder,10.1016/j.jns.2024.123360,"Background: Neuromyelitis optica spectrum disorder (NMOSD) is a commonly misdiagnosed condition. Driven by cost-consciousness and technological fluency, distinct generations may gravitate towards healthcare alternatives, including artificial intelligence (AI) models, such as ChatGPT (Generative Pre-trained Transformer). Our objective was to evaluate the speed and accuracy of ChatGPT-3.5 (GPT-3.5) in the diagnosis of people with NMOSD (PwNMOSD) initially misdiagnosed. Methods: Misdiagnosed PwNMOSD were retrospectively identified with clinical symptoms and time line of medically related events processed through GPT-3.5. For each subject, seven digital derivatives representing different races, ethnicities, and sexes were created and processed identically to evaluate the impact of these variables on accuracy. Scoresheets were used to track diagnostic success and time to diagnosis. Diagnostic speed of GPT-3.5 was evaluated against physicians using a Cox proportional hazards model, clustered by subject. Logistical regression was used to estimate the diagnostic accuracy of GPT-3.5 compared with the estimated accuracy of physicians. Results: Clinical time lines for 68 individuals (59 female, 42 Black/African American, 13 White, 11 Hispanic, 2 Asian; mean age at first symptoms 34.4 years (y) (standard deviation = 15.5y)) were analyzed and 476 digital simulations created, yielding 544 conversations for analysis. The instantaneous probability of correct diagnosis was 70.65% less for physicians relative to GPT-3.5 within 240 days of symptom onset (p < 0.0001). The estimated probability of correct diagnosis for GPT-3.5 was 80.88% [95% CI = (76.35%, 99.81%)]. Conclusion: GPT-3.5 may be of value in recognizing NMOSD. However, the manner in which medical information is conveyed, combined with the potential for inaccuracies may result in unnecessary psychological stress. © 2024 Elsevier B.V., All rights reserved.",Chatgpt; Generation Z; Generative Ai; Misdiagnosis; Neuromyelitis Optica Spectrum Disorder; Aquaporin 4; Aquaporin 4; Article; Chatgpt; Cohort Analysis; Controlled Study; Data Processing; Demographics; Diagnosis Time; Diagnostic Accuracy; Diagnostic Error; Ethnic Difference; Female; Hispanic; Human; Major Clinical Study; Male; Myelooptic Neuropathy; Neuroimmunology; Nuclear Magnetic Resonance Imaging; Physician; Proportional Hazards Model; Retrospective Study; Sex Difference; Adolescent; Adult; Artificial Intelligence; Diagnosis; Middle Aged; Time Factor; Young Adult; Adolescent; Adult; Artificial Intelligence; Diagnostic Errors; Female; Humans; Male; Middle Aged; Neuromyelitis Optica; Physicians; Retrospective Studies; Time Factors; Young Adult,Scopus
10.3390/ai5040096,Feasibility of GPT-3.5 versus Machine Learning for Automated Surgical Decision-Making Determination: A Multicenter Study on Suspected Appendicitis,10.3390/ai5040096,"Background: Nonsurgical treatment of uncomplicated appendicitis is a reasonable option in many cases despite the sparsity of robust, easy access, externally validated, and multimodally informed clinical decision support systems (CDSSs). Developed by OpenAI, the Generative Pre-trained Transformer 3.5 model (GPT-3) may provide enhanced decision support for surgeons in less certain appendicitis cases or those posing a higher risk for (relative) operative contra-indications. Our objective was to determine whether GPT-3.5, when provided high-throughput clinical, laboratory, and radiological text-based information, will come to clinical decisions similar to those of a machine learning model and a board-certified surgeon (reference standard) in decision-making for appendectomy versus conservative treatment. Methods: In this cohort study, we randomly collected patients presenting at the emergency department (ED) of two German hospitals (GFO, Troisdorf, and University Hospital Cologne) with right abdominal pain between October 2022 and October 2023. Statistical analysis was performed using R, version 3.6.2, on RStudio, version 2023.03.0 + 386. Overall agreement between the GPT-3.5 output and the reference standard was assessed by means of inter-observer kappa values as well as accuracy, sensitivity, specificity, and positive and negative predictive values with the “Caret” and “irr” packages. Statistical significance was defined as p < 0.05. Results: There was agreement between the surgeon’s decision and GPT-3.5 in 102 of 113 cases, and all cases where the surgeon decided upon conservative treatment were correctly classified by GPT-3.5. The estimated model training accuracy was 83.3% (95% CI: 74.0, 90.4), while the validation accuracy for the model was 87.0% (95% CI: 66.4, 97.2). This is in comparison to the GPT-3.5 accuracy of 90.3% (95% CI: 83.2, 95.0), which did not perform significantly better in comparison to the machine learning model (p = 0.21). Conclusions: This study, the first study of the “intended use” of GPT-3.5 for surgical treatment to our knowledge, comparing surgical decision-making versus an algorithm found a high degree of agreement between board-certified surgeons and GPT-3.5 for surgical decision-making in patients presenting to the emergency department with lower abdominal pain. © 2024 Elsevier B.V., All rights reserved.",Appendectomy; Artificial Intelligence; Surgical Decision-making,Scopus
2-s2.0-105000468879,FinBen: A Holistic Financial Benchmark for Large Language Models,,"LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 42 datasets spanning 24 financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community2, with results shared and updated regularly on the Open Financial LLM Leaderboard. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1016/j.pacfin.2024.102632,Finance-specific large language models: Advancing sentiment analysis and return prediction with LLaMA 2,10.1016/j.pacfin.2024.102632,"In this study, we develop an AI-driven summarization process for lengthy financial texts to improve our self-trained, finance-specific LLaMA-2 model. This approach allows for precise sentiment analysis, leading to more accurate return predictions on disclosures in the Management Discussion and Analysis sections of 10-K filings. Empirical results indicate that trading strategies based on LLaMA-2 sentiments produce significantly higher buy-and-hold returns (BHRs) compared to those derived from FinBERT (Huang et al., 2023) and traditional models. Furthermore, LLaMA-2 sentiment signals show a strong correlation with cumulative abnormal returns (CARs) and surpass traditional methods in predictive accuracy. The summarization process also enhances traditional models, generating significantly higher BHRs with summarized texts than with full texts. Both BHR and CAR results in our approach show robustness during periods of financial turbulence. These findings underscore the value of generative AI in finance and set a new standard for textual analysis. © 2024 Elsevier B.V., All rights reserved.",Ai-driven Summarization; Disclosures; Generative Ai; Large Language Model; Sentiment Analysis; Textual Analysis,Scopus
10.18653/v1/2024.emnlp-industry.77,Fine-Tuning Large Language Models for Stock Return Prediction Using Newsflow,10.18653/v1/2024.emnlp-industry.77,"Large language models (LLMs) and their fine-tuning techniques have demonstrated superior performance in various language understanding and generation tasks. This paper explores fine-tuning LLMs for predicting stock returns with financial newsflow. Return prediction is fundamental for quantitative investing tasks like portfolio construction and optimization. We formulate the model to include a text representation and forecasting modules. We propose to compare the encoder-only and decoder-only LLMs, considering they generate text representations in distinct ways. The impact of these different representations on return forecasting remains an open question. Meanwhile, we compare two simple methods of integrating LLMs’ token-level representations into the forecasting module. The experiments on real investment universes reveal that: (1) aggregated representations from LLMs’ token-level embeddings generally produce return predictions that enhance the performance of long-only and long-short portfolios; (2) in the relatively large investment universe, the decoder LLMs-based prediction model leads to stronger portfolios, whereas in the small universes, there are no consistent winners; (3) return predictions derived from LLMs’ text representations are a strong signal for portfolio construction, outperforming conventional sentiment scores. These findings suggest the potential of LLM fine-tuning for enhancing return prediction-based portfolio construction. © 2025 Elsevier B.V., All rights reserved.",Financial Markets; Investments; Sales; Fine Tuning; Language Generation; Language Model; Language Understanding; Optimisations; Performance; Simple Method; Stock Return Predictions; Stock Returns; Text Representation; Prediction Models,Scopus
10.1080/23311975.2025.2487219,Fine-tuning transformer models for M&A target prediction in the U.S. ENERGY sector,10.1080/23311975.2025.2487219,"This study explores the application of transformer models directly for classification in predicting mergers and acquisitions (M&A) targets within the U.S. energy sector. The primary objective is to evaluate the capability and performance of various transformer-based models in directly predicting M&A target companies, while the secondary objective investigates the relationship between target companies and renewable energy terminology in their annual reports. We present a novel approach to predicting M&A targets by utilizing cutting-edge Natural Language Processing (NLP) techniques, such as fine-tuned transformer LLMs (Large Language Models) for direct classification. We analyze textual data from 200 publicly-listed US energy companies’ SEC-filings and employ FinBERT, ALBERT, and GPT-3-babage-002 as predictive models of M&A targets. We provide empirical evidence on LLMs’ capability in the direct classification of M&A target companies, with FinBERT utilizing oversampling, being the top-performing model due to its high precision and minimized false positives, critical for precise financial decision-making. Additionally, while the study revealed key differences in target and non-target report characteristics, it finds no significant evidence that M&A target companies use more renewable energy-related terminology. It is the first paper applying fine-tuned transformer-LLMs to predict M&A targets, effectively showcasing their capability for this task of direct classification as predictive models. © 2025 Elsevier B.V., All rights reserved.","Algorithms & Complexity; Artificial Intelligence; Business, Management And Accounting; Computer Science (general); Corporate Finance; Environmental Economics; Finance; Green M&a; Large Language Models (llm); Mergers And Acquisitions (m&a); Natural Language Processing (nlp); Renewable Energy; Takeover Target Prediction; Transformer Models",Scopus
WOS:000503370500022,"First-Principles-Based Force Field for 2,6-Diamino-3,5-dinitropyrazine-1-oxide (LLM-105)",10.1021/acsomega.9b02410,"2,6-Diamino-3,5-dinitropyrazine-1-oxide (LLM-105) is a highly promising energetic material (EM) with high safety. Understanding its microscopic response mechanisms within the external stimulus is meaningful for the design of EMs. In order to comprehend the complicated phenomena, it is necessary to employ molecular simulation methods to investigate the response mechanisms with the force field (FF) at an atomic level. In this work, we developed a tailored FF for LLM-105 based on first-principles calculations. The validity of the FF was evaluated by molecular dynamics simulations. The structural parameters of LLM-105 predicted by FF are in good agreement with the experimental values, such as lattice constant, bond length, bond angle, dihedral angle and center of mass, and so forth. Moreover, the FF possesses good performance to describe the structural response on pressure accurately. In general, our work not only builds a balanced FF in gas and condensed phases, but also provides a useful tool to study the properties about LLM-105 at a large scale, which is helpful to improve the understanding about the balance between energy and safety in EMs.",,WoS
10.1007/978-981-97-0837-6_4,Forecasting Chinese Overnight Stock Index Movement Using Large Language Models with Market Summary,10.1007/978-981-97-0837-6_4,"Forecasting financial market movement constitutes a complex and pivotal research area within the realm of Financial Technology (Fintech). In this work, we investigate the ability of large language models to predict Chinese overnight stock index movement, utilizing market summary gleaned from news media sources. We fine-tune various pre-trained models to compare the performance with that of Generative Pre-training Transformer (GPT) models, specifically GPT-3.5 and GPT-4, as provided by OpenAI. The empirical findings underscore that the fine-tuned pre-trained models, characterized by fewer parameters and more straightforward architectures, surpass the esteemed GPT-3.5 and GPT-4 models in predictive metrics of accuracy and f1. All fine-tuned models are publicly available on the huggingface platform (https://huggingface.co/hw2942). © 2024 Elsevier B.V., All rights reserved.",Bert; Forecasting Overnight Stock Index Movement; Gpt; Large Language Models; Commerce; Computational Linguistics; Financial Markets; Bert; Forecasting Overnight Stock Index Movement; Generative Pre-training Transformer; Language Model; Large Language Model; News Media; Performance; Pre-training; Research Areas; Stock Indices; Forecasting,Scopus
2-s2.0-85217739767,Forecasting Credit Ratings: A Case Study where Traditional Methods Outperform Generative LLMs,,"Large Language Models (LLMs) have been shown to perform well for many downstream tasks. Transfer learning can enable LLMs to acquire skills that were not targeted during pretraining. In financial contexts, LLMs can sometimes beat well-established benchmarks. This paper investigates how well LLMs perform at forecasting corporate credit ratings. We show that while LLMs are very good at encoding textual information, traditional methods are still very competitive when it comes to encoding numeric and multimodal data. For our task, current LLMs perform worse than a more traditional XGBoost architecture that combines fundamental and macroeconomic data with high-density text-based embedding features. We investigate the degree to which the text encoding methodology affects performance and interpretability. The dataset reconstruction and model code from this paper is provided1. © 2025 Elsevier B.V., All rights reserved.",Benchmarking; Natural Language Processing Systems; Transfer Learning; Case-studies; Corporate Credit Ratings; Credit Ratings; Down-stream; Encodings; Language Model; Multi-modal; Pre-training; Textual Information; Computational Linguistics,Scopus
WOS:001575273300007,Forecasting SCFI using a deep learning model based on Time-LLM and news data,10.5351/kjas.2025.38.4.513,"Today, artificial intelligence (AI) is advancing rapidly and is now being utilized across various sectors of society. One of the most successful AI technologies is the Large Language Model (LLM), and recently, research on time series forecasting using LLMs has been actively conducted. Another emerging trend in time series forecasting is the development of leading indexes that can explain rapid changes in economic and social conditions in advance, using text data from news articles and social media. In this study, we propose a deep learning model that leverages LLMs and news data to forecast the Shanghai Containerized Freight Index (SCFI), addressing key recent trends in time series forecasting. SCFI reflects the volatility of container shipping rates and serves as a crucial indicator for assessing market conditions in the shipping industry. Since this index plays a significant role in the decision-making processes of logistics companies and shippers responding to shipping fluctuations, accurate forecasting of SCFI is of great importance. The proposed deep learning model demonstrates relatively higher predictive accuracy compared to competing models, even under rapidly changing SCFI conditions. As a result, it is expected to make a significant contribution to the maritime industry by enhancing decision-making processes and improving adaptability to market fluctuations.",large language model; time series; news data; Shanghai containerized freight index,WoS
10.1109/ISCMI63661.2024.10851549,Foreign Exchange Rate Forecast by a Large Language Model Integrated with Trend Description,10.1109/iscmi63661.2024.10851549,"In this paper, we propose a large language model to forecast the direction of change in a foreign exchange rate. The input of the proposed model is textual information as a prompt, whereas that of conventional forecast models is numerical information. A recent trend in the exchange rate is added to input textual information to enhance forecast accuracy. GPT-2 is adopted as our large language model and is fine-tuned using training data. The effectiveness of the proposed model is empirically examined using actual data. © 2025 Elsevier B.V., All rights reserved.",Deep Learning; Finance; Large Language Model; Machine Learning; Time Series; Adversarial Machine Learning; Contrastive Learning; Deep Learning; Forecast Models; Foreign Exchange Rates; Language Model; Large Language Model; Machine-learning; Numerical Information; Recent Trends; Textual Information; Times Series; Prediction Models,Scopus
10.1080/14765284.2023.2245279,From fiction to fact: the growing role of generative AI in business and finance,10.1080/14765284.2023.2245279,"Generative Artificial Intelligence (AI), such as ChatGPT by OpenAI, has revolutionized the business world, with benefits including improved accessibility, efficiency, and cost reduction. This article reviews recent developments of generative AI in business and finance, summarizes its practical applications, provides examples of the latest generative AI tools, and demonstrates that generative AI can revolutionize data analysis in industry and academia. To test the ability of generative AI to support decision-making in financial markets, we use the ChatGPT to capture corporate sentiments towards environmental policy by inputting text extracted from corporate financial statements. Our results demonstrate that the sentiment scores generated by ChatGPT can predict firms’ risk-management capabilities and stock return performance. This study also highlights the potential challenges and limitations associated with generative AI. Finally, we propose several questions for future research at the intersection of generative AI with business and finance. © 2023 Elsevier B.V., All rights reserved.",Chatgpt; Generative Ai; Natural Language Processing; Practical Applications; Sentiment Analysis,Scopus
10.1007/s10844-025-00971-3,From news to trends: a financial time series forecasting framework with LLM-driven news sentiment analysis and selective state spaces,10.1007/s10844-025-00971-3,"Stock price prediction is inherently challenging due to market volatility and the influence of external factors. Traditional forecasting methods primarily rely on historical price data, limiting their ability to capture market sentiment embedded in financial news. To address this limitation, we propose MambaMoE, a novel model that integrates historical stock prices with sentiment information extracted from financial news. Specifically, we fine-tune a DeepSeek-based large language model (LLM) for financial sentiment classification and incorporate the extracted sentiment information into our predictive framework. At the core of our approach is MambaMoE layer, which leverages the efficiency of state space models (SSMs) to model long-range dependencies while maintaining linear computational complexity, making it well-suited for financial time series forecasting. Additionally, the Mixture of Experts (MoE) mechanism improves the model’s ability to capture diverse market behaviors by dynamically selecting specialized experts based on stock data patterns. Experimental results demonstrate that MambaMoE outperforms LSTM-based models by 23.7% and Transformer-based models by 6.3%, highlighting its superior performance in short-term stock prediction. © 2025 Elsevier B.V., All rights reserved.",Llm; Mamba; Sentiment Analysis; Time Series Forecasting; Classification (of Information); Commerce; Costs; Electronic Trading; Financial Markets; Forecasting; State Space Methods; Time Series; Time Series Analysis; Financial News; Financial Time Series Forecasting; Language Model; Large Language Model; Mamba; Model-driven; Sentiment Analysis; State-space; Stock Price Prediction; Time Series Forecasting,Scopus
10.1007/s41870-025-02622-w,From text to trade: harnessing the potential of generative AI for investor sentiment analysis in financial markets through large language models,10.1007/s41870-025-02622-w,"This study explores the integration of generative artificial intelligence (AI) into financial sentiment analysis, focusing on enhancing market behavior predictions using advanced large language models (LLMs). A novel sentiment analysis framework is developed, leveraging cutting-edge LLMs and generative AI for data augmentation. The approach incorporates optimized word embeddings and fine-tuning techniques such as Few-shot Learning and Low-Rank Adaptation (LoRA) to handle the linguistic complexities of financial discourse. The framework is evaluated using five performance metrics, demonstrating improved accuracy and efficiency. These findings highlight the transformative potential of LLMs in financial decision-making and sentiment-driven trading strategies. © 2025 Elsevier B.V., All rights reserved.",Financial Markets; Generative Ai; Large Language Models; Sentiment Analysis,Scopus
2-s2.0-85203805495,GILOT: Interpreting Generative Language Models via Optimal Transport,,"While large language models (LLMs) surge with the rise of generative AI, algorithms to explain LLMs highly desire. Existing feature attribution methods adequate for discriminative language models like BERT often fail to deliver faithful explanations for LLMs, primarily due to two issues: (1) For every specific prediction, the LLM outputs a probability distribution over the vocabulary-a large number of tokens with unequal semantic distance; (2) As an autoregressive language model, the LLM handles input tokens while generating a sequence of probability distributions of various tokens. To address above two challenges, this work proposes GILOT that leverages Optimal Transport approach to measure the distributional change of all possible generated sequences upon the absence of every input token, while taking into account the tokens' similarity, so as to faithfully estimate feature attribution for LLMs. We have carried out extensive experiments on top of Llama families and their fine-tuned derivatives across various scales to validate the effectiveness of GILOT for estimating the input attributions. The results show that GILOT outperforms existing solutions on a number of faithfulness metrics under fair comparison settings. Source code is publicly available at https://github.com/holyseven/GiLOT. © 2024 Elsevier B.V., All rights reserved.",Generative Adversarial Networks; Ai Algorithms; Auto-regressive; Discriminative Language Models; Language Model; Model Outputs; Optimal Transport; Probability: Distributions; Semantic Distance; Source Codes; Semantics,Scopus
2-s2.0-85217771948,GMU-MU at the Financial Misinformation Detection Challenge Task: Exploring LLMs for Financial Claim Verification,,"This paper describes the team GMU-MU submission to the Financial Misinformation Detection challenge. The goal of this challenge is to identify financial misinformation and generate explanations justifying the predictions by developing or adapting LLMs. The participants were provided with a dataset of financial claims that were categorized into six financial domain categories. We experiment with the Llama model using two approaches; instruction-tuning the model with the training dataset, and a prompting approach that directly evaluates the off-the-shelf model. Our best system was placed 5th among the 12 systems, achieving an overall evaluation score of 0.6682. © 2025 Elsevier B.V., All rights reserved.",Contrastive Learning; Decentralized Finance; Finance; Financial Domains; Training Dataset; Computational Linguistics,Scopus
10.1126/science.adj0998,GPTs are GPTs: Labor market impact potential of LLMs. Research is needed to estimate how jobs may be affected,10.1126/science.adj0998,"We propose a framework for evaluating the potential impacts of large-language models (LLMs) and associated technologies on work by considering their relevance to the tasks workers perform in their jobs. By applying this framework (with both humans and using an LLM), we estimate that roughly 1.8% of jobs could have over half their tasks affected by LLMs with simple interfaces and general training. When accounting for current and likely future software developments that complement LLM capabilities, this share jumps to just over 46% of jobs. The collective attributes of LLMs such as generative pretrained transformers (GPTs) strongly suggest that they possess key characteristics of other “GPTs,” general-purpose technologies (1, 2). Our research highlights the need for robust societal evaluations and policy measures to address potential effects of LLMs and complementary technologies on labor markets. We consider the progress of LLMs’ capabilities and the potential breadth of the complementary technologies that they spawn, underscoring that maximizing the impact of LLMs requires their integration within broader systems (3–5). The rubric that we develop [see supplementary materials (SM) section 3.1.2] defines task exposure to LLMs, in the spirit of prior work on quantifying exposure to machine learning (6–8). Following prior literature, we use the concept of exposure as a proxy for potential economic impact, irrespective of labor-augmentation or displacement effects (see section 2 of the SM for further discussion of the literature). Our approach differs from the broader scope of complementary innovations in general-purpose technology discussions, focusing more narrowly on advanced software capabilities than on the potential for business process reengineering, new intangible assets creation, or workforce retraining. General-purpose technologies such as electricity or computing historically have had far-reaching effects that took decades to fully materialize. With evidence of the general-purpose technology potential of LLMs, we urge caution in making long-term predictions while offering an outline of where work might change. © 2025 Elsevier B.V., All rights reserved.",Business; Displacement; Electricity Generation; Electricity Industry; Machine Learning; Market Conditions; Software; Article; Economic Inequality; Exploratory Research; Finance; Generative Pretrained Transformer; Human; Labor; Large Language Model; Leadership; Long Term Exposure; Occupational Exposure; Pharmacist; Stakeholder Engagement; Training; Unemployment Insurance; Workflow; Article; Controlled Study; Job Market,Scopus
10.1007/s11042-022-11908-1,Gated three-tower transformer for text-driven stock market prediction,10.1007/s11042-022-11908-1,"Effective stock market prediction can significantly assist individual and institutional investors to make better trading decisions and help government stabilize the market. Therefore, a variety of methods have been proposed to tackle the issue of stock market prediction recently. However, it is still quite challenging to effectively extract the correlations and temporal information from multivariate time series of market data and integrate various kinds of features as well as auxiliary information, which is important for improving the performance of stock market prediction. This paper proposes an entirely Transformer based model, namely Gated Three-Tower Transformer (GT3), to incorporate numerical market information and social text information for accurate stock market prediction. Firstly, we devise a Channel-Wise Tower Encoder (CWTE) to capture the channel-wise features from transposed numerical data embeddings. Secondly, we design a Shifted Window Tower Encoder (SWTE) with Multi-Temporal Aggregation to extract and aggregate the multi-scale temporal features from the original numerical data embeddings. Then we adopt the encoder of vanilla Transformer as a Text Tower Encoder (TTE) to obtain the high-level textual features. Furthermore, we design a Cross-Tower Attention mechanism to assist the model to learn the trend-relevant significance of each daily text representation by leveraging the temporal features from SWTE. Finally, we unify CWTE, SWTE, and TTE as the GT3 model through a self-adaptive gate layer to perform end-to-end text-driven stock market prediction by fusing three types of features effectively and efficiently. Extensive experimental results on a real-world dataset show that the proposed model outperforms state-of-the-art baselines. © 2022 Elsevier B.V., All rights reserved.",Attention Mechanism; Feature Fusion; Stock Market Prediction; Text-driven; Transformer; Commerce; Data Mining; Embeddings; Financial Markets; Forecasting; Investments; Towers; Attention Mechanisms; Data Embedding; Features Fusions; Institutional Investors; Numerical Data; Stock Market Prediction; Temporal Features; Temporal Information; Text-driven; Transformer; Signal Encoding,Scopus
2-s2.0-105014724824,Generalised Probabilistic Modelling and Improved Uncertainty Estimation in Comparative LLM-as-a-judge,,"This paper explores generalised probabilistic modelling and uncertainty estimation in comparative LLM-as-a-judge frameworks. We show that existing Product-of-Experts methods are specific cases of a broader framework, enabling diverse modelling options. Furthermore, we propose improved uncertainty estimates for individual comparisons, enabling more efficient selection and achieving strong performance with fewer evaluations. We also introduce a method for estimating overall ranking uncertainty. Finally, we demonstrate that combining absolute and comparative scoring improves performance. Experiments show that the specific expert model has a limited impact on final rankings but our proposed uncertainty estimates, especially the probability of reordering, significantly improve the efficiency of systems reducing the number of needed comparisons by ∼ 50%. Furthermore, ranking-level uncertainty metrics can be used to identify low-performing predictions, where the nature of the probabilistic model has a notable impact on the quality of the overall uncertainty. © 2025 Elsevier B.V., All rights reserved.",Prediction Models; Expert Modeling; Improve Performance; Model Estimation; Performance; Probabilistic Models; Probabilistic Uncertainty; Product Of Experts; Uncertainty; Uncertainty Estimates; Uncertainty Estimation; Uncertainty Analysis,Scopus
10.1016/j.technovation.2025.103313,Generative artificial intelligence augmenting SME financial management,10.1016/j.technovation.2025.103313,"This study investigates the potential for entrepreneurs to leverage advances in technological innovation, specifically generative Artificial Intelligence (AI), to build management capability to mitigate business and financial risks. Drawing on theories of Technology Affordances and Constraints and the Resource-Based View (RBV) of the firm, recognising that small and medium-sized enterprises (SMEs) are inherently resource-constrained. We examine how AI-generated financial diagnostics can empower SMEs by generating accessible, real-time analysis and insights, thus bolstering the management function and increasing chances of survival and growth. Using a dataset of 1,150 UK SMEs spanning eight years of financial statements, we test a large language model (LLM) prediction assessment and analyse the potential for SMEs to utilise the technology, notwithstanding enterprise-specific constraints. We conclude that AI may be a very effective tool for smaller enterprises to augment the financial management function, although its efficacy hinges on organisational readiness, competence in interpreting data, and the will to act on automated red-flag alerts. These findings offer practical guidance for SMEs seeking to enhance their financial management processes in today's digital era. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence; Digital Technologies; Financial Management; Going Concern; Predictive Modelling; Smes; Finance; Information Management; Business Risks; Digital Technologies; Enterprise's Financial Managements; Financial Managements; Going Concern; Management Capabilities; Management Functions; Predictive Models; Small And Medium-sized Enterprise; Technological Innovation; Artificial Intelligence,Scopus
WOS:001114834600001,Genetic dissection of the degradation pathways for the mycotoxin fusaric acid in Burkholderia ambifaria T16,10.1128/aem.00630-23,"Fusaric acid (FA) is a mycotoxin produced by several Fusarium species. Burkholderia ambifaria T16 is a rhizosphere bacterium, able to use FA as sole nitrogen, carbon, and energy source. By screening a transposon insertional library, combined with proteomic analysis, genes and enzymes involved in the microbial degradation of FA were identified for the first time. A functional 2-methylcitrate cycle, an anaplerotic pathway where propionyl-coenzyme A (CoA) is converted to pyruvate and succinate, was shown to be essential for growth in the presence of FA. The proteomic profile of B. ambifaria T16 showed that more than 50 enzymes (including those belonging to the 2-methylcitrate cycle, fatty acid metabolism, valine catabolism, and flavin biosynthesis) were significantly more abundant when growing on FA than on citrate. Flavin mononucleotide (FMN)-dependent luciferases like monooxygenase (LLM) are shown to catalyze the pyridine-ring cleavage reaction of several N-heterocyclic compounds. Deletion of a gene encoding a predicted LLM enzyme that was highly upregulated during growth on FA, completely abolished the capability of B. ambifaria T16 to grow with this mycotoxin as sole nitrogen, carbon, and energy source. Re-introduction of the wild type gene was able to restore growth. The mentioned gene is part of a gene cluster of unknown function that we termed fua, due to its probable role in fusaric acid catabolism. Our results suggest that the LLM encoded in the fua cluster catalyzes the pyridine-ring opening reaction during FA degradation, and that propionyl-CoA is one of the intermediates of FA catabolism in B. ambifaria T16. IMPORTANCE Fusaric acid (FA) is an important virulence factor produced by several Fusarium species. These fungi are responsible for wilt and rot diseases in a diverse range of crops. FA is toxic for animals, humans and soil-borne microorganisms. This mycotoxin reduces the survival and competition abilities of bacterial species able to antagonize Fusarium spp., due to its negative effects on viability and the production of antibiotics effective against these fungi. FA biodegradation is not a common characteristic among bacteria, and the determinants of FA catabolism have not been identified so far in any microorganism. In this study, we identified genes, enzymes, and metabolic pathways involved in the degradation of FA in the soil bacterium Burkholderia ambifaria T16. Our results provide insights into the catabolism of a pyridine-derivative involved in plant pathogenesis by a rhizosphere bacterium.",Burkholderia ambifaria T16; fusaric acid; two-component flavin-dependent monooxygenase; 2-methylcitrate cycle; detoxification; catabolism,WoS
10.1109/CAI64502.2025.00032,Graph LLM-Based Portfolio Management Algorithm,10.1109/cai64502.2025.00032,"This paper explores the integration of large language models (LLMs) with graph-based financial networks for quantitative trading. By leveraging GPT-3 for stock network return predictions, we develop a Graph-LLM trading strategy. Experimental results demonstrate that the proposed strategy achieves lower volatility and more stable performance than traditional baselines. Our findings highlight the potential of combining LLMs with financial complex networks to enhance quantitative trading strategies. © 2025 Elsevier B.V., All rights reserved.",Financial Complex Networks; Graph-based Trading Strategies; Large Language Models; Quantitative Trading; Commerce; Decentralized Finance; Electronic Trading; Financial Markets; Graph Algorithms; Graphic Methods; Investments; Financial Complex Network; Financial Networks; Graph-based; Graph-based Trading Strategy; Language Model; Large Language Model; Model-based Opc; Portfolio Managements; Quantitative Trading; Trading Strategies; Complex Networks,Scopus
WOS:001378237107004,Hallucination Detection for Generative Large Language Models by Bayesian Sequential Estimation,,"Large Language Models (LLMs) have made remarkable advancements in the field of natural language generation. However, the propensity of LLMs to generate inaccurate or non-factual content, termed ""hallucinations"", remains a significant challenge. Current hallucination detection methods often necessitate the retrieval of great numbers of relevant evidence, thereby increasing response times. We introduce a unique framework that leverages statistical decision theory and Bayesian sequential analysis to optimize the trade-off between costs and benefits during the hallucination detection process. This approach does not require a predetermined number of observations. Instead, the analysis proceeds in a sequential manner, enabling an expeditious decision towards ""belief"" or ""disbelief"" through a stop-or-continue strategy. Extensive experiments reveal that this novel framework surpasses existing methods in both efficiency and precision of hallucination detection. Furthermore, it requires fewer retrieval steps on average, thus decreasing response times.",,WoS
10.1016/j.bar.2024.101507,Harnessing ChatGPT for predictive financial factor generation: A new frontier in financial analysis and forecasting,10.1016/j.bar.2024.101507,"The search for predictive financial factors in stock pricing of companies has long been a key focus in accounting and finance, but traditional methods often require complex, subjective inputs. This paper introduces a method using ChatGPT-4 to generate financial factors based on the structure of financial statements and key variables, eliminating the need for numerical data. Leveraging GPT's natural language processing capabilities and extensive knowledge base, our approach efficiently generates factors that are highly predictive of future returns and exhibit robustness over time, unaffected by variations in different conversational windows. Regression analysis demonstrates that these factors cannot be linearly explained by traditional financial factors. This paper highlights AI's potential in revolutionizing financial analysis and decision-making. © 2024 Elsevier B.V., All rights reserved.",Chatgpt; Company Performance Forecasting; Financial Data Analysis; Financial Factor Generation; Large Language Model,Scopus
10.1109/DOCS63458.2024.10704454,Harnessing Earnings Reports for Stock Predictions: A QLoRA-Enhanced LLM Approach,10.1109/docs63458.2024.10704454,"Accurate stock market predictions following earnings reports are crucial for investors. Traditional methods, particularly classical machine learning models, struggle with these predictions because they cannot effectively process and interpret extensive textual data contained in earnings reports and often overlook nuances that influence market movements. This paper introduces an advanced approach by employing Large Language Models (LLMs) instruction fine-tuned with a novel combination of instruction-based techniques and quantized low-rank adaptation (QLoRA) compression. Our methodology integrates 'base factors', such as financial metric growth and earnings transcripts, with 'external factors', including recent market indices performances and analyst grades, to create a rich, supervised dataset. This comprehensive dataset enables our models to achieve superior predictive performance in terms of accuracy, weighted Fl, and Matthews correlation coefficient (M CC), especially evident in the comparison with benchmarks such as GPT-4. We specifically highlight the efficacy of the llama-3-8b-Instruct-4bit model, which showcases significant improvements over baseline models. The paper also discusses the potential of expanding the output capabilities to include a 'Hold''''' option and extending the prediction horizon, aiming to accommodate various investment styles and time frames. This study not only demonstrates the power of integrating cutting-edge AI with fine-tuned financial data but also paves the way for future research in enhancing AI -driven financial analysis tools. © 2024 Elsevier B.V., All rights reserved.",Instruction Fine Tuning; Large Language Model; Quantized Low-rank Adaptation; Adversarial Machine Learning; Benchmarking; Contrastive Learning; Decentralized Finance; Financial Markets; Investments; Prediction Models; Supervised Learning; Fine Tuning; Instruction Fine Tuning; Language Model; Large Language Model; Machine Learning Models; Modeling Approach; Quantized Low-rank Adaptation; Stock Market Prediction; Stock Predictions; Textual Data; Earnings,Scopus
10.18653/v1/2023.emnlp-industry.69,Harnessing LLMs for Temporal Data - A Study on Explainable Financial Time Series Forecasting,10.18653/v1/2023.emnlp-industry.69,"Applying machine learning to financial time series has been an active area of industrial research enabling innovation in market insights, risk management, strategic decision-making, and policy formation. This paper explores the novel use of Large Language Models (LLMs) for explainable financial time series forecasting, addressing challenges in cross-sequence reasoning, multi-modal data integration, and result interpretation that are inherent in traditional approaches. Focusing on NASDAQ-100 stocks, we utilize public historical stock data, company metadata, and economic/financial news. Our experiments employ GPT-4 for zero-shot/few-shot inference and Open LLaMA for instruction-based fine-tuning. The study demonstrates LLMs' ability to generate well-reasoned decisions by leveraging cross-sequence information and extracting insights from text and price time series. We show that our LLM-based approach outperforms classic ARMA-GARCH and gradient-boosting tree models. Furthermore, fine-tuned public LLMs, such as Open-LLaMA, can generate reasonable and explainable forecasts, although they underperform compared to GPT-4. © 2025 Elsevier B.V., All rights reserved.",Data Integration; Decision Making; Financial Data Processing; Financial Markets; Forecasting; Industrial Research; Time Series; Active Area; Decision Policy; Financial Time Series; Financial Time Series Forecasting; Language Model; Machine-learning; Policy Formation; Risks Management; Strategic Decision Making; Temporal Data; Risk Management,Scopus
10.3390/en18071813,"Heating, Ventilation, and Air Conditioning (HVAC) Temperature and Humidity Control Optimization Based on Large Language Models (LLMs)",10.3390/en18071813,"Heating, Ventilation, and Air Conditioning (HVAC) systems primarily consist of pre-cooling air handling units (PAUs), air handling units (AHUs), and air ducts. Existing HVAC control methods, such as Proportional–Integral–Derivative (PID) control or Model Predictive Control (MPC), face limitations in understanding high-level information, handling rare events, and optimizing control decisions. Therefore, to address the various challenges in temperature and humidity control, a more sophisticated control approach is required to make high-level decisions and coordinate the operation of HVAC components. This paper utilizes Large Language Models (LLMs) as a core component for interpreting complex operational scenarios and making high-level decisions. A chain-of-thought mechanism is designed to enable comprehensive reasoning through LLMs, and an algorithm is developed to convert LLM decisions into executable HVAC control commands. This approach leverages adaptive guidance through parameter matrices to seamlessly integrate LLMs with underlying MPC controllers. Simulated experimental results demonstrate that the improved control strategy, optimized through LLM-enhanced Model Predictive Control (MPC), significantly enhances the energy efficiency and stability of HVAC system control. During the summer conditions, energy consumption is reduced by 33.3% compared to the on–off control strategy and by 6.7% relative to the conventional low-level MPC strategy. Additionally, during the system startup phase, energy consumption is slightly reduced by approximately 17.1% compared to the on–off control strategy. Moreover, the proposed method achieves superior temperature stability, with the mean squared error (MSE) reduced by approximately 35% compared to MPC and by 45% relative to on–off control. © 2025 Elsevier B.V., All rights reserved.",Hvac; Large Language Models; Model Predictive Control; Adaptive Control Systems; Air Conditioning Ducts; Hvac; Problem Oriented Languages; Three Term Control Systems; Ventilation Ducts; Air Conditioning Controls; Air-handling Unit; Conditioning Systems; Control Strategies; Heating Ventilation And Air Conditioning; Language Model; Large Language Model; Model-predictive Control; On/off Control; Temperature And Humidity Control; Predictive Control Systems,Scopus
WOS:001386336900004,Higher education students' trust and use of ChatGPT: empirical evidence,10.1504/ijtel.2024.10061581,"This paper combines a modified version of the unified theory of acceptance and use of technology (UATAU) with the expectancy value theory (EVT) to examine the variables that influence higher education students' trust and use of ChatGPT. The quantitative method was used, with a structured questionnaire developed to collect data from respondents, which was then analysed using Smart PLS 4. According to the findings, perceived mobility influenced performance and effort expectancy, while social influence and performance expectancy determined students' trust. The three predicted elements that influenced ChatGPT adoption were perceived learning gains, perceived risks, and trust in ChatGPT. The study presented some recommendations for universities.",ChatGPT; perceived mobility; trust in ChatGPT; use of ChatGPT,WoS
10.1016/j.jbef.2024.100971,Human bias in AI models? Anchoring effects and mitigation strategies in large language models,10.1016/j.jbef.2024.100971,"This study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples. © 2024 Elsevier B.V., All rights reserved.",Anchoring Bias; Artificial Intelligence,Scopus
10.1057/s41599-025-05014-4,Human-machine in the vortex of digital synergy,10.1057/s41599-025-05014-4,"This study explores whether experience with AI tools and the intensity of their use influence individuals’ adoption of ChatGPT in the Czech Republic. Using data from 1232 respondents (aged 15+), collected via a quota-based online survey from April 8 to April 26, 2024, logistic regression analyses investigated two key questions: (1) Does increased use of virtual assistants correlate with a higher likelihood of ChatGPT adoption? and (2) Does frequent ChatGPT usage predict more intensive engagement with other AI tools? Findings confirm that people who use voice/chatbots more often are significantly more likely to try ChatGPT, and vice versa. Preference for text-based assistants also correlates positively with ChatGPT adoption. Unexpectedly, a generally positive outlook on AI across sectors (banking, healthcare, customer service) does not always translate into ChatGPT usage, implying that trust or scepticism can be context-specific. Another notable insight is that ethical concerns and a strong preference for human contact consistently dampen ChatGPT uptake, suggesting that perceived privacy risks remain a critical barrier. These results highlight the importance of digital synergy in AI adoption. Policymakers and industry stakeholders can use these insights to develop targeted strategies for fostering inclusive, ethical, and sustainable digital transformation. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1109/TTE.2024.3391938,Hybrid Prompt-Driven Large Language Model for Robust State-of-Charge Estimation of Multitype Li-ion Batteries,10.1109/tte.2024.3391938,"State-of-charge (SOC) estimation is critical for reliable operation of Li-ion batteries (LIBs). However, the distinct electrochemical characteristics coupled with harsh low-temperature environments make a single estimator struggle to robustly estimate the volatile SOC of multitype LIBs. To address these issues, this article proposes a hard-soft hybrid prompt learning method to unleash the potential of a pretrained large language model (LLM) for SOC estimation. A textual encoder is introduced to convert LIB measurements into hard text prompts for language modeling, naturally eliciting the pretrained LLM to capture the intrarelations of measured values over time and their interrelations with contextual semantics for accurate estimates. A side adapter network is constructed to reparameterize model adaptation towards different LIB tasks into optimizations within a low-dimensional subspace, strengthening the estimation generalization of the pretrained LLM in a parameter-efficient manner. A knowledge infusion mechanism is designed to encapsulate task-specific information as soft prompt vectors for model integration along forward propagation, dynamically conditioning the hidden states inside the pretrained LLM to enhance the estimation robustness against SOC volatilities. Extensive experiments verify that the hybrid prompt-driven LLM can simultaneously perform estimations for multitype LIBs under diverse operations and sub-zero temperatures with superior accuracy, generalization, and robustness. © 2025 Elsevier B.V., All rights reserved.",Hybrid Prompt Learning; Large Language Model (llm); Multitype Li-ion Batteries (libs); State-of-charge (soc) Estimation; Charging (batteries); Computational Linguistics; Ions; Learning Systems; Lithium-ion Batteries; Modeling Languages; Semantics; Temperature Measurement; Generalisation; Hybrid Prompt Learning; Language Model; Large Language Model; Multi-type Li-ion Battery; Robustness; State-of-charge Estimation; States Of Charges; Task Analysis; Transformer; Temperature,Scopus
10.1145/3648188.3677049,HyperCausal: Visualizing Causal Inference in 3D Hypertext,10.1145/3648188.3677049,"We present HyperCausal, a 3D hypertext visualization framework for exploring causal inference in generative Large Language Models (LLMs). HyperCausal maps the generative processes of LLMs into spatial hypertexts, where tokens are represented as nodes connected by probability-weighted edges. The edges are weighted by the prediction scores of next tokens, depending on the underlying language model. HyperCausal facilitates navigation through the causal space of the underlying LLM, allowing users to explore predicted word sequences and their branching. Through comparative analysis of LLM parameters such as token probabilities and search algorithms, HyperCausal provides insight into model behavior and performance. Implemented using the Hugging Face transformers library and Three.js, HyperCausal ensures cross-platform accessibility to advance research in natural language processing using concepts from hypertext research. We demonstrate several use cases of HyperCausal and highlight the potential for detecting hallucinations generated by LLMs using this framework. The connection with hypertext research arises from the fact that HyperCausal relies on user interaction to unfold graphs with hierarchically appearing branching alternatives in 3D space. This approach refers to spatial hypertexts and early concepts of hierarchical hypertext structures. A third connection concerns hypertext fiction, since the branching alternatives mediated by HyperCausal manifest non-linearly organized reading threads along artificially generated texts that the user decides to follow optionally depending on the reading context. © 2024 Elsevier B.V., All rights reserved.",3d Hypertext; Large Language Models; Visualization; 3d Modeling; Generative Adversarial Networks; Hypertext Systems; Natural Language Processing Systems; Risk Assessment; 3d Hypertext; Causal Inferences; Comparative Analyzes; Generative Process; Language Model; Large Language Model; Modeling Parameters; Spatial Hypertext; Underlying Language; Visualization Framework; Visualization,Scopus
2-s2.0-85216098698,ICoMS 2024 - Proceedings of 2024 7th International Conference on Mathematics and Statistics,,"The proceedings contain 21 papers. The topics discussed include: spike it up: enhancing STL with spike detection for intraday volatility and liquidity forecasting; enhancing spatially-disaggregated simulations with large language models; the pricing of mortgage loan guarantee insurance under fractional Brownian motion; robust covariance matrix estimator with change points for multivariate jump diffusion process; development and validation of an automated weight window generator for reactor Monte Carlo programs; comparison of machine learning methods for binary classification of multicollinearity data; revisiting the problem of uniqueness in sparse reconstruction; and Lagrange multipliers applied to constrained maximization of volume of frustum of a right circular cone. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85180631189,"IFIP WG 8.6 International Working Conference on Transfer and Diffusion of IT, TDIT 2023",,"The proceedings contain 112 papers. The special focus in this conference is on Transfer and Diffusion of IT. The topics include: Assessing the Factors Influencing the Adoption of Generative Artificial Intelligence (GenAI) in the Manufacturing Sector; sentence Generator for Hindi Language Using Formal Semantics; human Resource Analytics: Leveraging Machine Learning for Employee Attrition Prediction; Continuance Intention of ChatGPT Use by Students; amazon Alexa and I: Exploring Factors Affecting Usage Behaviours and Patterns Over Time; analysing Platform Design Consideration to Ensure Digital Inclusion Among Indigenous People; understanding the Usage and Opinion Formation on LinkedIn: Uses and Gratifications Theory; blockchain-Based Application Security Versus Centralized and Distributed Data Management Systems – A Comparative Study; struggle for Visibility: Mobilizing Dormant Logic on Social Media Platforms; The European Union’s Artificial Intelligence Act: An Analysis of Preliminary Perceptions and Responses of Irish SMEs; does Women Mobile Technology Inclusion Shape Their Attitude Towards Intimate Partner Violence? An Empirical Evidence from Sub-Saharan African Communities; blockchain: A Structural Topic Modelling Approach; portfolio Selection Using Network Filtering Methods: A Graph Theoretic Approach; exploring the Fusion of Metaverse and Sports: Current Trends and Future Directions; Understanding the Role of Time in Content Selection Decisions on OTT Platforms; how Successful Online Platforms Create Value?; institutional Voids and Digital Ecosystems of India’s Public Sector; how Social Media Marketing Enhances Brand Communities Engagement: Developing an Integrated Model Using S-O-R Paradigm; e-Government and Well-Being: A Cross-Country Study; information Security Awareness Safety Governance Model for Senior Citizens in Indian Banking Sector for Mobile and Internet Banking. © 2023 Elsevier B.V., All rights reserved.",,Scopus
10.1007/978-981-96-1024-2_17,Improving Event-Level Financial Sentiment Analysis with Retrieval-Augmented Multipath Chain-of-Thought Prompting,10.1007/978-981-96-1024-2_17,"Event-level Financial Sentiment Analysis (EFSA) aims to extract all the quintuples containing five sentiment elements from a given financial news text, which has gained prominence as an emerging domain recently. The present study utilizes a 4-hop Chain-of-Thought (CoT) prompting based on LLMs to predict sentiment elements in a fixed order, which neglects the interdependencies among the sentiment elements within a quintuple. Inspired by recent multi-view prompting (MvP) and CoT ideas, we propose a novel framework termed Retrieval-Augmented Multipath Chain-of-Thought (RMP-CoT) that aggregates quintuples generated by LLMs through different reasoning paths, leveraging a retrieval-augmented mechanism. Specifically, RMP-CoT integrates different element orders into CoT prompting to guide LLMs in generating multiple sentiment quintuples through the utilization of retrieval-augmented mechanism, and then selects the most plausible quintuples by voting. To investigate the effectiveness of our framework, we conduct extensive experiments on four benchmark tasks of EFSA. RMP-CoT pushes the state-of-the-art by over 6% F1 on the EFSA task and also performs quite effectively on the other sub-tasks of EFSA. © 2025 Elsevier B.V., All rights reserved.",Chain-of-thought; Event-level Sentiment Analysis; Financial Sentiment Analysis; Chain-of-thought; Emerging Domains; Event-level Sentiment Analyze; Financial News; Financial Sentiment Analyze; Fixed-order; Multi-views; Multipath; Sentiment Analysis,Scopus
10.1117/12.3017920,Improving Strategies for Educational Imbalance Based on Large Language Model,10.1117/12.3017920,"It can be observed on a global scale that the quality of education between different regions (such as between urban and rural areas) is usually affected by the local economic structure and development stage. For example, many well-known metropolises have high education levels and famous colleges and universities, and the proportion of students admitted to prestigious universities is also high. Rural areas often lack famous schools and corresponding resources, resulting in lower overall student performance and a lower proportion of students admitted to prestigious universities. This article uses statistical methods to study this issue. Further use data including infrastructure investment, hiring more high-quality teachers, providing more diverse and inclusive courses, providing more learning materials and resources, and using a variety of machine learning and deep learning and large language model techniques to predict the growth of educational resources in underdeveloped areas. Evaluation of these predictive models reveals the strength of large language models in predicting the growth of educational resources in underdeveloped regions. The findings provide strategic insights for education policy makers and stakeholders to close pervasive education disparities. © 2024 Elsevier B.V., All rights reserved.",Deep Learning; Education Policy; Large Language Model; Machine Learning; Quality Of Education In Different Cities; Computational Linguistics; Deep Learning; Economics; Education Computing; Investments; Learning Systems; Rural Areas; Education Policies; Educational Resource; Global Scale; Language Model; Large Language Model; Machine-learning; Quality Of Education; Quality Of Education In Different City; Urban And Rural Areas; Students,Scopus
10.1016/j.eswa.2025.128676,In the beginning was the Word: LLM-VaR and LLM-ES,10.1016/j.eswa.2025.128676,"This study introduces LLM-VaR and LLM-ES, novel risk estimation metrics that utilize general-purpose large language models (LLMs) for the forecasting tasks of Value at Risk (VaR) and Expected Shortfall (ES) in a zero-shot setting. Building on the input encoding mechanism of the LLMTime framework, we extend its application by defining new financial risk measures and performing an empirical evaluation of three generations of GPT models, GPT-3.5, GPT-4 and GPT-4o, versus advanced benchmark models such as GARCH with Student innovations and EWMA with Dynamic Conditional Score (DCS). Financial time series are encoded as numerical strings, allowing for model-free inference without requiring retraining. Results show that LLMs perform well when short rolling windows are used, particularly in volatile markets like cryptocurrencies. GPT-3.5 frequently outperforms or matches the performance of newer models, raising questions about model complexity, alignment, and biases. In contrast, performance deteriorates with longer windows, where the econometric models prove more reliable. Our findings demonstrate the potential of general-purpose LLMs as adaptive tools for short-horizon financial risk assessment and contribute a first-of-its-kind benchmark for LLM-based VaR/ES estimation. © 2025 Elsevier B.V., All rights reserved.",Expected Shortfall; Gpt; Large Language Models; Llm-es; Llm-var; Value At Risk; Benchmarking; Computational Linguistics; Finance; Financial Data Processing; Risk Perception; Statistical Methods; Value Engineering; Expected Shortfall; Gpt; Language Model; Large Language Model; Large Language Model-expected Shortfall; Large Language Model-value At Risk; Value At Risk; Risk Assessment,Scopus
10.1007/978-3-031-23633-4_10,InFi-BERT 1.0: Transformer-Based Language Model for Indian Financial Volatility Prediction,10.1007/978-3-031-23633-4_10,"In recent years, BERT-like pretrained neural language models have been successfully developed and utilized for multiple financial domain-specific tasks. These domain-specific pre-trained models are effective enough to learn the specialized language used in financial context. In this paper, we consider the task of textual regression for the purpose of forecasting financial volatility from financial texts, and designed Infi-BERT (Indian Financial BERT), a transformer-based pre-trained language model using domain-adaptive pre-training approach, which effectively learns linguistic-context from annual financial reports from Indian financial texts. In addition, we present the first Indian financial corpus for the task of volatility prediction. With detailed experimentation and result analysis, we demonstrated that our model outperforms the base model as well as the previous domain-specific models for financial volatility forecasting task. © 2023 Elsevier B.V., All rights reserved.",Domain-adaptive Pre-training; Financial Volatility Prediction; Indian Financial Corpus; Textual Regression; Transformer-based Models; Computational Linguistics; Finance; Domain Specific; Domain-adaptive Pre-training; Financial Domains; Financial Volatility Prediction; Indian Financial Corpus; Language Model; Learn+; Pre-training; Textual Regression; Transformer-based Model; Forecasting,Scopus
10.1038/s41598-025-98483-1,Industrial applications of large language models,10.1038/s41598-025-98483-1,"Large language models (LLMs) are artificial intelligence (AI) based computational models designed to understand and generate human like text. With billions of training parameters, LLMs excel in identifying intricate language patterns, enabling remarkable performance across a variety of natural language processing (NLP) tasks. After the introduction of transformer architectures, they are impacting the industry with their text generation capabilities. LLMs play an innovative role across various industries by automating NLP tasks. In healthcare, they assist in diagnosing diseases, personalizing treatment plans, and managing patient data. LLMs provide predictive maintenance in automotive industry. LLMs provide recommendation systems, and consumer behavior analyzers. LLMs facilitates researchers and offer personalized learning experiences in education. In finance and banking, LLMs are used for fraud detection, customer service automation, and risk management. LLMs are driving significant advancements across the industries by automating tasks, improving accuracy, and providing deeper insights. Despite these advancements, LLMs face challenges such as ethical concerns, biases in training data, and significant computational resource requirements, which must be addressed to ensure impartial and sustainable deployment. This study provides a comprehensive analysis of LLMs, their evolution, and their diverse applications across industries, offering researchers valuable insights into their transformative potential and the accompanying limitations. © 2025 Elsevier B.V., All rights reserved.",Large Language Models; Llms; Nlp; Transformers; Artificial Intelligence; Human; Industry; Large Language Model; Natural Language Processing; Artificial Intelligence; Humans; Industry; Large Language Models; Natural Language Processing,Scopus
10.12720/jait.16.1.12-20,Initial Coin Offerings Success Prediction Using Social Media and Large Language Models,10.12720/jait.16.1.12-20,"Initial Coin Offering (ICO) is a fundraising method utilized by blockchain startups to raise capital by issuing and selling digital tokens to investors. ICOs have become widely popular for cryptocurrency fundraising, often generating millions of dollars, and surpassing traditional crowdfunding methods like Initial Public Offerings. However, ICO is a risky way of investing and raising capital due to the lack of regulations and standardisation. In this research, we delve into the impact of social media and sentiment analysis on the success of ICOs, employing various machine learning models and Large Language Models. Our analysis is based on data from over 1,000 ICOs gathered from diverse ICO information platforms, coupled with a corpus of 910,478 tweets associated with these ICOs. We extend our investigation to include other social media platforms such as BitcoinTalk, Telegram, Facebook, and Medium. Our analysis revealed that valuable insights regarding the success of ICOs can be derived by examining text sentiment and investigating metadata across these diverse social media channels. © 2025 Elsevier B.V., All rights reserved.",Bidirectional Encoder Representations From Transformers (bert); Sentiment Analysis; Social Media; Token Sales; Web Scraping,Scopus
10.3390/bdcc8110143,"Innovative Sentiment Analysis and Prediction of Stock Price Using FinBERT, GPT-4 and Logistic Regression: A Data-Driven Approach",10.3390/bdcc8110143,"This study explores the comparative performance of cutting-edge AI models, i.e., Finaance Bidirectional Encoder representations from Transsformers (FinBERT), Generatice Pre-trained Transformer GPT-4, and Logistic Regression, for sentiment analysis and stock index prediction using financial news and the NGX All-Share Index data label. By leveraging advanced natural language processing models like GPT-4 and FinBERT, alongside a traditional machine learning model, Logistic Regression, we aim to classify market sentiment, generate sentiment scores, and predict market price movements. This research highlights global AI advancements in stock markets, showcasing how state-of-the-art language models can contribute to understanding complex financial data. The models were assessed using metrics such as accuracy, precision, recall, F1 score, and ROC AUC. Results indicate that Logistic Regression outperformed the more computationally intensive FinBERT and predefined approach of versatile GPT-4, with an accuracy of 81.83% and a ROC AUC of 89.76%. The GPT-4 predefined approach exhibited a lower accuracy of 54.19% but demonstrated strong potential in handling complex data. FinBERT, while offering more sophisticated analysis, was resource-demanding and yielded a moderate performance. Hyperparameter optimization using Optuna and cross-validation techniques ensured the robustness of the models. This study highlights the strengths and limitations of the practical applications of AI approaches in stock market prediction and presents Logistic Regression as the most efficient model for this task, with FinBERT and GPT-4 representing emerging tools with potential for future exploration and innovation in AI-driven financial analytics. © 2024 Elsevier B.V., All rights reserved.",Finbert; Finbert Model; Logistic Regression; Optuna; Time Series Cross-validation; Electronic Trading; Financial Markets; Prediction Models; Cross Validation; Data-driven Approach; Finaance Bidirectional Encoder Representation From Transsformer; Finaance Bidirectional Encoder Representation From Transsformer Model; Logistics Regressions; Optuna; Sentiment Analysis; Stock Price; Time Series Cross-validation; Times Series; Logistic Regression,Scopus
WOS:000168658500032,Integrated lossy and lossless image coding based on lossless wavelet transform and lossy-lossless multi-channel prediction,,"In this report, we propose an integrated lossy and Lossless image coding, which is possible to be implemented on one architecture. based on combination of lossless wavelet transform (LWT) and lossy-lossless multi-channel prediction (LLMP). The LWT is applied to divide input signals into frequency sub-bands as octave-band decomposition. whereas the LLMP is designed as a non-separable two-dimensional filter bank including quantization step size and local decoding to enhance coding performance in both lossless coding and lossy coding. Its filter coefficients are determined to minimize total bit rate ibr lossless coding, and the optimum quantization step size is applied to maximize lossy coding gain. The local decoding is applied to avoid quantization error effect. The experimental results confirm effectiveness of our proposed method.",image; coding; lossless; lossy; filter bank,WoS
WOS:001539567900001,"Integrating Large Language Models into Robotic Autonomy: A Review of Motion, Voice, and Training Pipelines",10.3390/ai6070158,"This survey provides a comprehensive review of the integration of large language models (LLMs) into autonomous robotic systems, organized around four key pillars: locomotion, navigation, manipulation, and voice-based interaction. We examine how LLMs enhance robotic autonomy by translating high-level natural language commands into low-level control signals, supporting semantic planning and enabling adaptive execution. Systems like SayTap improve gait stability through LLM-generated contact patterns, while TrustNavGPT achieves a 5.7% word error rate (WER) under noisy voice-guided conditions by modeling user uncertainty. Frameworks such as MapGPT, LLM-Planner, and 3D-LOTUS++ integrate multi-modal data-including vision, speech, and proprioception-for robust planning and real-time recovery. We also highlight the use of physics-informed neural networks (PINNs) to model object deformation and support precision in contact-rich manipulation tasks. To bridge the gap between simulation and real-world deployment, we synthesize best practices from benchmark datasets (e.g., RH20T, Open X-Embodiment) and training pipelines designed for one-shot imitation learning and cross-embodiment generalization. Additionally, we analyze deployment trade-offs across cloud, edge, and hybrid architectures, emphasizing latency, scalability, and privacy. The survey concludes with a multi-dimensional taxonomy and cross-domain synthesis, offering design insights and future directions for building intelligent, human-aligned robotic systems powered by LLMs.",large language models (LLMs); autonomous navigation; simulation-to-real transfer; multi-modal datasets; voice-based interaction; Task and Motion Planning (TAMP); physics-informed neural networks (PINNs); semantic reasoning; robot manipulation; reinforcement learning; human-robot interaction (HRI); cloud-edge hybrid architecture,WoS
10.5220/0013191200003890,Integrating Traditional Technical Analysis with AI: A Multi-Agent LLM-Based Approach to Stock Market Forecasting,10.5220/0013191200003890,"Traditional technical analysis methods face limitations in accurately predicting trends in today’s complex f inancial markets. This paper introduces ElliottAgents, an multi-agent system that integrates the Elliott Wave Principle with AI for stock market forecasting. The inherent complexity of financial markets, characterized by non-linear dynamics, noise, and susceptibility to unpredictable external factors, poses significant challenges for accurate prediction. To address these challenges, the system employs LLMs to enhance natural language understanding and decision-making capabilities within a multi-agent framework. By leveraging technologies such as Retrieval-Augmented Generation (RAG) and Deep Reinforcement Learning (DRL), ElliottAgents performs continuous, multi-faceted analysis of market data to identify wave patterns and predict future price movements. The research explores the system’s ability to process historical stock data, recognize Elliott wave patterns, and generate actionable insights for traders. Experimental results, conducted on historical data from major U.S. companies, validate the system’s effectiveness in pattern recognition and trend forecasting across various time frames. This paper contributes to the field of AI-driven financial analysis by demonstrating how traditional technical analysis methods can be effectively combined with modern AI approaches to create more reliable and interpretable market prediction systems. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1080/15427560.2025.2538879,Intraday Stock Prediction Using Sentiment Analysis: Evidence from Dividend Announcements,10.1080/15427560.2025.2538879,"This study explores whether sentiment extracted from financial news using large language models (LLMs) can predict abnormal intraday stock returns following dividend announcements. Drawing on 4,682 news items linked to 1,258 announcements from 394 S&P 500 companies (January 2023–January 2024), we use ChatGPT to extract sentiment polarity scores and we apply different models to forecast cumulative abnormal returns (CARs) in 30-minute intervals. Our findings reveal that sentiment–especially when captured immediately after news releases–has significant predictive power over intraday price movements. Strategies based on ChatGPT-derived sentiment consistently outperform benchmark models, particularly within the first two hours of trading. These results remain robust across alternative specifications and placebo tests, highlighting the value of LLMs for real-time market prediction. This research advances the literature on sentiment analysis and behavioral finance by linking emotion-driven news interpretation to high-frequency trading performance. © 2025 Elsevier B.V., All rights reserved.",Financial News; Intraday Trading; Investment Strategies; Market Reaction; Sentiment Analysis,Scopus
10.1109/ICNLP65360.2025.11108638,Investigating the Predictive Capabilities of Large Language Models in Day Trading by Leveraging Multimodal Data,10.1109/icnlp65360.2025.11108638,"This paper evaluates the predictive capabilities of six LLMs-GPT-4, GPT-4o, Llama 3, Claude 3.5, Mistral 0.3, and Gemma 2-in day trading using multimodal data. The LLMs process diverse inputs, including text-based price histories, news titles, and images. The lowest Mean Absolute Percentage Errors (MAPEs) (1.4%) were achieved by Claude 3.5 and Gemma 2 using only price history text and by Claude 3.5 and Mistral 0.3 with combined price and news history inputs, demonstrating LLMs' potential for accurate financial predictions through prompting without advanced technical expertise. Remarkably, GPT-4 and Claude 3.5 achieve MAPEs of just 1.7% and 1.5%, respectively, by processing only price history images. Furthermore, Gemma 2 achieves a MAPE of 1.5% using only news history inputs, without any information from the price history. © 2025 Elsevier B.V., All rights reserved.",Large Language Models; Llms; Multimodal; Natural Language Processing; Nlp; Artificial Intelligence; Commerce; Electronic Trading; Financial Prediction; Language Model; Language Processing; Large Language Model; Llm; Multi-modal; Natural Language Processing; Natural Languages; Percentage Error; Predictive Capabilities; Costs,Scopus
WOS:001540598500049,LLM-Driven Stock Prediction: Capturing Market Trends with LLaMA,10.2478/picbe-2025-0043,"Stock price forecasting remains a challenging task due to the dynamic nature of financial markets and the influence of external factors such as investor sentiment and macroeconomic events. Traditional time series statistical models often struggle to capture complex nonlinear dependencies and market signals embedded in unstructured data. With advancements in Large Language Models (LLMs), it is now possible to integrate textual information from financial news, social media, and earnings reports to enhance predictive accuracy. In this study, we leverage the LLaMA family of LLMs to improve stock price forecasting by combining historical price with news. We evaluate the performance of LLaMA 3.3 against LLaMA 3.1 and the benchmark ARIMA model to assess its effectiveness in capturing time series patterns and textual signals. Our results indicate that LLaMA 3.3 outperforms both LLaMA 3.1 and ARIMA, demonstrating its superior capability in modeling complex financial relationships. Additionally, our analysis confirms that market sentiment has an impact on stock returns, with sentiments influencing short-term price fluctuations. By incorporating news sentiment in LLMs prompt, we achieve improved forecasts compared to models without news. This highlights the importance of integrating both structured (numerical time series) and unstructured (news sentiment) data for enhanced financial modeling. Our findings suggest that LLM-driven forecasting methods hold substantial promise for traders, analysts, and financial institutions seeking more accurate market predictions. Future work will explore fine-tuning LLaMA models for domain-specific financial tasks and improving interpretability in decision making processes.",LLMs; LLaMA; ARIMA; News Sentiment; Time series forecasting; Stock Price,WoS
10.1007/978-981-96-9986-5_8,LLM-Enhanced Feature Engineering for Multi-factor Electricity Price Predictions,10.1007/978-981-96-9986-5_8,"Accurately forecasting electricity price volatility is crucial for effective risk management and decision-making. Traditional forecasting models often fall short in capturing the complex, non-linear dynamics of electricity markets, particularly when external factors like weather conditions and market volatility are involved. These limitations hinder their ability to provide reliable predictions in markets with high volatility, such as the New South Wales (NSW) electricity market. To address these challenges, we introduce FAEP, a Feature-Augmented Electricity Price Prediction framework, FAEP leverages Large Language Models (LLMs) combined with advanced feature engineering to enhance prediction accuracy. By incorporating external features such as weather data and price volatility jumps, and utilizing Retrieval-Augmented Generation (RAG) for effective feature extraction, FAEP overcomes the shortcomings of traditional approaches. A hybrid XGBoost-LSTM model in FAEP further refines these augmented features, resulting in a more robust prediction framework. Experimental results demonstrate that FAEP achieves state-of-art (SOTA) performance compared to other electricity price prediction models in the Australian New South Wale electricity market, showcasing the efficiency of LLM-enhanced feature engineering and hybrid machine learning architectures. © 2025 Elsevier B.V., All rights reserved.",Electricity Price Prediction; Feature Engineering; Llm; Costs; Decision Making; Electric Industry; Meteorology; Power Markets; Prediction Models; Weather Forecasting; Electricity Price Prediction; Electricity Price Volatilities; Electricity Prices; Feature Engineerings; Forecasting Electricity; Language Model; Large Language Model; Multi-factor; New South Wales; Price Prediction; Risk Management,Scopus
10.3390/math13152523,LLM-Guided Ensemble Learning for Contextual Bandits with Copula and Gaussian Process Models,10.3390/math13152523,"Contextual multi-armed bandits (CMABs) are vital for sequential decision-making in areas such as recommendation systems, clinical trials, and finance. We propose a simulation framework integrating Gaussian Process (GP)-based CMABs with vine copulas to model dependent contexts and GARCH processes to capture reward volatility. Rewards are generated via copula-transformed Beta distributions to reflect complex joint dependencies and skewness. We evaluate four policies—ensemble, Epsilon-greedy, Thompson, and Upper Confidence Bound (UCB)—over 10,000 replications, assessing cumulative regret, observed reward, and cumulative reward. While Thompson sampling and LLM-guided policies consistently minimize regret and maximize rewards under varied reward distributions, Epsilon-greedy shows instability, and UCB exhibits moderate performance. Enhancing the ensemble with copula features, GP models, and dynamic policy selection driven by a large language model (LLM) yields superior adaptability and performance. Our results highlight the effectiveness of combining structured probabilistic models with LLM-based guidance for robust, adaptive decision-making in skewed, high-variance environments. © 2025 Elsevier B.V., All rights reserved.",Adaptive Policy; Contextual Bandits; Functional Garch; Gaussian Processes; Large Language Models; Sequential Decision-making; Vine Copulas,Scopus
10.3390/s25175372,LLM-Powered Prediction of Hyperglycemia and Discovery of Behavioral Treatment Pathways from Wearables and Diet,10.3390/s25175372,"Postprandial hyperglycemia, marked by the blood glucose level exceeding the normal range after consuming a meal, is a critical indicator of progression toward type 2 diabetes in people with prediabetes and in healthy individuals. A key metric for understanding blood glucose dynamics after eating is the postprandial Area Under the Curve (AUC). Predicting postprandial AUC in advance based on a person’s lifestyle factors, such as diet and physical activity level, and explaining the factors that affect postprandial blood glucose could allow an individual to adjust their behavioral choices accordingly to maintain normal glucose levels. In this work, we develop an explainable machine learning solution, GlucoLens, that takes sensor-driven inputs and utilizes advanced data processing, large language models, and trainable machine learning models to estimate postprandial AUC and predict hyperglycemia from diet, physical activity, and recent glucose patterns. We use data obtained using wearables in a five-week clinical trial of 10 adults who worked full-time to develop and evaluate the proposed computational model that integrates wearable sensing, multimodal data, and machine learning. Our machine learning model takes multimodal data from wearable activity and glucose monitoring sensors, along with food and work logs, and provides an interpretable prediction of the postprandial glucose patterns. GlucoLens achieves a normalized root mean squared error (NRMSE) of 0.123 in its best configuration. On average, the proposed technology provides a 16% better predictive performance compared to the comparison models. Additionally, our technique predicts hyperglycemia with an accuracy of 79% and an F1 score of 0.749 and recommends different treatment options to help avoid hyperglycemia through diverse counterfactual explanations. With systematic experiments and discussion supported by established prior research, we show that our method is generalizable and consistent with clinical understanding. © 2025 Elsevier B.V., All rights reserved.","Continuous Glucose Monitoring; Diabetes; Hyperglycemia; Large Language Models; Machine Learning; Metabolic Health; Blood Glucose; Behavioral Research; Blood; Data Handling; Forecasting; Glucose; Glucose Sensors; Learning Algorithms; Learning Systems; Machine Learning; Nutrition; Wearable Sensors; Areas Under The Curves; Blood Glucose; Continuous Glucose Monitoring; Hyperglycaemia; Language Model; Large Language Model; Machine Learning Models; Machine-learning; Metabolic Health; Multi-modal; Medical Problems; Adult; Blood; Diagnosis; Diet; Exercise; Female; Glucose Blood Level; Human; Hyperglycemia; Machine Learning; Male; Middle Aged; Non Insulin Dependent Diabetes Mellitus; Physiology; Postprandial State; Self-monitoring Blood Glucose; Wearable Electronic Device; Adult; Blood Glucose; Blood Glucose Self-monitoring; Diabetes Mellitus, Type 2; Diet; Exercise; Female; Humans; Hyperglycemia; Machine Learning; Male; Middle Aged; Postprandial Period; Wearable Electronic Devices",Scopus
10.1016/j.fraope.2025.100359,LLM-guided semantic feature selection for interpretable financial market forecasting in low-resource financial markets,10.1016/j.fraope.2025.100359,"Feature selection is critical for accurate and interpretable financial forecasting, particularly in data-scarce environments. This study introduces a semantic feature selection framework empowered by GPT-4, which ranks financial indicators through prompt engineering and retrieval-augmented descriptions. Using weekly macro-financial data from the Malaysian equity market, including the FTSE Bursa Malaysia KLCI index obtained via the yfinance API, the proposed method is integrated with XGBoost and evaluated against multiple baselines. Results show that the LLM-based approach achieves the best forecasting accuracy, with RMSE = 12.82, MSE = 164.38, and R2 = 0.75, consistently outperforming alternatives across different feature sizes and time windows. These findings highlight the effectiveness of semantic feature selection in improving predictive accuracy, robustness, and interpretability, offering a promising direction for financial forecasting in low-resource settings. © 2025 Elsevier B.V., All rights reserved.",Data-centric Ai; Financial Forecasting; Large Language Models; Semantic Feature Selection; Time Series; Commerce; Data Accuracy; Electronic Trading; Feature Extraction; Forecasting; Semantics; Time Series; Data Centric; Data-centric Ai; Features Selection; Financial Forecasting; Language Model; Large Language Model; Market Forecasting; Semantic Feature Selection; Semantic Features; Times Series; Financial Markets,Scopus
10.1145/3650212.3680388,LLM4Fin: Fully Automating LLM-Powered Test Case Generation for FinTech Software Acceptance Testing,10.1145/3650212.3680388,"FinTech software, crucial for both safety and timely market deployment, presents a compelling case for automated acceptance testing against regulatory business rules. However, the inherent challenges of comprehending unstructured natural language descriptions of these rules and crafting comprehensive test cases demand human intelligence. The emergence of Large Language Models (LLMs) holds promise for automated test case generation, leveraging their natural language processing capabilities. Yet, their dependence on human intervention for effective prompting hampers efficiency. In response, we introduce a groundbreaking, fully automated approach for generating high-coverage test cases from natural language business rules. Our methodology seamlessly integrates the versatility of LLMs with the predictability of algorithmic methods. We fine-tune pre-trained LLMs for improved information extraction accuracy and algorithmically generate comprehensive testable scenarios for the extracted business rules.Our prototype, LLM4Fin, is designed for testing real-world stock-trading software. Experimental results demonstrate LLM4Fin's superiority over both state-of-the-art LLM, such as ChatGPT, and skilled testing engineers. We achieve remarkable performance, with up to 98.18% and an average of 20%-110% improvement on business scenario coverage, and up to 93.72% on code coverage, while reducing the time cost from 20 minutes to a mere 7 seconds. These results provide robust evidence of the framework's practical applicability and efficiency, marking a significant advancement in FinTech software testing. © 2024 Elsevier B.V., All rights reserved.",Fintech Software; Large Language Model; Software Acceptance Testing; Test Case Generation; Algorithmic Languages; Automatic Test Pattern Generation; Enterprise Software; Financial Markets; Fintech; Model Checking; Software Prototyping; Software Testing; Acceptance Testing; Automated Acceptance Testing; Business Rules; Fintech Software; Language Model; Large Language Model; Natural Languages; Software Acceptance Testing; Test Case; Test Case Generation; Acceptance Tests,Scopus
2-s2.0-85188265330,Language Models Don't Always Say What They Think: Unfaithful Explanations in Chain-of-Thought Prompting,,"Large Language Models (LLMs) can achieve strong performance on many tasks by producing step-by-step reasoning before giving a final output, often referred to as chain-of-thought reasoning (CoT). It is tempting to interpret these CoT explanations as the LLM's process for solving a task. This level of transparency into LLMs' predictions would yield significant safety benefits. However, we find that CoT explanations can systematically misrepresent the true reason for a model's prediction. We demonstrate that CoT explanations can be heavily influenced by adding biasing features to model inputs-e.g., by reordering the multiple-choice options in a few-shot prompt to make the answer always “(A)”-which models systematically fail to mention in their explanations. When we bias models toward incorrect answers, they frequently generate CoT explanations rationalizing those answers. This causes accuracy to drop by as much as 36% on a suite of 13 tasks from BIG-Bench Hard, when testing with GPT-3.5 from OpenAI and Claude 1.0 from Anthropic. On a social-bias task, model explanations justify giving answers in line with stereotypes without mentioning the influence of these social biases. Our findings indicate that CoT explanations can be plausible yet misleading, which risks increasing our trust in LLMs without guaranteeing their safety. Building more transparent and explainable systems will require either improving CoT faithfulness through targeted efforts or abandoning CoT in favor of alternative methods. © 2025 Elsevier B.V., All rights reserved.",Bias Modeling; Language Model; Model Inputs; Model Prediction; Multiple Choice; Performance; Safety Benefits; Task Modelling; Computational Linguistics,Scopus
WOS:001464889800040,Large Language Model Based Multi-Objective Optimization for Integrated Sensing and Communications in UAV Networks,10.1109/lwc.2025.3529082,"This letter investigates an un-crewed aerial vehicle (UAV) network with integrated sensing and communication (ISAC) systems, where multiple UAVs simultaneously sense the locations of ground users with radars and provide communication services. To find the trade-off between communication and sensing (C&S) in the system, we formulate a multi-objective optimization problem (MOP) to maximize the total network utility and the localization Cram & eacute;r-Rao bounds (CRB) of ground users, which jointly optimizes the deployment and power control of UAVs. Inspired by the huge potential of large language models (LLM) for prediction and inference, we propose an LLM-enabled decomposition-based multi-objective evolutionary algorithm (LEDMA) for solving the highly non-convex MOP. We first adopt a decomposition-based scheme to decompose the MOP into a series of optimization sub-problems. We second integrate LLMs as black-box search operators with MOP-specifically designed prompt engineering into the framework of MOEA to solve optimization sub-problems simultaneously. Numerical results demonstrate that the proposed LEDMA can find the clear trade-off between C&S and outperforms baseline MOEAs in terms of obtained Pareto fronts and convergence.",Integrated sensing and communications; multi objective optimization; large language model; large language model,WoS
10.1109/IECON55916.2024.10906045,Large Language Model for Extreme Electricity Price Forecasting in the Australia Electricity Market,10.1109/iecon55916.2024.10906045,"This work addresses the challenge of accurately forecasting electricity prices within the volatile Australian market, especially during extreme conditions. It leverages advanced generative pre-trained Large Language Models (LLMs) to analyze the content of electricity market notices with the goal of identifying the drivers behind extreme price fluctuations. Additionally, this approach employs LLMs for an in-depth time-series analysis of electricity prices, providing Australian electricity company traders with insights to refine their trading strategies. To enhance forecasting accuracy this study adopts the QLoRA method for fine-tuning open access LLMs, enabling the analysis of market notices to generate a time series event dataset. A CNN-LSTM network architecture is designed to process both electricity price data and market notice information, thereby improving forecast precision in periods of extreme price volatility. The proposed decision support framework undergoes simulation and evaluation using data from the Australian electricity market, demonstrating its potential to significantly benefit traders in navigating the complexities of the energy sector. © 2025 Elsevier B.V., All rights reserved.",Australia; Australian Electricities; Depth-time; Electricity Prices; Electricity Prices Forecasting; Extreme Conditions; Forecasting Electricity; Language Model; Price Fluctuation; Time-series Analysis; Power Markets,Scopus
10.1109/IST64061.2024.10843617,Large Language Models (LLM) for Estimating the Cost of Cyber-attacks,10.1109/ist64061.2024.10843617,"With the expansion of digital services and intelligent agents, cyber-attacks are increasingly frequent and impactful. Estimating the financial consequences of these attacks has become crucial in guiding investments in mitigation and defense strategies. This paper presents a framework leveraging Large Language Models (LLMs) and big data analytics to estimate the financial impact of cyber threats, specifically focusing on lost business opportunities in the banking sector. As a frequent target of cyberattacks, the banking industry suffers significant financial losses and a decline in customer trust. By analyzing over 23 billion transactions, the LLM algorithm identifies business activity patterns and calculates losses during operational downtimes. The study compares the performance of LLMs with alternative models, including Deep Learning, Support Vector Machines (SVM), and Random Walk, highlighting the superior accuracy of LLMs in estimating business activity disruptions. The findings provide a scalable methodology for calculating the financial cost of cyber-attacks in the banking sector, with potential applications in other industries. The study underscores the critical need for robust cybersecurity measures and effective risk mitigation strategies, given the high costs incurred during cyber-attack downtimes. © 2025 Elsevier B.V., All rights reserved.",Big Data Analytics; Cyber Risk Assessment; Cyber-attack; Deep Learning; Large Language Models; Qos; Statistical Analysis; Computer Viruses; Costs; Decentralized Finance; Deep Learning; Network Security; Phishing; Risk Assessment; Banking Sectors; Big Data Analytic; Business Activities; Cybe Risk Assessment; Cyber-attacks; Data Analytics; Language Model; Large Language Model; Risks Assessments; Cyber Attacks,Scopus
10.18653/v1/2024.findings-naacl.130,Large Language Models Sensitivity to The Order of Options in Multiple-Choice Questions,10.18653/v1/2024.findings-naacl.130,"Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions-commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13% to 85% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks. © 2024 Elsevier B.V., All rights reserved.",Benchmarking; Computational Linguistics; Language Model; Model Bias; Model Prediction; Model Robustness; Model Sensitivity; Multiple-choice Questions; Optimal Strategies; Percentage Points; Performance Gaps; Forecasting,Scopus
10.1109/ACCESS.2024.3445413,"Large Language Models and Sentiment Analysis in Financial Markets: A Review, Datasets, and Case Study",10.1109/access.2024.3445413,"This paper comprehensively examines Large Language Models (LLMs) in sentiment analysis, specifically focusing on financial markets and exploring the correlation between news sentiment and Bitcoin prices. We systematically categorize various LLMs used in financial sentiment analysis, highlighting their unique applications and features. We also investigate the methodologies for effective data collection and categorization, underscoring the need for diverse and comprehensive datasets. Our research features a case study investigating the correlation between news sentiment and Bitcoin prices, utilizing advanced sentiment analysis and financial analysis methods to demonstrate the practical application of LLMs. The findings reveal a modest but discernible correlation between news sentiment and Bitcoin price fluctuations, with historical news patterns showing a more substantial impact on Bitcoin's longer-term price than immediate news events. This highlights LLMs' potential in market trend prediction and informed investment decision-making. © 2024 Elsevier B.V., All rights reserved.",Bitcoin Price; Large Language Model; Machine Learning; Market Dynamics; Sentiment Analysis; Bitcoin Price; Correlation; Language Model; Large Language Model; Machine-learning; Market Dynamics; Predictive Models; Quality Assessment; Sentiment Analysis; Bitcoin,Scopus
10.3390/electronics14102061,Large Language Models for Predictive Maintenance in the Leather Tanning Industry: Multimodal Anomaly Detection in Compressors,10.3390/electronics14102061,"Predictive maintenance in industrial settings increasingly demands systems capable of integrating heterogeneous data streams while balancing computational efficiency and contextual reasoning. This paper introduces a novel framework leveraging Large Language Models (LLMs) to address these challenges in compressor monitoring, demonstrating their potential to enhance anomaly detection accuracy and operational cost-effectiveness. We evaluate Qwen 2.5-32B against traditional machine learning models (ANN, CNN, LSTM), achieving superior recall (92.3%) and AUC-ROC (0.991) through transformer-based architectures optimized for multimodal data fusion. A financial case study reveals operational cost reductions of 18% via reduced downtime and optimized maintenance schedules, while a real-time monitoring dashboard validates scalability for industrial deployment. Our findings highlight the transformative role of LLMs in bridging technical innovation with domain-specific operational constraints, offering a blueprint for predictive maintenance in niche industries. © 2025 Elsevier B.V., All rights reserved.",Anomaly Detection; Industrial Compressors; Large Language Models; Machine Learning; Multimodal Data Fusion; Predictive Maintenance; Real-time Monitoring; Costs; Sensor Data Fusion; Tanning; Anomaly Detection; Industrial Compressors; Language Model; Large Language Model; Leather Tanning Industry; Machine-learning; Multi-modal; Multimodal Data Fusion; Predictive Maintenance; Real Time Monitoring,Scopus
10.1016/j.slast.2025.100285,Large language models in breast cancer reconstruction: A framework for patient-specific recovery and predictive insights,10.1016/j.slast.2025.100285,"Breast cancer reconstruction, a vital part of comprehensive cancer therapy, can be performed concurrently with cancer resection, improving both physical and psychological recovery for patients. However, the intricacy and variety of recovery demand a specialized strategy. Thus, a unique framework that uses Natural Language Processing (NLP) and Large Language Models (LLMs) is developed to improve patient-specific recovery and predictive insights during breast cancer reconstruction. Lemmatization/Stemming is used for pre-processing large volumes of data from medical records, clinical notes, and treatment histories and BioBERT, a model pretrained on biomedical texts to capture complex medical terminology used for feature extraction and aids in the transformation of text data into numerical vectors. The approach employs forecasting models like ChatGPT-4 and Gemini to offer insights into the likelihood of successful reconstruction and associated problems based on specific patient characteristics, treatment options, and recovery timelines. Using sophisticated LLMs, this framework provides clinicians with a powerful tool for personalizing care by anticipating postoperative complications, recovery durations, and psychosocial consequences. Furthermore, it allows for the development of targeted rehabilitation programs that are adapted to unique patient needs, enabling greater recovery and overall quality of life. This approach not only improves clinical decision-making but also empowers patients by offering personalized recovery strategies. As a result, the accuracy of ChatGPT-4 is 98.4 % and Gemini is 98.7 %; the score per response is 2.52 for ChatGPT-4 and 2.89 for Gemini. Readability of ChatGPT-4 is 93.0 % and Gemini is 94.5 %; a relevance score is 95.5 % and 94.0 % for ChatGPT-4 and Gemini, and time response is 2.5 s for ChatGPT-4 and 2.5 s for Gemini. Finally, this research indicates how NLP and LLMs can transform breast cancer reconstruction by offering predictive insights and promoting tailored, patient-centered therapy, bridging the gap between powerful computational technologies and life science research to better patient care. © 2025 Elsevier B.V., All rights reserved.",Breast Cancer Reconstruction; Large Language Models (llms); Natural Language Processing (nlp); Patient Recovery; Personalizing Care; Clinical Research; Hospital Data Processing; Medical Computing; Natural Language Processing Systems; Patient Rehabilitation; Patient Treatment; Personalized Medicine; Breast Cancer; Breast Cancer Reconstruction; Language Model; Language Processing; Large Language Model; Natural Language Processing; Natural Languages; Patient Recovery; Patient Specific; Personalizing Care; Diseases; Adult; Aged; Article; Breast Cancer; Breast Cancer Reconstruction; Cancer Surgery; Cancer Therapy; Chatgpt; Clinical Decision Making; Cohort Analysis; Comorbidity; Demographics; Duration; Feature Extraction; Female; Health Care; Human; Large Language Model; Logistic Regression Analysis; Machine Learning; Medical Record; Medical Terminology; Natural Language Processing; Observational Study; Patient Care; Patient-specific Recovery; Predictive Model; Quality Of Life; Retrospective Study; Socioeconomic Background; Breast Reconstruction; Breast Tumor; Procedures; Rehabilitation; Surgery; Breast Neoplasms; Female; Humans; Large Language Models; Mammaplasty; Natural Language Processing,Scopus
10.1007/978-981-99-8391-9_4,Lateral AI: Simulating Diversity in Virtual Communities,10.1007/978-981-99-8391-9_4,"In this paper, we present Lateral AI that offers a diverse and multi-dimensional world experience. It makes use of semi-automated prompt engineering on top of GPT3.5. The coupling with named entity recognition and text summarization enables creation of a diversity of AI personas and a multiplicity of requests. The features of Lateral AI, such as creation of custom AI personas, prioritisation of user-embedded knowledge in those personas and follow-up requests, enable users to co-create with AI. Users can contribute certain information and perspectives to the application if a Large Language Model does not have access to it. Lateral AI makes the user an active component of the integrated system rather than a mere AI consumer. We demonstrate use of Lateral AI to generate a range of diverse responses and illustrate the ability of AI to predict beyond its factual knowledge. Lateral AI is a unique and alternative option to other AI models, contributing to the diverse and creative pool of emerging AI technologies and applications. The principles behind Lateral AI can be used to simulate diverse communities in a variety of settings such as online virtual communities and human robotics. © 2023 Elsevier B.V., All rights reserved.",Knowledge Prioritization In Llms; Lateral Ai; Lateral Thinking; Llm Predictions; Virtual Communities; Character Recognition; Social Networking (online); Knowledge Prioritization In Llm; Lateral Ai; Lateral Thinking; Llm Prediction; Multi Dimensional; Named Entity Recognition; Prioritization; Text Summarisation; Virtual Community; World Experience; Virtual Reality,Scopus
10.1145/3589334.3645611,Learning to Generate Explainable Stock Predictions using Self-Reflective Large Language Models,10.1145/3589334.3645611,"Explaining stock predictions is generally a difficult task for traditional non-generative deep learning models, where explanations are limited to visualizing the attention weights on important texts. Today, Large Language Models (LLMs) present a solution to this problem, given their known capabilities to generate human-readable explanations for their decision-making process. However, the task of stock prediction remains challenging for LLMs, as it requires the ability to weigh the varying impacts of chaotic social texts on stock prices. The problem gets progressively harder with the introduction of the explanation component, which requires LLMs to explain verbally why certain factors are more important than the others. On the other hand, to fine-tune LLMs for such a task, one would need expert-annotated samples of explanation for every stock movement in the training set, which is expensive and impractical to scale. To tackle these issues, we propose our Summarize-Explain-Predict (SEP) framework, which utilizes a verbal self-reflective agent and Proximal Policy Optimization (PPO) that allow a LLM teach itself how to generate explainable stock predictions, in a fully autonomous manner. The reflective agent learns how to explain past stock movements through a self-reasoning process, while the PPO trainer trains the model to generate the most likely explanations given the input texts at test-time. The training samples for the PPO trainer are also the responses generated during the reflective process, which eliminates the need for human annotators. Using our SEP framework, we fine-tune a specialized LLM that can outperform both traditional deep-learning and LLM methods in prediction accuracy and Matthews correlation coefficient, for the stock classification task. To justify the generalization capability of our framework, we further test it on the portfolio construction task, and demonstrate its effectiveness through various portfolio metrics. Our code can be accessed through https://github.com/koa-fin/sep. © 2024 Elsevier B.V., All rights reserved.",Explainable Ai; Large Language Models; Stock Prediction; Autonomous Agents; Computational Linguistics; Deep Learning; Electronic Trading; Financial Markets; Forecasting; Learning Systems; Chaotics; Decision-making Process; Explainable Ai; Human-readable; Language Model; Large Language Model; Learning Models; Policy Optimization; Stock Movement; Stock Predictions; Decision Making,Scopus
10.2118/224146-MS,Leveraging Language Models for Carbon Market Insights: News Sentiment and Price Dynamics,10.2118/224146-ms,"The carbon credit system plays a pivotal role in offsetting emissions, mitigating climate change, and enabling trading opportunities. We examine California's Low Carbon Fuel Standard (LCFS) using time series data from 2013 to 2024 to analyze carbon credit price dynamics and improve predictive capability with machine learning and large language models (LLMs). Technical analysis is employed to capture short-term trends (using monthly LCFS transaction data). While effective in identifying general price trends, these models struggle to adapt to shifts caused by policy changes or supply-demand fluctuations and offer limited insight into market dynamics. To address this, we incorporate news articles covering general carbon market topics. LLMs are employed for sentiment analysis, generating sentiment scores ranging from -1 (extremely negative) to 1 (extremely positive) and categorizing influence into short-term, mid-term, or long-term. The aggregated sentiment scores achieve over 60% alignment with price change. We further enhance prediction performance by integrating news data directly with trading data into advanced LLMs, including Gemini 1.5 Pro, Claude 3.5 Sonnet, GPT-4o, and o1-preview, resulting in higher F1 scores and improved accuracy. These LLMs demonstrated the ability to synthesize diverse information and provided clear market insights. For long-term forecasting, we integrate news data and LCFS trading data with California’s gasoline and diesel prices, annual CO<inf>2</inf> emissions, electric vehicle sales, Cap-and-Trade (CaT) carbon tax prices, EU Emissions Trading Scheme (ETS) carbon prices, and Canada’s federal fuel charge into LLMs. The long-term prediction achieves F1 score up to 0.8, capturing price transitions and providing reasoned insights. This study highlights the potential of LLMs in carbon market forecasting, especially in enhancing interpretability and decision-making. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1109/ACCESS.2025.3560254,Leveraging Large Language Models for Discrepancy Value Prediction in Custody Transfer Systems: A Comparative Analysis of Probabilistic and Point Forecasting Approaches,10.1109/access.2025.3560254,"Discrepancies in custody transfer systems in the oil and gas industry pose significant financial, regulatory, and operational risks. Accurate prediction of these discrepancies is critical to optimizing operations and minimizing potential losses. This study evaluates the effectiveness of Large Language Models (LLMs), specifically the Chronos-FineTuning Amazon Chronos T5 Small model, alongside statistical, machine learning, and deep learning models, in both probabilistic and point forecasting tasks. The evaluation covers metrics such as Weighted Quantile Loss (WQL), Scaled Quantile Loss (SQL), Mean Absolute Error (MAE), Symmetric Mean Absolute Percentage Error (SMAPE), and Root Mean Square Error (RMSE). The results highlight the superior performance of the Chronos model in both forecasting paradigms, demonstrating its ability to capture uncertainty and deliver precise predictions. This research offers valuable insights into selecting forecasting methodologies to improve custody transfer operations, underscoring the transformative potential of LLMs in industrial applications. © 2025 Elsevier B.V., All rights reserved.",Custody Transfer System; Discrepancy; Large Language Models; Probabilistic Time-series Forecasting; Losses; Prediction Models; Custody Transfer; Custody Transfer System; Discrepancy; Language Model; Large Language Model; Probabilistic Time-series Forecasting; Probabilistics; Quantile Loss; Time Series Forecasting; Transfer Systems; Gas Industry,Scopus
10.1109/BigData62323.2024.10825362,Leveraging Large Language Models for Institutional Portfolio Management: Persona-Based Ensembles,10.1109/bigdata62323.2024.10825362,"Large language models (LLMs) have demonstrated promising performance in various financial applications, though their potential in complex investment strategies remains underexplored. To address this gap, we investigate how LLMs can predict price movements in stock and bond portfolios using economic indicators, enabling portfolio adjustments akin to those employed by institutional investors. Additionally, we explore the impact of incorporating different personas within LLMs, using an ensemble approach to leverage their diverse predictions. Our findings show that LLM-based strategies, especially when combined with the mode ensemble, outperform the buy-and-hold strategy in terms of Sharpe ratio during periods of rising consumer price index (CPI). However, traditional strategies are more effective during declining CPI trends or sharp market downturns. These results suggest that while LLMs can enhance portfolio management, they may require complementary strategies to optimize performance across varying market conditions. © 2025 Elsevier B.V., All rights reserved.",Ensemble Method; Finance; Large Language Models; Persona; Portfolio Management; Prompt Engineering; Asset Management; Commerce; Decentralized Finance; Financial Markets; Investments; Consumer Price Index; Ensemble Methods; Financial Applications; Investment Strategy; Language Model; Large Language Model; Performance; Persona; Portfolio Managements; Prompt Engineering,Scopus
10.1109/ICDMW65004.2024.00019,Leveraging Large Language Models for Predicting Stock Option Valuation and Financial Risk Mitigation,10.1109/icdmw65004.2024.00019,"The prediction of short-term stock options with near-future expiration dates is a challenging task due to high volatility, limited information, market noise and the risk of time decay. This work focuses on the new approach to the stock options valuation by leveraging Large Language Models (LLMs) through the integration of quantitative (i.e. financial features-lagged prices, moving averages, and volatility indicators) and qualitative data (i.e. news data, including article titles, full textual content, and publication dates). More specifically, our approach fuses sentiment analysis from LLMs applied to financial news from two reputable outlets (i.e. Economic Times and Yahoo Finance India) with quantitative data on stock options, which includes stock option closing price. By conducting experiments on companies from the NIFTY 50 index using ChatGPT-3.5, ChatGPT-4, and LLaMA 3.1, we show that our method achieves superior prediction accuracy compared to other similar approaches. The paper develops a new framework to improve the valuation of short-term stock options using advanced natural language processing behaviors afforded by LLMs to achieve a more holistic capture of market dynamics and sentiment in option pricing. © 2025 Elsevier B.V., All rights reserved.",Bearish Prediction; Bullish Prediction; Chatgpt-3.5; Chatgpt-4; Financial Markets; Large Language Models; Llama 3.1; Nifty50; Option Valuation; Risk Management; Sentiment Analysis; Short-term Options; Decentralized Finance; Financial Markets; Modeling Languages; Natural Language Processing Systems; Prediction Models; Risk Analysis; Risk Assessment; Risk Management; Bearish Prediction; Bullish Prediction; Chatgpt-3.5; Chatgpt-4; Language Model; Large Language Model; Llama 3.1; Nifty50; Option Valuation; Risks Management; Sentiment Analysis; Short-term Option; Costs,Scopus
10.1007/s10791-025-09573-7,Leveraging large language model as news sentiment predictor in stock markets: a knowledge-enhanced strategy,10.1007/s10791-025-09573-7,"In the fast-evolving artificial intelligence era, the intersection of natural language processing and financial analysis has attracted significant attention, primarily due to its potential to provide valuable insights into financial market behavior. Sentiment analysis of financial news articles is a crucial aspect of this intersection, providing cues about market sentiment that may affect stock price dynamics. Traditional sentiment analysis methods often rely on rules or machine learning algorithms trained on labeled datasets, but these methods face challenges in capturing the context within the text. This paper proposes a framework that incorporates prompt engineering strategies, including a novel Domain Knowledge Chain-of-Thought (DK-CoT) strategy, integrating domain-specific financial knowledge with chain-of-thought reasoning, designed to leverage and enhance the performance of large language models (LLMs) in financial news sentiment analysis. DK-CoT has been compared with various prompt engineering techniques, including zero-shot, few-shot, and chain-of-thought, as well as other benchmark models like BERT and RoBERTa. Through comprehensive experiments and evaluations, we introduce the weighted F1 score as a more practical metric, emphasizing the disproportionate impact of negative news on financial markets, which better reflects real-world financial dynamics, as negative sentiments often lead to more significant market reactions than positive or neutral sentiments. Experimental results have shown that DK-CoT adopted in an LLM called GLM is effective in improving the performance and reliability of financial news sentiment analysis. Our findings provide insights into optimal prompt designs and highlight the importance of incorporating financial knowledge to uplift LLM performance while reducing the need for extensive computational resources and fine-tuning. © 2025 Elsevier B.V., All rights reserved.",Accessible Machine Learning; Financial News; Large Language Model; Natural Language Processing; Prompt Engineering; Sentiment Analysis,Scopus
WOS:001562898900002,Linearly-interpretable concept embedding models for text analysis,10.1007/s10994-025-06839-5,"Despite their success, Large-Language Models (LLMs) still face criticism due to their lack of interpretability. Traditional post-hoc interpretation methods, based on attention and gradient-based analysis, offer limited insights as they only approximate the model's decision-making processes and have been proved to be unreliable. For this reason, Concept-Bottleneck Models (CBMs) have been lately proposed in the textual field to provide interpretable predictions based on human-understandable concepts. However, CBMs still exhibit several limitations due to their architectural constraints limiting their expressivity, to the absence of task-interpretability when employing non-linear task predictors and for requiring extensive annotations that are impractical for real-world text data. In this paper, we address these challenges by proposing a novel Linearly Interpretable Concept Embedding Model (LICEM) going beyond the current accuracy-interpretability trade-off. LICEMs classification accuracy is better than existing interpretable models and matches black-box ones. We show that the explanations provided by our models are more intervenable and causally consistent with respect to existing solutions. Finally, we show that LICEMs can be trained without requiring any concept supervision, as concepts can be automatically predicted when using an LLM backbone.",Concept-XAI; Text analysis; Linear concept attribution,WoS
10.1016/j.amjsurg.2025.116229,Management of lower extremity traumas: Comparing appropriate use criteria ChatGPT recommendations,10.1016/j.amjsurg.2025.116229,"Background: High-energy lower extremity injury presents with difficult clinical decisions because successful limb salvage is the best scenario for complex traumas, but early amputation may be necessary to limit complications. Artificial Intelligence is a tool rising in popularity to help make clinical judgements. Purpose/questions: The aim of this study is to determine whether ChatGPT-4 can produce accurate recommendations for limb salvage or amputation given various patient scenarios. Methods: Various lower leg trauma scenarios were given to the appropriate use criteria for limb salvage made by AAOS or ChatGPT-4. A recommendation score for limb salvage and early amputation were collected. Tests to determine statistical significance between AAOS and ChatGPT-4 were performed. Results: A total of 196 patient scenario combinations were utilized. The mean error for limb salvage and early amputation were −0.3 and −0.2 respectively. AAOS and ChatGPT had significant positive correlations when predicting limb salvage and early amputation scores. The effect size of limb salvage and early amputation was −0.094 and −0.14, respectively. Conclusion: ChatGPT-4 generally under-estimates appropriateness scores for both limb salvage and early amputation treatment options, but produces similar scores. ChatGPT-4 may be used to aid physicians in choosing between limb salvage and early amputation, though with caution. © 2025 Elsevier B.V., All rights reserved.","Adult; Amputation; Article; Artificial Intelligence; Bone Injury; Chatgpt; Clinical Outcome; Comparative Study; Contamination; Decision Making; High Risk Patient; Human; Limb Salvage; Lower Leg; Major Clinical Study; Muscle Injury; Clinical Decision Making; Female; Leg Injury; Male; Procedures; Surgery; Therapy; Adult; Amputation, Surgical; Artificial Intelligence; Clinical Decision-making; Female; Humans; Leg Injuries; Limb Salvage; Male",Scopus
WOS:001481727900001,Measuring the commercial potential of science,10.1002/smj.3720,"Research SummaryWe develop an ex ante measure of commercial potential of science, an otherwise unobservable variable driving the performance of innovation-intensive firms. To do so, we rely on large language models and neural networks to predict whether scientific articles will influence firms' use of science. Incorporating time-varying models and the quantification of uncertainty, the measure is validated through both traditional methods and out-of-sample exercises, leveraging a major university's technology transfer data. To illustrate the methodological contributions of our measure, we apply it to examining the impact of university reputation and university privatization of science, finding that firms' reliance on reputation may lead to foregone opportunities, and privatization (i.e., patenting) appears to increase firms' use of the science of one university. We make our measure and method available to researchers.Managerial SummaryUsing machine learning, we develop a measure that estimates the probability that a scientific discovery will contribute to a commercially valuable innovation. This work addresses a key challenge: the inability to observe what scientific discoveries are worth pushing forward into commercial application. We illustrate the usefulness of this measure with two examples: 1.) firms' use of research from prestigious universities over equally promising work from less prominent ones; and 2.) how patenting affects the diffusion of commercially relevant science across firms. For practitioners, this measure can inform R&D, licensing, and other innovation related decisions by guiding firms' search for commercially relevant scientific research. The measure and the associated code are publicly available.",deep learning; innovation; large language models; science commercialization; technological opportunity,WoS
10.1016/j.artmed.2024.102938,MedExpQA: Multilingual benchmarking of Large Language Models for Medical Question Answering,10.1016/j.artmed.2024.102938,"Large Language Models (LLMs) have the potential of facilitating the development of Artificial Intelligence technology to assist medical experts for interactive decision support. This potential has been illustrated by the state-of-the-art performance obtained by LLMs in Medical Question Answering, with striking results such as passing marks in licensing medical exams. However, while impressive, the required quality bar for medical applications remains far from being achieved. Currently, LLMs remain challenged by outdated knowledge and by their tendency to generate hallucinated content. Furthermore, most benchmarks to assess medical knowledge lack reference gold explanations which means that it is not possible to evaluate the reasoning of LLMs predictions. Finally, the situation is particularly grim if we consider benchmarking LLMs for languages other than English which remains, as far as we know, a totally neglected topic. In order to address these shortcomings, in this paper we present MedExpQA, the first multilingual benchmark based on medical exams to evaluate LLMs in Medical Question Answering. To the best of our knowledge, MedExpQA includes for the first time reference gold explanations, written by medical doctors, of the correct and incorrect options in the exams. Comprehensive multilingual experimentation using both the gold reference explanations and Retrieval Augmented Generation (RAG) approaches show that performance of LLMs, with best results around 75 accuracy for English, still has large room for improvement, especially for languages other than English, for which accuracy drops 10 points. Therefore, despite using state-of-the-art RAG methods, our results also demonstrate the difficulty of obtaining and integrating readily available medical knowledge that may positively impact results on downstream evaluations for Medical Question Answering. Data, code, and fine-tuned models will be made publicly available.1 © 2024 Elsevier B.V., All rights reserved.",Large Language Models; Medical Question Answering; Multilinguality; Natural Language Processing; Retrieval Augmented Generation; Computational Linguistics; Decision Support Systems; Gold; Medical Applications; Natural Language Processing Systems; Artificial Intelligence Technologies; Language Model; Language Processing; Large Language Model; Medical Knowledge; Medical Question Answering; Multilinguality; Natural Language Processing; Natural Languages; Retrieval Augmented Generation; Benchmarking; Article; Benchmarking; Controlled Study; Data Accuracy; English (language); French (language); Information Retrieval; Italian (language); Knowledge; Large Language Model; Medical Examination; Multilingualism; Physician; Spanish (language); Artificial Intelligence; Human; Natural Language Processing; Artificial Intelligence; Humans; Multilingualism; Natural Language Processing,Scopus
10.1109/ACCESS65134.2025.11135599,MediFlow - AI and Blockchain in Pharmaceutical Supply Chain: Enhancing Traceability and Decision-Making,10.1109/access65134.2025.11135599,"MediFlow is an advanced pharmaceutical supply-chain management platform leveraging blockchain technology to enhance efficiency and transparency in drug distribution. By integrating Ethereum blockchain with predictive analytics powered by the TIME LLM model, MediFlow facilitates secure and streamlined drug procurement and delivery processes. The system utilizes smart contracts to automate transactions, optimize vendor collaboration, and eliminate unnecessary intermediaries, leading to improved efficiency and stakeholder trust. Real-time inventory and shipment tracking capabilities reduce logistical delays and errors, while predictive demand analysis helps mitigate stock shortages and minimize waste. Furthermore, the immutable nature of blockchain technology ensures the authenticity of medicines, thereby reducing the prevalence of counterfeit drugs in the market. Implementation results indicate notable improvements in transparency, cost-effectiveness, and overall healthcare service delivery, making MediFlow a transformative solution for pharmaceutical supplychain management. © 2025 Elsevier B.V., All rights reserved.",Blockchain; Drug Supply Chain; Predictive Analytics; Smart Contracts; Time Llm Model; Big Data; Chains; Controlled Drug Delivery; Cost Effectiveness; Decision Making; Drug Discovery; Predictive Analytics; Smart Contract; Supply Chain Management; Supply Chains; Targeted Drug Delivery; Block-chain; Chain Management; Decisions Makings; Drug Distribution; Drug Supply; Drug Supply Chain; Management Platforms; Pharmaceutical Supply Chains; Platform Leveraging; Time Llm Model; Blockchain,Scopus
WOS:001547314300015,Mitigating Catastrophic Forgetting in Molecular Property Prediction via Refresh Learning and Pareto Optimization,10.1109/tcbbio.2025.3571046,"Continual Learning (CL) enables Large Language Models (LLMs) to adapt to new episodes and data streams without forgetting previously acquired knowledge, a critical requirement for dynamic fields like Molecular property Prediction (MP). LLMs, however, often face Catastrophic Forgetting (CF), where learning new information erodes prior knowledge, particularly when data distributions shift significantly between episodes, as seen in chemical, genomic, and proteomic datasets. To address CF, the existing replay-based techniques use memory buffers to store past episode data but often overlook the relationships between episodes, resulting in sub-optimal performance when revisiting earlier episodes. To this, the paper proposes a Multi-task Learning (MTL) framework that reconciles existing CL techniques into a unified hierarchical gradient aggregation framework. It builds a novel framework using the ChemBERTa model, namely MTL-PORL (Multi-task Learner-Pareto Optimized Refresh Learning), i.e., Refresh Learning (RL), inspired by neuroscience, where the brain discards outdated information to enhance retention and facilitate new learning with Pareto Optimization (PO) for MP. The hyper-gradient approach in the MTL-PORL leverages unlearning current data before relearning it, acting as a flexible plug-in that enhances existing CL methods. The MTL-PORL exhibits Anytime Average Accuracy (91.63%, 94.89%, and 92.67%), Test Accuracy (92.48%, 96.48%, and 96.86%), and Forgetting Measure (-0.0048, -0.0045, and -0.0063) on the BBBP, bitter, and sweet datasets, respectively. The comprehensive empirical analysis highlighted significant improvements in sequential learning compared to existing methods, addressing the stability-plasticity trade-off and effectiveness of RL.",Adaptation models; Drugs; Computational modeling; Data models; Predictive models; Drug discovery; Diffusion tensor imaging; Multitasking; Bioinformatics; Pareto optimization; refresh learning; catastrophic forgetting; multi-task learner (MTL); simplified input line entry system (SMILES),WoS
WOS:001428007500001,Model interpretability enhances domain generalization in the case of textual complexity modeling,10.1016/j.patter.2025.101177,"Balancing prediction accuracy, model interpretability, and domain generalization (also known as [a.k.a.] out- of-distribution testing/evaluation) is a central challenge in machine learning. To assess this challenge, we took 120 interpretable and 166 opaque models from 77,640 tuned configurations, complemented with ChatGPT, 3 probabilistic language models, and Vec2Read. The models first performed text classification to derive principles of textual complexity (task 1) and then generalized these to predict readers' appraisals of processing difficulty (task 2). The results confirmed the known accuracy-interpretability trade-off on task 1. However, task 2's domain generalization showed that interpretable models outperform complex, opaque models. Multiplicative interactions further improved interpretable models' domain generalization incrementally. We advocate for the value of big data for training, complemented by (1) external theories to enhance interpretability and guide machine learning and (2) small, well-crafted out-of-distribution data to validate models-together ensuring domain generalization and robustness against data shifts.",,WoS
2-s2.0-85217810699,Modeling Interactions Between Stocks Using LLM-Enhanced Graphs for Volume Prediction,,"Accurate trading volume prediction is essential for portfolio optimization, market regulation, and financial risk control. An effective method for predicting trading volume involves building a graph to model relations between stocks. Recent research has enhanced these models by integrating stock news to improve forecasting ability. However, existing approaches primarily integrate news data as auxiliary features for nodes in Graph Neural Networks (GNNs), often overlooking the relational information between stocks embedded in news. To address this, we propose LLM-Enhanced Dynamic Graph Neural Network (LED-GNN), a framework that constructs dynamic graphs using inter-stock relationships extracted from news via a large language model (LLM)-centered pipeline. The news graph is then combined with graphs learned from historical price-volume data and fed into a dynamic GNN to generate predictions. Evaluated on a real-world dataset, TOPIX, with Reuters Financial News, LED-GNN consistently outperformed all baseline models, achieving a 2% improvement over the strongest baseline. © 2025 Elsevier B.V., All rights reserved.",Graph Neural Networks; Dynamic Graph; Financial Risks; Language Model; Market Regulation; Model Interaction; Portfolio Optimization; Risks Controls; Trading Volumes; Volume Predictions; Prediction Models,Scopus
10.1111/cogs.13312,Modeling Structure-Building in the Brain With CCG Parsing and Large Language Models,10.1111/cogs.13312,"To model behavioral and neural correlates of language comprehension in naturalistic environments, researchers have turned to broad-coverage tools from natural-language processing and machine learning. Where syntactic structure is explicitly modeled, prior work has relied predominantly on context-free grammars (CFGs), yet such formalisms are not sufficiently expressive for human languages. Combinatory categorial grammars (CCGs) are sufficiently expressive directly compositional models of grammar with flexible constituency that affords incremental interpretation. In this work, we evaluate whether a more expressive CCG provides a better model than a CFG for human neural signals collected with functional magnetic resonance imaging (fMRI) while participants listen to an audiobook story. We further test between variants of CCG that differ in how they handle optional adjuncts. These evaluations are carried out against a baseline that includes estimates of next-word predictability from a transformer neural network language model. Such a comparison reveals unique contributions of CCG structure-building predominantly in the left posterior temporal lobe: CCG-derived measures offer a superior fit to neural signals compared to those derived from a CFG. These effects are spatially distinct from bilateral superior temporal effects that are unique to predictability. Neural effects for structure-building are thus separable from predictability during naturalistic listening, and those effects are best characterized by a grammar whose expressive power is motivated on independent linguistic grounds. © 2023 Elsevier B.V., All rights reserved.",Fmri; Grammar; Language Modeling; Neural Networks; Parsing; Surprisal; Syntax; Brain; Brain Mapping; Comprehension; Diagnostic Imaging; Hearing; Human; Language; Linguistics; Auditory Perception; Brain; Brain Mapping; Comprehension; Humans; Language; Linguistics,Scopus
10.13336/j.1003-6520.hve.20241015,Multi-objective Collaborative Operation Method for Park-level Integrated Energy System Cluster Based on Large Language Model for Green Electricity Prediction and Trading; 基于大语言模型绿电预测和绿电交易的园区综合能源系统集群多目标协同运行方法,10.13336/j.1003-6520.hve.20241015,"In order to achieve digital and intelligent upgrading of traditional industrial parks, and to support high-quality regional development, there is an urgent need for intelligent dispatch models in parks. Therefore, this paper combines the smart park management system and the physical model of the park-level integrated energy system (PIES) to establish a cluster architecture of the PIES. A three-stage collaborative operation method for green electricity trading in the PIES cluster is proposed to solve the problem of green electricity trading in multiple PIES, and to achieve accurate prediction of distributed renewable energy and on-site consumption. Firstly, based on the large language model LLAMA-7B, green electricity prediction is achieved, and further green electricity power is divided into purchasing and selling electricity PIES. Secondly, based on the green electricity price quota curve prediction model and dynamic green electricity pricing strategy, the differential prices for green electricity trading between parks is established. On this basis, a multi-objective low-carbon economic optimization operation model is established to solve the contradiction between economic and environmental factors brought about by the green electricity exchange. The example analysis shows that the proposed model can comprehensively schedule the economic cost, actual carbon emissions, and renewable energy utilization rate of the PIES cluster, which has a positive promoting effect on the intelligent dispatch of multiple PIES. © 2024 Elsevier B.V., All rights reserved.",Green Electricity Trading; Large Language Model; Low-carbon Economic Dispatch; Multi-object Optimization; Park-level Integrated Energy System; Time Series Data Prediction; Carbon; Cluster Computing; Commerce; Cost Benefit Analysis; Economic Analysis; Electric Load Dispatching; Energy Utilization; Forecasting; Data Prediction; Economic Dispatch; Electricity Trading; Green Electricity; Green Electricity Trading; Integrated Energy Systems; Language Model; Large Language Model; Low-carbon Economic; Low-carbon Economic Dispatch; Multi-object Optimization; Park-level Integrated Energy System; Time Series Data Prediction; Time-series Data; Parks,Scopus
10.1016/j.energy.2025.138377,Multivariate events enhanced pre-trained large language model for carbon price forecasting,10.1016/j.energy.2025.138377,"Carbon prices are affected by multiple factors, exhibiting high volatility and nonlinear complexity, making their accurate prediction still a challenging task. However, existing models are limited in scale and struggle to effectively extract complex features from high-dimensional data. Recently, pre-trained large language models have demonstrated significant performance advantages in a variety of tasks, but their application in the field of carbon price prediction remains limited. Therefore, this study proposes a multivariate event-enhanced pre-trained large language model, which is constructed based on pre-trained large language models and thus does not require large data for fine-tuning. Specifically, this study first adopts the least absolute shrinkage and selection operator method to identify the key variables that have the most significant impact on carbon price. Meanwhile, unstructured events information is quantified through text sentiment analysis techniques, thus forming a complete model input dataset. Subsequently, these datasets are sequentially processed through the input module, the frozen large language model and the output module to finally generate the prediction results. Carbon price datasets from Hubei and Shanghai carbon markets are used as study cases. The experimental results show that the proposed multivariate events enhanced pre-trained large language model exhibits significant advantages in both prediction accuracy and robustness compared with other existing artificial models. © 2025 Elsevier B.V., All rights reserved.",Carbon Price Forecasting; Large Language Model; Multivariate Prediction; Policies And News Information; Carbon; Carbon Economy; Costs; Data Mining; Large Datasets; Prediction Models; Carbon Price; Carbon Price Forecasting; High Volatility; Language Model; Large Language Model; Multiple Factors; Multivariate Prediction; News Information; Policy And News Information; Price Forecasting; Forecasting; Carbon Emission; Emissions Trading; Language; Model Test; Multivariate Analysis; Price Determination; China; Hubei; Shanghai,Scopus
10.3390/app14167015,New Approach for Automated Explanation of Material Phenomena (AA6082) Using Artificial Neural Networks and ChatGPT,10.3390/app14167015,"Artificial intelligence methods, especially artificial neural networks (ANNs), have increasingly been utilized for the mathematical description of physical phenomena in (metallic) material processing. Traditional methods often fall short in explaining the complex, real-world data observed in production. While ANN models, typically functioning as “black boxes”, improve production efficiency, a deeper understanding of the phenomena, akin to that provided by explicit mathematical formulas, could enhance this efficiency further. This article proposes a general framework that leverages ANNs (i.e., Conditional Average Estimator—CAE) to explain predicted results alongside their graphical presentation, marking a significant improvement over previous approaches and those relying on expert assessments. Unlike existing Explainable AI (XAI) methods, the proposed framework mimics the standard scientific methodology, utilizing minimal parameters for the mathematical representation of physical phenomena and their derivatives. Additionally, it analyzes the reliability and accuracy of the predictions using well-known statistical metrics, transitioning from deterministic to probabilistic descriptions for better handling of real-world phenomena. The proposed approach addresses both aleatory and epistemic uncertainties inherent in the data. The concept is demonstrated through the hot extrusion of aluminum alloy 6082, where CAE ANN models and predicts key parameters, and ChatGPT explains the results, enabling researchers and/or engineers to better understand the phenomena and outcomes obtained by ANNs. © 2024 Elsevier B.V., All rights reserved.",Aluminum Alloy; Artificial Neural Networks; Automatic Explanation; Chatgpt; Hot Extrusion; Large Language Models; Gluing; Artificial Neural Network Modeling; Automatic Explanation; Chatgpt; Hot Extrusion; Language Model; Large Language Model; Neural-networks; New Approaches; Physical Phenomena; Real-world; Metal Extrusion,Scopus
10.1007/978-981-97-5934-7_19,News that Moves the Market: DSEX-News Dataset for Forecasting DSE Using BERT,10.1007/978-981-97-5934-7_19,"Stock market is a complex and dynamic industry that has always presented challenges for stakeholders and investors due to its unpredictable nature. This unpredictability motivates the need for more accurate prediction models. Traditional prediction models have limitations in handling the dynamic nature of the stock market. Additionally, previous methods have used less relevant data, leading to suboptimal performance. This study proposes the use of Bidirectional Encoder Representations from Transformers (BERT), a pre-trained Large Language Model (LLM), to predict Dhaka Stock Exchange (DSE) market movements. We also introduce a new dataset designed specifically for this problem, capturing important characteristics and patterns that were missing in other datasets. We test our new dataset of headlines and stock market indexes on various machine learning techniques, including Decision Tree (DT), Logistic Regression (LR), K-Nearest Neighbors (KNN), Random Forest (RF), Linear Support Vector Machine (LSVM), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRUs), Bidirectional Long Short-Term Memory (Bi-LSTM), BERT, Financial Bidirectional Encoder Representations from Transformers (FinBERT), and RoBERTa, which are compared to assess their predictive capabilities. Our proposed model achieves 99.83% accuracy on the training set and 99.78% accuracy on the test set, outperforming previous methods. © 2024 Elsevier B.V., All rights reserved.",Deep Learning; Large Language Model; Machine Learning; Natural Language Processing; Sentiment Analysis; Stock Exchange; Commerce; Financial Markets; Investments; K-nearest Neighbors; Logistic Regression; Marketplaces; Prediction Models; Support Vector Machines; Deep Learning; Language Model; Language Processing; Large Language Model; Machine-learning; Natural Language Processing; Natural Languages; Prediction Modelling; Sentiment Analysis; Stock Exchange; Long Short-term Memory,Scopus
WOS:001356729806013,On Measuring Faithfulness or Self-consistency of Natural Language Explanations,,"Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models' inner workings - but rather their self-consistency at output level. Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model's input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests. Our code is available at https://github.com/Heidelberg-NLP/CC-SHAP",,WoS
WOS:001238391600046,On-device query intent prediction with lightweight LLMs to support ubiquitous conversations,10.1038/s41598-024-63380-6,"Conversational Agents (CAs) have made their way to providing interactive assistance to users. However, the current dialogue modelling techniques for CAs are predominantly based on hard-coded rules and rigid interaction flows, which negatively affects their flexibility and scalability. Large Language Models (LLMs) can be used as an alternative, but unfortunately they do not always provide good levels of privacy protection for end-users since most of them are running on cloud services. To address these problems, we leverage the potential of transfer learning and study how to best fine-tune lightweight pre-trained LLMs to predict the intent of user queries. Importantly, our LLMs allow for on-device deployment, making them suitable for personalised, ubiquitous, and privacy-preserving scenarios. Our experiments suggest that RoBERTa and XLNet offer the best trade-off considering these constraints. We also show that, after fine-tuning, these models perform on par with ChatGPT. We also discuss the implications of this research for relevant stakeholders, including researchers and practitioners. Taken together, this paper provides insights into LLM suitability for on-device CAs and highlights the middle ground between LLM performance and memory footprint while also considering privacy implications.",Conversational Agents; Design; Information retrieval; Graphical user nterfaces,WoS
10.1109/ICWS62655.2024.00098,OptLLM: Optimal Assignment of Queries to Large Language Models,10.1109/icws62655.2024.00098,"Large Language Models (LLMs) have garnered considerable attention owing to their remarkable capabilities, leading to an increasing number of companies offering LLMs as services. Different LLMs achieve different performance at different costs. A challenge for users lies in choosing the LLMs that best fit their needs, balancing cost and performance. In this paper, we propose a framework for addressing the cost-effective query allocation problem for LLMs. Given a set of input queries and candidate LLMs, our framework, named OptLLM, provides users with a range of optimal solutions to choose from, aligning with their budget constraints and performance preferences, including options for maximizing accuracy and minimizing cost. OptLLM predicts the performance of candidate LLMs on each query using a multi-label classification model with uncertainty estimation and then iteratively generates a set of non-dominated solutions by destructing and reconstructing the current solution. To evaluate the effectiveness of OptLLM, we conduct extensive experiments on various types of tasks, including text classification, question answering, sentiment analysis, reasoning, and log parsing. Our experimental results demonstrate that OptLLM substantially reduces costs by 2.40% to 49.18% while achieving the same accuracy as the best LLM. Compared to other multi-objective optimization algorithms, OptLLM improves accuracy by 2.94% to 69.05% at the same cost or saves costs by 8.79% and 95.87% while maintaining the highest attainable accuracy. © 2025 Elsevier B.V., All rights reserved.",Cost-performance Tradeoff; Large Language Models; Multi-objective Optimization; Performance Prediction; Query Assignment; Cost Benefit Analysis; Query Languages; Query Processing; Structured Query Language; Cost Performance; Cost-performance Tradeoff; Language Model; Large Language Model; Multi-objectives Optimization; Optimal Assignment; Performance; Performance Prediction; Performance Tradeoff; Query Assignment; Budget Control,Scopus
10.1145/3724154.3724240,Optimizing Stock Market Return Forecasts with Uncertainty Sentiment: Leveraging LLM-based Insights,10.1145/3724154.3724240,"Accurately predicting stock market returns is crucial for both investors and policy makers. This study proposes an innovative hybrid Particle Swarm Optimization Support Vector Regression (PSO-SVR) machine learning framework that integrates uncertainty sentiment to improve the accuracy of stock market return prediction. Uncertainty sentiment and historical stock market returns are identified as the main input factors, and the PSO algorithm is used to fine-tune the parameters of SVR to obtain the integrated PSO-SVR model. Empirical results show that the PSO-SVR model significantly reduces the root mean square error (RMSE), mean absolute error (MAE) and mean absolute percentage error (MAPE) when uncertainty sentiment is added. This provides a novel and effective method for predicting stock market returns. © 2025 Elsevier B.V., All rights reserved.",Empirical Asset Pricing; Machine Learning; Pso-svr Hybrid Model; Uncertainty Sentiment; Asset Pricing; Empirical Asset Pricing; Hybrid Model; Machine-learning; Particle Swarm; Particle Swarm Optimization Support Vector Regression Hybrid Model; Support Vector Regressions; Swarm Optimization; Uncertainty; Uncertainty Sentiment; Mean Square Error,Scopus
WOS:001537915200008,Optimizing the Utilization of Large Language Models via Schedule Optimization: An Exploratory Study,10.1145/3674805.3686671,"Background: Large Language Models (LLMs) have gained significant attention in machine-learning-as-a-service (MLaaS) offerings. In-context learning (ICL) is a technique that guides LLMs towards accurate query processing by providing additional information. However, longer prompts lead to higher costs of LLM service, creating a performance-cost trade-off. Aims: We aim to investigate the potential of combining schedule optimization with ICL to optimize LLM utilization. Method: We conduct an exploratory study. First, we consider the performance-cost trade-off in LLM utilization as a multi-objective optimization problem, aiming to select the most suitable prompt template for each LLM job to maximize accuracy (the percentage of correctly processed jobs) and minimize invocation cost. Next, we investigate three methods for prompt performance prediction to address the challenge of evaluating the accuracy objective in the fitness function, as the result can only be determined after submitting the job to the LLM. Finally, we apply widely used search-based techniques and evaluate their effectiveness. Results: The results indicate that the machine learning-based technique is an effective approach for prompt performance prediction and fitness function calculation. Schedule optimization can achieve higher accuracy or lower cost by selecting a suitable prompt template for each job, compared to simply submitting all jobs using a single prompt template, e.g., saving costs from 21.33% to 86.92% in our experiments on LLM-based log parsing. However, the performance of the evaluated search-based techniques varies across different instances and metrics, with no single technique consistently outperforming the others. Conclusions: This study demonstrates the potential of combining schedule optimization with ICL to improve the utilization of LLMs. However, there is still ample room for improving the searched-based techniques and prompt performance prediction techniques for more cost-effective LLM utilization.",Large Language Models; Schedule Optimization; Multi-objective Optimization; Search-based Techniques,WoS
WOS:001199449109117,Panel: Multimodal Large Foundation Models,10.1145/3581783.3617350,"The surprisingly fluent predictive performance of LLM (Large Language Models) as well as the high-quality photo-realistic rendering of Diffusion Models has heralded a new beginning in the area of Generative AI. Such kinds of deep learning based models with billions of parameters and pre-trained on massive-scale data-sets are also called Large Foundation Models (LFM). These models not only have caught the public imagination but also have led to an unprecedented surge in interest towards the applications of these models. Instead of the previous approach of developing AI models for specific tasks, more and more researchers are developing large task-agnostic models pre-trained on massive data, which can then be adapted to a variety of downstream tasks via fine-tuning, fewshot learning, or zero-shot learning. Some examples are ChatGPT, LLaMA, GPT-4, Flamingo, MidJourney, Stable-Diffusion and DALLE. Some of them can handle text (e.g., ChatGPT, LLaMA) while some others (e.g., GPT-4 and Flamingo) can utilize multimodal data and can hence be considered Multimodal Large Foundation Models (MLFM). Several recent studies have shown that when adapted to specific tasks (e.g., visual question answering), the foundation models can often surpass the performance of state-of-the-art, fully supervised AI models. However, applying foundation models to specialized domain tasks (e.g., medical diagnosis, financial recommendation etc.) raises many ethical issues (e.g., privacy, model bias or hallucinations). The panel members will discuss the emerging trends in the development and use of large multimodal foundation models. Some of the issues to be discussed are: center dot Research issues in going from LLM to MLFM center dot Behaviour of MLFM center dot Application Potential of MLFM center dot Trust issues in MLFM center dot Limitations of MLFM center dot Societal, Legal and Regulatory issues of MLFM center dot Promising future research in MLFM This panel will bring together several leading experts from universities, research institutions, and industry who will discuss and debate together with the audience. We invite everybody to participate and contribute towards this important and promising research direction.",Large Language Models; Foundation Models; Deep Learning; Multimodal Models,WoS
10.1145/3629527.3651436,Performance Optimization in the LLM World 2024,10.1145/3629527.3651436,"The popularity and adoption of large language models (LLM) like ChatGPT has evolved rapidly. LLM pre-training is expensive. ChatGPT is estimated to cost over 700,000 per day to operate, and using GPT-4 to support customer service can cost a small business over 21,000 a month. The high infrastructure and financial costs, coupled with the specialized talent required, make LLM technology inaccessible to most organizations. For instance, the up-front costs include the emissions generated to manufacture the relevant hardware and the cost to run that hardware during the training procedure, both while the machines are operating at full capacity and while they are not. The best estimate of the dynamic computing cost in the case of GPT-3, the model behind the original ChatGPT, is approximately 1,287,000 kWh, or 552 tons of carbon dioxide. The goal of this workshop is to address the urgency of reducing energy consumption of LLM applications, by bringing together researchers from the academia and industry to share their experience and insights in performance engineering in the LLM world. © 2024 Elsevier B.V., All rights reserved.",Cost Benefit Analysis; Cost Estimating; Energy Utilization; Best Estimates; Customer-service; Financial Costs; Infrastructure Costs; Language Model; Modeling Technology; Performance Optimizations; Pre-training; Small Business; Training Procedures; Carbon Dioxide,Scopus
WOS:001181085100165,Performance and Risk Trade-offs for Multi-word Text Prediction at Scale Warning: The paper contains examples which the reader might find offensive.,,"Large Language Models such as GPT-3 are well-suited for text prediction tasks, which can help and delight users during text composition. LLMs are known to generate ethically inappropriate predictions even for seemingly innocuous contexts. Toxicity detection followed by filtering is a common strategy for mitigating the harm from such predictions. However, as we shall argue in this paper, in the context of text prediction, it is not sufficient to detect and filter toxic content. One also needs to ensure factual correctness and group-level fairness of the predictions; failing to do so can make the system ineffective and nonsensical at best, and unfair and detrimental to the users at worst. We discuss the gaps and challenges of toxicity detection approaches - from blocklist-based approaches to sophisticated state-of-the-art neural classifiers - by evaluating them on the text prediction task for English against a manually crafted CheckList of harms targeted at different groups and different levels of severity.",,WoS
2-s2.0-105000551550,PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations,,"Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through knowledge-invariant perturbations. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of response consistency analyses that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1145/3701716.3715254,Ploutos: Towards Explainable Stock Movement Prediction with Financial Large Language Model,10.1145/3701716.3715254,"Recent advancements in large language models (LLMs) have opened new pathways for many domains. However, the full potential of LLMs in financial investments remains largely untapped. There are two main challenges for typical deep learning-based methods for quantitative finance. First, they struggle to fuse textual and numerical information flexibly for stock movement prediction. Second, traditional methods lack clarity and explainability, which impedes their application in scenarios where the justification for predictions is essential. To solve the above challenges, we propose Ploutos, a novel financial LLM framework that consists of PloutosGen and PloutosGPT. The PloutosGen contains multiple primary experts that can analyze different modal data, such as text and numbers, and provide quantitative strategies from different perspectives. Then PloutosGPT combines their insights and predictions and generates explainable rationales. To generate accurate and faithful rationales, the training strategy of PloutosGPT leverages a rearview-mirror prompting mechanism to guide GPT-4 to generate rationales and a dynamic token weighting mechanism to finetune LLM by detecting and emphasizing key tokens in rationales. Extensive experiments show our framework outperforms the state-of-the-art methods on both prediction accuracy and explainability. © 2025 Elsevier B.V., All rights reserved.",Computational Finance; Large Language Model; Quantitative Investment; Stock Prediction; Deep Learning; Economics; Investments; Learning Systems; Modal Analysis; Computational Finance; Financial Investments; Language Model; Large Language Model; Learning-based Methods; Movement Prediction; Quantitative Investment; Stock Movement; Stock Predictions; Textual Information; Forecasting,Scopus
10.1007/978-3-031-99353-4_33,Predicting Startup IPO: A Data-Driven Approach Using Crunchbase Insights,10.1007/978-3-031-99353-4_33,"The startup ecosystem is constantly evolving, posing significant challenges for investors and venture capitalists (VCs) in identifying and evaluating the most promising startups for investment. Additionally, To address these challenges, a comprehensive solution has been developed that leverages data analytics, including Crunchbase data and machine learning models, to provide tailored insights. By integrating large language model capabilities, the platform delivers customized predictions and evaluates critical factors such as industry dynamics, funding rounds, team composition, and growth metrics, empowering users to make well-informed decisions. For investors and VCs, the platform offers a data-driven approach to identifying high-growth organizations by analyzing market trends, financial data, and various other non-financial metrics. Founders and aspiring entrepreneurs can use the platform to validate their startup ideas, analyze competitors, and explore funding opportunities, with the help of industry-specific machine learning models that assess the viability of their ventures. Startup enthusiasts can stay informed about market trends and draw inspiration from success stories, gaining deeper insights into the startup ecosystem. © 2025 Elsevier B.V., All rights reserved.",Api; Cosine Similarity; Explainable Ai; Feature Engineering; Llm; Machine Learning; Pearson Correlation; Startup Ipo Prediction; Tf-idf (term Frequency-inverse Document Frequency); Commerce; Correlation Methods; Inverse Problems; Investments; Machine Learning; Reactor Startup; Api; Cosine Similarity; Explainable Ai; Feature Engineerings; Llm; Machine-learning; Pearson Correlation; Startup Ipo Prediction; Term Frequency-inverse Document Frequency; Term Frequencyinverse Document Frequency (tf-idf); Learning Systems,Scopus
10.1007/978-3-031-97564-6_24,Predicting Stock Prices with ChatGPT-Annotated Reddit Sentiment: Hype or Reality?,10.1007/978-3-031-97564-6_24,"The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit’s r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment’s role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models’ predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions. © 2025 Elsevier B.V., All rights reserved.",Chatgpt; Sentiment; Social Media; Stock Market; Commerce; Costs; Data Mining; Electronic Trading; Investments; Sales; Sentiment Analysis; Social Networking (online); Analysis Method; Chatgpt; Model Predictive; Predictive Power; Sentiment; Simple Metrics; Social Media; Stock Price; Weak Correlation; Financial Markets,Scopus
10.1016/j.procs.2025.08.075,Prediction of Crude Oil Price using LLM: An Empirical Analysis,10.1016/j.procs.2025.08.075,"In this paper we conducted empirical studies to investigate the application of large language model to evaluate and estimate investment sentiment from the news text on the internet. We further proposed a crude oil price forecasting model with the estimated investment sentiment scores using LLM. Empirical evaluations using the crude oil spot prices demonstrate the superior forecasting accuracy of the large language model-based approach to forecast the future crude oil price movements. This result suggests that the large language model extracts valuable information from the news text, enhancing the predictive modeling of the crude oil price movements. © 2025 Elsevier B.V., All rights reserved.",Crude Oil Price Forecasting; Large Language Model; Time Series Forecasting; Transformer Model; Costs; Crude Oil Price; Forecasting; Investments; Petroleum Analysis; Prediction Models; Sentiment Analysis; Time Series Analysis; Crude Oil Price Forecasting; Crude Oil Prices; Empirical Analysis; Empirical Studies; Forecasting Models; Language Model; Large Language Model; Price Movement; Time Series Forecasting; Transformer Modeling; Modeling Languages,Scopus
2-s2.0-85216409938,Prediction of Foreign Exchange Rates by a Large Language Model,,"This paper proposes a prompt-based method utilizing a large language model (LLM) to predict changes in foreign exchange rates based on limit order information. While traditional deep learning models for prediction utilize numerical values as input and output, LLMs use sentences and prompts. To address this, we design prompts that incorporate the numerical values. GPT-2, a widely adopted LLM, is employed and fine-tuned using a training dataset. The effectiveness of our proposed method is demonstrated through empirical analysis using actual time series data. © 2025 Elsevier B.V., All rights reserved.",Finance; Foreign Exchange Rate; Large Language Model; Machine Learning; Time Series; Adversarial Machine Learning; Contrastive Learning; Decentralized Finance; Deep Learning; Foreign Exchange Rates; Input And Outputs; Language Model; Large Language Model; Learning Models; Limit Orders; Machine-learning; Numerical Values; Times Series; Training Dataset; Prediction Models,Scopus
10.54808/WMSCI2023.01.168,"Predictive Analytics Based on Digital Twins, Generative AI, and ChatGPT",10.54808/wmsci2023.01.168,"ChatGPT (Chat Generative Pre-Trained Transformer) is one of the latest technologies in modern Artificial Intelligence (AI) and probably the technology with the highest impact in this area for the near future. The latest version of ChatGPT - GPT-4 has improved in several areas, including Predictive Analytics. Generative AI and Ghat GPT can be used in different areas - not only to generate human-like content easily but also in different business domains in the modern industry, like the construction industry. This research gives an overview of the application of Generative AI, particularly ChatGPT, for predictive analytics in different areas focusing on the construction and building industry. The paper analyzes options to use Generative AI together with another essential for modern analysis technology - Digital Twins in two different aspects: 1) To design and build systems for Predictive Analytics 2) To implement Cognitive Digital Twins, The research used prototypes based on Microsoft Power Platform (Power Virtual Agents, Power Automate), Open AI, and Azure Digital Twins, which can offer predictive analytics in the construction industry. The article includes results, providing information about cost savings and time reduction when using Generative AI for predictive analytics in the construction industry. © 2023 Elsevier B.V., All rights reserved.",Ai; Artificial Intelligence; Chatgpt; Digital Twin; Generative Ai; Gpt-4; Industry 4.0; Iot; Machine Learning; Predictive Analytics; Cognitive Systems; Construction Industry; E-learning; Internet Of Things; Learning Systems; Machine Learning; Predictive Analytics; Business Domain; Chat Generative Pre-trained Transformer; Generative Artificial Intelligence; Gpt-4; High Impact; Human Like; Iot; Latest Technology; Machine-learning; Power; Industry 4.0,Scopus
10.1038/s41598-025-06280-7,Predictive model on employee stock ownership impacting corporate performance,10.1038/s41598-025-06280-7,"Enterprises in the context of smart manufacturing face great challenges in terms of human capital strategies as well as incentive mechanisms. Employee Stock Ownership Plans (ESOPs) is one of the key incentive mechanisms with long-term oriented function, but due to the lack of relevant explanations in the context of smart manufacturing, the mechanism of the dynamic impact of ESOPs on corporate performance has not yet been elucidated. In this study, with the idea of combining AI and accounting, we constructed a prediction model of the impact of ESOPs on enterprise performance that integrates language modeling and social sentiment mass data analysis, and introduced the prediction model to analyze the long-term, dynamic and nonlinear impact of ESOPs on enterprises; finally, we constructed an explainable AI (XAI) based on the LSTM model, and used the SHAP value method to explain the impact of ESOPs on enterprise performance. Finally, the Explainable AI (XAI) is built based on the LSTM model, and the SHAP value method is used to downsize the performance of the complex black box model LSTM, present the model “black box”, and analyze the common roles played by the elements of ESOPs, the maturity level of smart manufacturing, and the social sentiment on ESOPs in the long term and nonlinear process. Aiming at the above research problems and shortcomings, the main contributions of this paper include: analyzing the dynamic evolution path of ESOP effectiveness from the perspective of intelligent transformation of manufacturing enterprises; predicting the ESOP effectiveness of enterprises through multi-source heterogeneous data (financial data, social sentiment data, operation data) and advanced AI models (LSTM, LLM), and proposing new prediction tools and prediction theories; using XAI technology to realize ESOP effectiveness; and using XAI technology to realize ESOP effectiveness in the long term and non-linear process. Theory; the use of XAI technology to achieve ESOP incentive effect attribution analysis, for management accounting decision support to provide a new dimension of interpretation, which can be used as a research on ESOP dynamic incentive evaluation, integration of non-financial information, predictive analysis of new perspectives for the field of accounting to develop a new research direction, and for the transformation of intelligent manufacturing design and optimization of ESOP to provide empirical data basis and decision support. The study also provides empirical data basis and decision support for the design and optimization of ESOPs during the transformation of smart manufacturing. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence And Big Data; Corporate Performance; Employee Stock Ownership Plan (esop); Incentive Mechanism; Lstm Prediction Model; Smart Manufacturing; Animal Experiment; Article; Artificial Intelligence; Big Data; Data Analysis; Decision Support System; Employee; Explainable Artificial Intelligence; Human; Incentive; Long Short Term Memory Network; Male; Organization And Management; Prediction; Predictive Model,Scopus
10.3390/math11132883,"Price, Complexity, and Mathematical Model",10.3390/math11132883,"The whole world has entered the era of the Vuca. Some traditional methods of problem analysis begin to fail. Complexity science is needed to study and solve problems from the perspective of complex systems. As a complex system full of volatility and uncertainty, price fluctuations have attracted wide attention from researchers. Therefore, through a literature review, this paper analyzes the research on complex theories on price prediction. The following conclusions are drawn: (1) The price forecast receives widespread attention year by year, and the number of published articles also shows a rapid rising trend. (2) The hybrid model can achieve higher prediction accuracy than the single model. (3) The complexity of models is increasing. In the future, the more complex methods will be applied to price forecast, including AI technologies such as LLM. (4) Crude-oil prices and stock prices will continue to be the focus of research, with carbon prices, gold prices, Bitcoin, and others becoming new research hotspots. The innovation of this research mainly includes the following three aspects: (1) The whole analysis of all the articles on price prediction using mathematical models in the past 10 years rather than the analysis of a single field such as oil price or stock price. (2) Classify the research methods of price forecasting in different fields, and found the common problems of price forecasting in different fields (including data processing methods and model selection, etc.), which provide references for different researchers to select price forecasting models. (3) Use VOSviewer to analyze the hot words appearing in recent years according to the timeline, find the research trend, and provide references for researchers to choose the future research direction. © 2023 Elsevier B.V., All rights reserved.",Algorithm; Chaos; Complexity; Fluctuate; Mathematic; Price; Volatility,Scopus
10.1016/j.renene.2025.124004,Probabilistic prediction of photovoltaic power: A multi-task learning and large language model-based approach,10.1016/j.renene.2025.124004,"Accurate weekly probabilistic forecasting of photovoltaic power holds immense value for optimizing power generation schedules and market trading strategies. However, current research for photovoltaic power forecasting focuses on short-term prediction, and there is insufficient research on weekly probabilistic prediction, especially when data availability is limited. For this purpose, this paper proposes a weekly probabilistic photovoltaic power forecasting approach based on multi-task learning and a large language model (LLM). First, the wavelet transform is employed to decompose the photovoltaic power time series into smoother sub-frequency curves, which are predicted using a new LLM meta AI (LLaMA)-based LLM. The proposed LLM harnesses the shared feature correlations derived from multi-task learning, coupled with the robust generalization capabilities of the pre-trained LLaMA, to effectively capture intricate nonlinear characteristics of photovoltaic power under zero-shot and few-shot data. Then, to adapt to the photovoltaic power prediction task and improve the prediction accuracy, a dilated convolutional bidirectional long short-term memory-based adapter is introduced for fine-tuning the LLM. Finally, a new probabilistic forecasting approach that integrates the proposed LLM with direct probability forecasting methods is introduced to characterize uncertainties across different quantiles, and deterministic forecasting is achieved by setting the quantile to 0.5. The proposed deterministic and probabilistic forecasting performance has been validated using weekly data from two photovoltaic power stations in northwestern China, and experimental results have indicated that the proposed approach achieves an average improvement of 112.16% in the average interval sharpness metric compared with state-of-the-art benchmarks under zero-shot and few-shot data predictions. © 2025 Elsevier B.V., All rights reserved.",Deepseekr1; Fine-tuning Adapter; Large Language Model; Multi-task Learning; Photovoltaic Power; Weekly Probabilistic Forecasting; Forecasting; Learning Systems; Photovoltaics; Power Markets; Tuning; Deepseekr1; Fine Tuning; Fine-tuning Adapter; Language Model; Large Language Model; Multitask Learning; Photovoltaic Power; Probabilistic Forecasting; Probabilistic Prediction; Weekly Probabilistic Forecasting; Wavelet Transforms; Detection Method; Integrated Approach; Photovoltaic System; Power Generation; Probability; Time Series Analysis; China,Scopus
2-s2.0-105013190863,Probabilities of Chat LLMs Are Miscalibrated but Still Predict Correctness on Multiple-Choice Q&A,,"We evaluate 15 large language models (LLMs) fine-tuned for chat on multiple-choice Q&A. Consistent with prior work, we find that their maximum softmax probabilities (MSPs) are consistently miscalibrated on multiple-choice Q&A. However, those MSPs might still encode useful uncertainty information. Specifically, we hypothesized that wrong answers would be associated with smaller MSPs compared to correct answers. Via rigorous statistical testing, we show that this hypothesis holds for models which perform well on the underlying Q&A task. We also find a strong direct correlation between Q&A accuracy and MSP correctness prediction, while finding no correlation between Q&A accuracy and calibration error. This suggests that within the current fine-tuning paradigm, we can expect correctness prediction but not calibration to improve as LLM capabilities progress. To demonstrate the utility of correctness prediction, we show that when models have the option to abstain, performance can be improved by selectively abstaining based on the MSP of the initial model response, using only a small amount of labeled data to choose the MSP threshold. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-105011288552,"Proceedings - 2025 IEEE Ural-Siberian Conference on Biomedical Engineering, Radioelectronics and Information Technology, USBEREIT 2025",,"The proceedings contain 109 papers. The topics discussed include: improving the security of network services by changing the transport protocol port; high sensitivity and compact microwave sensor based on metamaterial and coupled DSRR for iron content in lubricating oils; from data to insights: LightGBM approach in DGA family classification; predicting agricultural commodity stock prices: a comparison of statistical methods and machine learning algorithms; development of an automated system for generating assignments in mathematical disciplines using LLMs; time series forecasting method based on the ensemble of finite state machines; and powerful microwave pulse radio transmitter of coherent aerological radar. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85196751232,"Proceedings of the 6th International Conference on Finance, Economics, Management and IT Business, FEMIB 2024",,"The proceedings contain 14 papers. The topics discussed include: what do customers demand? inclusive and sustainable entrepreneurial marketing; applications of artificial intelligence in sustainability assessment and risk management in European banking; safeguarding downside risk in portfolio insurance: navigating Swiss stock market regimes with options, trading signals, and financial products; internal audit: friend or foe of innovation in an organization: case of Czech banking sector; the recruiting process as an attractiveness factor: how do companies manage to position themselves competitively as employers?; developing a framework for city brand-image promotion via social media communication; leveraging multimodal large language models and natural language processing techniques for comprehensive ESG risk score prediction; ChatGPT in higher education: a risk management approach to academic integrity, critical thinking, and workforce readiness; applying text analytics methodology to analyze project reports; and stock market forecasting using machine learning models through volatility-driven trading strategies. © 2024 Elsevier B.V., All rights reserved.",,Scopus
10.1145/3658644.3690231,ProphetFuzz: Fully Automated Prediction and Fuzzing of High-Risk Option Combinations with Only Documentation via Large Language Model,10.1145/3658644.3690231,"Vulnerabilities related to option combinations pose a significant challenge in software security testing due to their vast search space. Previous research primarily addressed this challenge through mutation or filtering techniques, which inefficiently treated all option combinations as having equal potential for vulnerabilities, thus wasting considerable time on non-vulnerable targets and resulting in low testing efficiency. In this paper, we utilize carefully designed prompt engineering to drive the large language model (LLM) to predict high-risk option combinations (i.e., more likely to contain vulnerabilities) and perform fuzz testing automatically without human intervention. We developed a tool called ProphetFuzz and evaluated it on a dataset comprising 52 programs collected from three related studies. The entire experiment consumed 10.44 CPU years. ProphetFuzz successfully predicted 1748 high-risk option combinations at an average cost of only $8.69 per program. Results show that after 72 hours of fuzzing, ProphetFuzz discovered 364 unique vulnerabilities associated with 12.30% of the predicted high-risk option combinations, which was 32.85% higher than that found by state-of-the-art in the same timeframe. Additionally, using ProphetFuzz, we conducted persistent fuzzing on the latest versions of these programs, uncovering 140 vulnerabilities, with 93 confirmed by developers and 21 awarded CVE numbers. © 2025 Elsevier B.V., All rights reserved.",Fuzzing; Large Language Model; Option-aware; Vulnerability; Program Processors; Filtering Technique; Fully Automated; Fuzzing; Language Model; Large Language Model; Option-aware; Search Spaces; Software Security Testing; Testing Efficiency; Vulnerability; Software Testing,Scopus
WOS:001540696400003,Psychometric Evaluation of Large Language Model Embeddings for Personality Trait Prediction,10.2196/75347,"Background: Recent advancements in large language models (LLMs) have generated significant interest in their potential for assessing psychological constructs, particularly personality traits. While prior research has explored LLMs' capabilities in zero-shot or few-shot personality inference, few studies have systematically evaluated LLM embeddings within a psychometric validity framework or examined their correlations with linguistic and emotional markers. Additionally, the comparative efficacy of LLM embeddings against traditional feature engineering methods remains underexplored, leaving gaps in understanding their scalability and interpretability for computational personality assessment. Objective: This study evaluates LLM embeddings for personality trait prediction through four key analyses: (1) performance comparison with zero-shot methods on PANDORA Reddit data, (2) psychometric validation and correlation with LIWC (Linguistic Inquiry and Word Count) and emotion features, (3) benchmarking against traditional feature engineering approaches, and (4) assessment of model size effects (OpenAI vs BERT vs RoBERTa). We aim to establish LLM embeddings as a psychometrically valid and efficient alternative for personality assessment. Methods: We conducted a multistage analysis using 1 million Reddit posts from the PANDORA Big Five personality dataset. First, we generated text embeddings using 3 LLM architectures (RoBERTa, BERT, and OpenAI) and trained a custom bidirectional long short-term memory model for personality prediction. We compared this approach against zero-shot inference using prompt-based methods. Second, we extracted psycholinguistic features (LIWC categories and National Research Council emotions) and performed feature engineering to evaluate potential performance enhancements. Third, we assessed the psychometric validity of LLM embeddings: reliability validity using Cronbach alpha and convergent validity analysis by examining correlations between embeddings and established linguistic markers. Finally, we performed traditional feature engineering on static psycholinguistic features to assess performance under different settings. Results: LLM embeddings trained using simple deep learning techniques significantly outperform zero-shot approaches on average by 45% across all personality traits. Although psychometric validation tests indicate moderate reliability, with an average Cronbach alpha of 0.63, correlation analyses spark a strong association with key linguistic or emotional markers; openness correlates highly with social (r=0.53), conscientiousness with linguistic (r=0.46), extraversion with social (r=0.41), agreeableness with pronoun usage (r=0.40), and neuroticism with politics-related text (r=0.63). Despite adding advanced feature engineering on linguistic features, the performance did not improve, suggesting that LLM embeddings inherently capture key linguistic features. Furthermore, our analyses demonstrated efficacy on larger model size with a computational cost trade-off. Conclusions: Our findings demonstrate that LLM embeddings offer a robust alternative to zero-shot methods in personality trait analysis, capturing key linguistic patterns without requiring extensive feature engineering. The correlation between established psycholinguistic markers and the performance trade-off with computational cost provides a hint for future computational linguistic work targeting LLM for personality assessment. Further research should explore fine-tuning strategies to enhance psychometric validity.",psychology; Big Five personality; artificial intelligence; large language models; embeddings; deep learning; social media; Reddit,WoS
10.1038/s41562-025-02273-8,Quantifying large language model usage in scientific papers,10.1038/s41562-025-02273-8,"Scientific publishing is the primary means of disseminating research findings. There has been speculation about how extensively large language models (LLMs) are being used in academic writing. Here we conduct a systematic analysis across 1,121,912 preprints and published papers from January 2020 to September 2024 on arXiv, bioRxiv and Nature portfolio journals, using a population-level framework based on word frequency shifts to estimate the prevalence of LLM-modified content over time. Our findings suggest a steady increase in LLM usage, with the largest and fastest growth estimated for computer science papers (up to 22%). By comparison, mathematics papers and the Nature portfolio showed lower evidence of LLM modification (up to 9%). LLM modification estimates were higher among papers from first authors who post preprints more frequently, papers in more crowded research areas and papers of shorter lengths. Our findings suggest that LLMs are being broadly used in scientific writing. © 2025 Elsevier B.V., All rights reserved.",,Scopus
2-s2.0-85184348532,Quantum Feature Embeddings for Graph Neural Networks,,"Quantum computing offers a promising avenue to reduce growing machine learning model complexity as required in large language models and simulation models for weather forecasts, financial forecasts, or engineering. Graph neural networks are a particular class of machine learning models that have garnered much attention for their ability to deal well with structured data. We investigate how to enhance existing GNNs and find through the inductive bias that quantum circuits are used best to encode node features. The proposed Quantum Feature Embeddings (QFEs) turn raw input features into quantum states, enabling non-linear and entangled representations. In particular, QFEs provide normalized, non-redundant weight matrices in an exponentially larger feature space and require much fewer qubits than fully quantum graph neural networks. On standard graph benchmark datasets, we showcase that for the same parameter count QFEs perform better than their classical counterpart, and are able to match the performance of an exponentially larger model. Finally, we study the potential benefit of using a hybrid quantum graph neural network over a classic alternative on a concrete use case, laser cutting. We find that the proposed model has the performance and thus the near-term potential to uplift these business applications. © 2024 Elsevier B.V., All rights reserved.",Gnns; Quantum Machine Learning; Benchmarking; Computational Linguistics; Embeddings; Graph Neural Networks; Quantum Entanglement; Quantum Optics; Qubits; Weather Forecasting; Feature Embedding; Gnn; Machine Learning Models; Machine-learning; Performance; Quantum Features; Quantum Graph; Quantum Machine Learning; Quantum Machines; Machine Learning,Scopus
2-s2.0-85174411111,R-U-SURE? Uncertainty-Aware Code Suggestions By Maximizing Utility Across Random User Intents,,"Large language models show impressive results at predicting structured text such as code, but also commonly introduce errors and hallucinations in their output. When used to assist software developers, these models may make mistakes that users must go back and fix, or worse, introduce subtle bugs that users may miss entirely. We propose Randomized Utility-driven Synthesis of Uncertain REgions (R-U-SURE), an approach for building uncertainty-aware suggestions based on a decision-theoretic model of goal-conditioned utility, using random samples from a generative model as a proxy for the unobserved possible intents of the end user. Our technique combines minimum-Bayes-risk decoding, dual decomposition, and decision diagrams in order to efficiently produce structured uncertainty summaries, given only sample access to an arbitrary generative model of code and an optional AST parser. We demonstrate R-U-SURE on three developer-assistance tasks, and show that it can be applied different user interaction patterns without retraining the model and leads to more accurate uncertainty estimates than token-probability baselines. We also release our implementation as an open-source library at https://github.com/google-research/r_u_sure. © 2023 Elsevier B.V., All rights reserved.",Machine Learning; Open Source Software; Program Debugging; Decision-theoretic; End-users; Generative Model; Language Model; Random Sample; Random Users; Software Developer; Structured Text; Theoretic Model; Uncertainty; Uncertainty Analysis,Scopus
10.1038/s42003-025-07694-9,RNA-protein interaction prediction using network-guided deep learning,10.1038/s42003-025-07694-9,"Accurate computational determination of RNA-protein interactions remains challenging, particularly when encountering unknown RNAs and proteins. The limited number of RNAs and their flexibility constrained the effectiveness of the deep-learning models for RNA-protein interaction prediction. Here, we introduce ZHMolGraph, which integrates graph neural network and unsupervised large language models to predict RNA-protein interaction. We validate ZHMolGraph predictions on two benchmark datasets and outperform the current best methods. For the dataset of entirely unknown RNAs and proteins, ZHMolGraph shows an improvement in achieving high AUROC of 79.8% and AUPRC of 82.0%. This represents a substantial improvement of 7.1%–28.7% in AUROC and 4.6%–30.0% in AUPRC over other methods. We utilize ZHMolGraph to enhance the challenging SARS-CoV-2 RPI and unbound RNA-protein complex predictions. Such enhancements make ZHMolGraph a reliable option for genome-wide RNA-protein prediction. ZHMolGraph holds broad potential for modeling and designing RNA-protein complexes. © 2025 Elsevier B.V., All rights reserved.","Rna; Rna; Rna, Viral; Rna-binding Proteins; Protein Binding; Rna; Rna Binding Protein; Virus Rna; Artificial Neural Network; Bioinformatics; Chemistry; Coronavirus Disease 2019; Deep Learning; Genetics; Human; Metabolism; Procedures; Severe Acute Respiratory Syndrome Coronavirus 2; Virology; Computational Biology; Covid-19; Deep Learning; Humans; Neural Networks, Computer; Protein Binding; Rna, Viral; Rna-binding Proteins; Sars-cov-2",Scopus
10.1109/ICIRCA65293.2025.11089919,Real-Time Employee Attrition Monitoring and Recommendation System,10.1109/icirca65293.2025.11089919,"Employee attrition is an important challenge that impacts organizational productivity, staff morale, and financial stability. Increased attrition rates result in higher costs of recruitment, workflow disturbances, and reduced efficiency overall. The paper presents a Real-Time Employee Attrition Monitoring and Recommendation System that combines rule-based scoring and AIanalysis through GPT-4 for forecasting and avoiding attrition risks. The system captures employee survey responses, assigns quantifiable scores along predetermined metrics such as job satisfaction, work-life balance, and career development, and processes structured scores through GPT-4 to create tailored recommendations for both employees and HR teams. A Firebase backend provides secure data storage, real-time updating, authentication, and visualization in the form of interactive dashboards, allowing HR teams to study workforce trends and undertake proactive retention measures. The results show that this hybrid model enhances early attrition detection by 30% over conventional approaches, enabling timely HR interventions and enhanced workforce retention. The performance of the system is measured on the basis of attrition prediction accuracy, response time, and intervention success rate. © 2025 Elsevier B.V., All rights reserved.",Authentication; Employee Attrition; Firebase; Gpt-4; Interactive Dashboards; Recommendation System; Rule-based Scoring; Workforce Management; Digital Storage; Employment; Human Resource Management; Information Systems; Information Use; Job Satisfaction; Employee Attrition; Financial Stability; Firebase; Gpt-4; Interactive Dashboard; Organisational; Real- Time; Rule Based; Rule-based Scoring; Workforce Management; Recommender Systems,Scopus
10.1007/978-3-031-98459-4_3,Reasoning and Sampling-Augmented MCQ Difficulty Prediction via LLMs,10.1007/978-3-031-98459-4_3,"The difficulty of multiple-choice questions (MCQs) is a crucial factor for educational assessments. Predicting MCQ difficulty is challenging since it requires understanding both the complexity of reaching the correct option and the plausibility of distractors, i.e., incorrect options. In this paper (This work is partially supported by Renaissance Philanthropy via the learning engineering virtual institute (LEVI) and NSF grants 2118706, 2237676, and 2341948.), we propose a novel, two-stage method to predict the difficulty of MCQs. First, to better estimate the complexity of each MCQ, we use large language models (LLMs) to augment the reasoning steps required to reach each option. We use not just the MCQ itself but also these reasoning steps as input to predict the difficulty. Second, to capture the plausibility of distractors, we sample knowledge levels from a distribution to account for variation among students responding to the MCQ. This setup, inspired by item response theory (IRT), enable us to estimate the likelihood of students selecting each (both correct and incorrect) option. We align these predictions with their ground truth values, using a Kullback-Leibler (KL) divergence-based regularization objective, and use estimated likelihoods to predict MCQ difficulty. We evaluate our method on two real-world math MCQ and response datasets with ground truth difficulty values estimated using IRT. Experimental results show that our method outperforms all baselines, up to a 28.3% reduction in mean squared error and a 34.6% improvement in the coefficient of determination. We also qualitatively discuss how our novel method results in higher accuracy in predicting MCQ difficulty. © 2025 Elsevier B.V., All rights reserved.",Large Language Models; Multiple Choice Questions; Question Difficulty Prediction; Student Selection Likelihood; Engineering Education; Forecasting; Learning Systems; Prediction Models; Students; Educational Assessment; Ground Truth; Item Response Theory; Language Model; Large Language Model; Multiple-choice Questions; Question Difficulty Prediction; Student Selection Likelihood; Student Selections; Virtual Institutes; Mean Square Error,Scopus
10.16920/jeet/2024/v37is2/24102,Retrieval method as a learning intervention for long term retention and creative thinking skills,10.16920/jeet/2024/v37is2/24102,"Few key skills among many that are needed in the 21st century are critical thinking, creativity, collaboration, meta cognitive skills, technology literacy and decision making under uncertainty. With the emergence of ChatGPT and other Generative AI tools, there is a dire need for a distinctive way of learning and application of learning to ensure learning effectiveness. Mere content is no longer the most important factor to learn these skills but. what is needed is the ability to retrieve material learnt, in the context of application. For the learner to be able to apply the critical thinking levels and higher learning levels of Blooms taxonomy, the basic first level of remembering forms the basic foundation, particularly when the students are from first generation educated families or with disadvantaged socioeconomic backgrounds. However, when students cram for the exams, they generally get an ‘illusion of learning’, where they think they know, but in reality often they may not be able to recall what they crammed when they need it later. This is a case of inefficient investment of time and effort in learning for the student. For the teacher, it slows down the teaching process and opportunity to engage in higher order thinking level applications. Addressing the frustrations of both teacher and students, the authors applied the concept of Retrieval method for MBA I year for Marketing Management course during a semester and a survey was conducted at the end of the semester to understand the perception of students on effectiveness of Retrieval techniques. The results supported that retrieval techniques were effective in supporting the student learning. © 2024 Elsevier B.V., All rights reserved.",Blooms Taxonomy; Learning Intervention; Retention Technique; Retrieval Technique,Scopus
2-s2.0-85204303359,RisQNet: Rescuing SMEs from Financial Shocks with a Novel Networked-Loan Risk Assessment,,"In the face of economic downturns, Small and Medium-sized Enterprises (SMEs) within interconnected networked-loans are vulnerable to cascading debt crises, exacerbated by factors like social media-induced financial shocks. Traditional risk assessment models, which mainly rely on financial data, inadequately predict such crises, as evidenced by the collapse of Silicon Valley Bank in 2023. To address this issue, we developed RisQNet, a model that uses temporal graph networks to incorporate diverse risks, including real-time media influences. This approach not only advances risk prediction through news feature extraction and large language models but also enhances risk management strategies with intuitive visualization tools. Validated on a dataset with a total loan volume of USD 3 trillion, RisQNet outperforms the state-of-the-art baseline and achieves 87.1% of AUC. Our collaborative effort with financial regulators and the SME community underpins the model's development, aligning with the UN SDG 8. RisQNet represents a significant step forward in leveraging AI for financial stability, offering a promising approach to combat the propagation of debt crises in financial networks. © 2024 Elsevier B.V., All rights reserved.",Artificial Intelligence; Economic And Social Effects; Risk Assessment; Risk Management; Risk Perception; Debt Crisis; Economic Downturn; Financial Data; Graph Networks; Risk Assessment - Modelling; Risks Assessments; Silicon Valley; Small And Medium-sized Enterprise; Social Media; Temporal Graphs; Decentralized Finance,Scopus
10.1007/s40888-021-00223-x,Robots and employment: evidence from Italy,10.1007/s40888-021-00223-x,"Increased robot diffusion has raised concerns for its possible negative impact on employment. Following an empirical approach in line with those applied to the US and Germany with contrasting results, this paper provides evidence about the effect of robots on employment outcomes in Italy (second European economy for robot stock) from the early 1990s up to 2016, both at the local labour market (LLM) level and at the worker level. In order to purge from demand and other confounding shocks, the identification relies on an instrumental variables strategy based on robots’ sectoral growth in other European countries. No harmful impact on total employment emerges from the LLM analysis; the estimated effect is negative when limited to manufacturing employment, but its statistical significance is weak or absent once concurrent trends relating to trade and ICT are controlled for. Results at the worker level show that incumbent workers in manufacturing were not damaged on average, with an overall positive (though not large) employment effect, driven by longer working relationships with the original firm; conditional on remaining at the original firm, the impact is also positive on wages. On the other hand, robot diffusion turns out to have contributed to reshaping the sectoral distribution of the new labour force inflows towards less robot intensive industries. © 2021 Elsevier B.V., All rights reserved.",Automation; Employment; Local Labour Markets; Matched Employer–employee Data; Robot,Scopus
10.1109/JIOT.2025.3599836,STCA-LLM: Spatial-Temporal Cross-Attention Large Language Model for Wind Speed Forecasting,10.1109/jiot.2025.3599836,"Accurately forecasting wind speed is crucial for efficiently utilizing the renewable energy, stabilizing the energy system and advancing the progress of the decarbonization of our society. However, due to its inherently temporal volatility and intermittency, accurate wind speed forecasting in a wind farm is challenging. Recently, Large Language Models (LLMs) have demonstrated notable performance in abundant natural language processing and computer vision tasks. However, the conventional LLMs fail to learn the complex spatial and temporal correlations of the wind speed data at multiple turbines in a wind farm, which makes wind speed forecasting cant fully benefit from the significant breakthroughs of LLM. To fill in this gap, we propose a novel spatial-temporal cross-attention LLM framework for wind speed forecasting, namely STCA-LLM, composed of alignment phase, and fine-tuning phase. In detail, our contributions are given as follows. First, the alignment phase aligns the general-purpose LLM with task-specific data, i.e., training the LLM with wind speed data. Second, in the fine-tuning phase, two representation learning modules, i.e., convolution network and graph neural network (GNN) are respectively used to extract temporal features of intra time series in each turbine, and the correlation of inter time-series at multiple turbines in a wind farm. Moreover, the cross-attention module is innovatively proposed to establish the connections between spatial and temporal embeddings. Then, the spatial-temporal representation modules and the aligned LLM are fine-tuned in two-stage way. Finally, thorough experiments on real wind speed dataset demonstrate that our proposed STCA-LLM outperforms state-of-the-art time series forecasting models including Transformer-based models, spatial-temporal GNN-based models, and pertained LLM-based models. © 2025 Elsevier B.V., All rights reserved.",Graph Attention Network; Large Language Model (llm); Spatial-temporal Cross-attention; Wind Speed Forecasting; Forecasting; Natural Language Processing Systems; Neural Networks; Spatio-temporal Data; Time Series; Tuning; Wind Effects; Wind Forecasting; Wind Power; Wind Turbines; Fine Tuning; Graph Attention Network; Language Model; Large Language Model; Spatial Temporals; Spatial-temporal Cross-attention; Wind Farm; Wind Speed; Wind Speed Data; Wind Speed Forecasting; Electric Utilities,Scopus
10.1609/aaai.v39i21.34447,STEM-LTS: Integrating Semantic-Temporal Dynamics in LLM-driven Time Series Analysis,10.1609/aaai.v39i21.34447,"Time series forecasting plays a crucial role in domains such as finance, healthcare, and climate science. However, as modern time series data become increasingly complex, featuring high dimensionality, intricate spatiotemporal dependencies, and multi-scale evolutionary patterns, traditional analytical methods and existing predictive models face significant challenges. Although Large Language Models (LLMs) excel in capturing long-range dependencies, they still struggle with multi-scale dynamics and seasonal patterns. Moreover, while LLMs’ semantic representation capabilities are rich, they often lack explicit alignment with the numerical patterns and temporal structures of time series data, leading to limitations in predictive accuracy and interpretability. To address these challenges, this paper proposes a novel framework, STEM-LTS (Semantic-TEmporal Modeling for Large-scale Time Series). STEM-LTS enhances the ability to capture complex spatiotemporal dependencies by integrating time series decomposition techniques with LLM-based modeling. The semantic-temporal alignment mechanism within the framework significantly improves LLMs’ ability to interpret and forecast time series data. Additionally, we develop an adaptive multi-task learning strategy to optimize the model’s performance across multiple dimensions. Through extensive experiments on various real-world datasets, we demonstrate that STEM-LTS achieves significant improvements in prediction accuracy, robustness to noise, and interpretability. Our work not only advances LLM-based time series analysis but also offers new perspectives on handling complex temporal data. © 2025 Elsevier B.V., All rights reserved.",Time Series Analysis; Interpretability; Language Model; Large-scale Time Series; Model-based Opc; Model-driven; Spatio-temporal Dependencies; Temporal Dynamics; Temporal Models; Time-series Analysis; Time-series Data; Multi-task Learning,Scopus
2-s2.0-85196140051,Self-Evaluation Improves Selective Generation in Large Language Models,,"Safe deployment of large language models (LLMs) may benefit from a reliable method for assessing their generated content to determine when to abstain or to selectively generate. While likelihood-based metrics such as perplexity are widely employed, recent research has demonstrated the limitations of using sequence-level probability estimates given by LLMs as reliable indicators of generation quality. Conversely, LLMs have demonstrated strong calibration at the token level, particularly when it comes to choosing correct answers in multiple-choice questions or evaluating true/false statements. In this work, we reformulate open-ended generation tasks into token-level prediction tasks, and leverage LLMs’ superior calibration at the token level. We instruct an LLM to self-evaluate its answers, employing either a multi-way comparison or a point-wise evaluation approach, with the option to include a “None of the above” option to express the model’s uncertainty explicitly. We benchmark a range of scoring methods based on self-evaluation and evaluate their performance in selective generation using TRUTHFULQA and TL;DR. Through experiments with PALM-2 and GPT-3, we demonstrate that self-evaluation based scores not only improve accuracy, but also correlate better with the overall quality of generated content. © 2024 Elsevier B.V., All rights reserved.",Benchmarking; Computational Linguistics; Quality Control; Evaluation Approach; Language Model; Multiple-choice Questions; Point Wise; Prediction Tasks; Probability Estimate; Recent Researches; Reliable Methods; Selective Generation; Self Evaluation; Calibration,Scopus
10.1109/ICCIMS61672.2024.10690672,Self-Evolving Programs: A Novel Approach Leveraging LLMs and Quine Programs,10.1109/iccims61672.2024.10690672,"In an era where software systems undergo rapid evolution, the demand for self-evolving programs has received much attention. This paper introduces a groundbreaking methodology that amalgamates the predictive power of Large Language Model-Based Methods with the self-replicating nature of Quine Programs to organize self-evolving software. This innovative integration not only facilitates dynamic code adaptation to fluctuating runtime conditions but also pioneers a shift from traditional static coding paradigms. Through this approach, we aim to revolutionize software development, particularly enhancing security and performance with a focus on countering selfish mining attacks in Bitcoin. It should be noted that the suggested approach has not yet been considered in the field of software engineering, and its applications are not limited to software security. It has high potential to be used in every domain that requires adaptive programs. Since the suggested approach is novel, the proposed solution in Bitcoin is also novel. The results of simulation show the efficiency of the proposed solution. © 2024 Elsevier B.V., All rights reserved.",Dynamic Code Optimization; Language Model-based Methods; Quine Programs; Self-evolving Programs; Selfish Mining Defense; Application Programs; Dynamic Programming; Software Design; Code Adaptation; Dynamic Code Optimizations; Language Model; Language Model-based Method; Model-based Method; Predictive Power; Quine Program; Self-evolving Program; Selfish Mining Defense; Software-systems; Bitcoin,Scopus
10.1109/CIFER62890.2024.10772910,Semantic Graph Learning for Trend Prediction from Long Financial Documents,10.1109/cifer62890.2024.10772910,"The advent of large language models (LLMs) has initiated much research into their various financial applications. However, in applying LLMs on long documents, semantic relations are not explicitly incorporated, and a full or arbitrarily sparse attention operation is employed. In recent years, progress has been made in Abstract Meaning Representation (AMR), which is a graph-based representation of text to preserve its semantic relations. Since AMR can represent semantic relationships at a deeper level, it can be beneficially utilized by graph neural networks (GNNs) for constructing effective document-level graph representations built upon LLM embeddings to predict target metrics in the financial domain. We propose FLAG: Financial Long document classification via AMR-based GNN, an AMR graph based framework to generate document-level embeddings for long financial document classification. We construct document-level graphs from sentence-level AMR graphs, endow them with specialized LLM word embeddings in the financial domain, apply a deep learning mechanism that utilizes a GNN, and examine the efficacy of our AMR-based approach in predicting labeled target data from long financial documents. Extensive experiments are conducted on a dataset of quarterly earnings calls transcripts of companies in various sectors of the economy, as well as on a corpus of more recent earnings calls of companies in the S&P 1500 Composite Index. We find that our AMR-based approach outperforms fine-tuning LLMs directly on text in predicting stock price movement trends at different time horizons in both datasets. Our work also outperforms previous work utilizing document graphs and GNNs for text classification. © 2025 Elsevier B.V., All rights reserved.",Deep Learning; Earnings; Graph Neural Networks; Labeled Data; Network Embeddings; Semantics; Document Classification; Embeddings; Financial Applications; Financial Domains; Language Model; Level Graphs; Semantic Graphs; Semantic Relations; Trend Prediction; Graph Embeddings,Scopus
10.1109/ICDMW65004.2024.00021,Sentiment Score of Bloomberg Market Wraps with ChatGPT,10.1109/icdmw65004.2024.00021,"In this paper, we used a large dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines affect stock market movements. To make this analysis more effective, we employed ChatGPT. First, from the summary of daily financial updates, we identify top global news headlines that could have a significant influence on stock markets. Second, for each headline, we questioned ChatGPT to answer whether the news might lead to a rise, a fall in stock prices or an indecisive future. This two-stage method proves more effective than a direct question on the entire text. By gathering ChatGPT's predictions day by day, we formed an overall market sentiment score and transform this score into a practical investment strategy in the NASDAQ index, demonstrating the significance of minimizing noise in sentiment scores by initially accumulating and then detrending them. This approach showcases that ChatGPT's analysis of news headlines can provide valuable insights into future stock market behaviours and be a valuable tool to develop intuitive NLP-driven investment strategies leveraging news predictive power. © 2025 Elsevier B.V., All rights reserved.",Bloomberg News; Chatgpt; Nlp; Sentimentscore; Commerce; Financial Markets; Bloomberg; Bloomberg News; Chatgpt; Detrending; Investment Strategy; Large Datasets; Sentiment Scores; Sentimentscore; Stock Price; Two-stage Methods; Marketplaces,Scopus
10.1016/j.frl.2024.105227,Sentiment trading with large language models,10.1016/j.frl.2024.105227,"We analyse the performance of the large language models (LLMs) OPT, BERT, and FinBERT, alongside the traditional Loughran-McDonald dictionary, in the sentiment analysis of 965,375 U.S. financial news articles from 2010 to 2023. Our findings reveal that the GPT-3-based OPT model significantly outperforms the others, predicting stock market returns with an accuracy of 74.4%. A long-short strategy based on OPT, accounting for 10 basis points (bps) in transaction costs, yields an exceptional Sharpe ratio of 3.05. From August 2021 to July 2023, this strategy produces an impressive 355% gain, outperforming other strategies and traditional market portfolios. This underscores the transformative potential of LLMs in financial market prediction and portfolio management and the necessity of employing sophisticated language models to develop effective investment strategies based on news sentiment. © 2024 Elsevier B.V., All rights reserved.",Artificial Intelligence Investment Strategies; Generative Pre-trained Transformer (gpt); Large Language Models; Machine Learning In Stock Return Prediction; Natural Language Processing (nlp),Scopus
WOS:001503479400001,Small but mighty: enhancing time series forecasting with lightweight LLMs,10.1007/s11227-025-07491-5,"While large language models (LLMs) have demonstrated remarkable potential in time series forecasting, their practical deployment remains constrained by excessive computational demands and memory footprints. Existing LLM-based methods typically suffer from three critical limitations: (1) inefficient parameter utilization in handling numerical time series patterns; (2) modality misalignment between continuous temporal signals and discrete text embeddings; and (3) inflexibility for real-time expert knowledge integration. We present small but mighty enhancing time series (SMETimes), the first systematic investigation of small language models with sub-3B parameters (SLM) for efficient and accurate time series forecasting. Our method centers on three key innovations: (1) a statistically enhanced prompt structure that bridges numerical time series with textual semantics through descriptive statistical features; (2) an adaptive fusion embedding structure that aligns temporal patterns with language model token spaces through learnable parameters; and (3) a dynamic mixture-of-experts structure enabled by SLMs' computational efficiency, adaptively combining base predictions with domain-specific models. Extensive evaluations across seven benchmark datasets (ETTh1/2, ETTm1/2, Weather, Solar, ECL) demonstrate that our 3B-parameter SLM achieves state-of-the-art performance on five primary datasets while maintaining 3.8x\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\times$$\end{document} faster training and 5.2x\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$\times$$\end{document} lower memory consumption compared to 7B-parameter LLM baselines. In particular, the proposed model exhibits better learning capabilities, achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that our statistically enhanced prompt structure and adaptive fusion embedding structure contribute, respectively, to the reduction of 15.7% and 18.2% errors in long-horizon forecasting tasks. By redefining the efficiency-accuracy trade-off landscape, this work establishes SLMs as viable alternatives to resource-intensive LLMs for practical time series forecasting. The code and models are available at https://github.com/xiyan1234567/SMETimes.",Small language models; Statistically enhanced prompt; Adaptive fusion embedding; Dynamic mixture-of-experts,WoS
10.1007/s10614-025-11024-w,Stock Market Forecasting: From Traditional Predictive Models to Large Language Models,10.1007/s10614-025-11024-w,"Stock market forecasting is a complex research problem due to the complexity of the factors influencing stock market trends. This survey provides a comprehensive overview of recent advancements in stock market forecasting, focusing on the impact of large language models (LLMs) in financial analytics. The survey explores the strengths and challenges of feature engineering, ensemble methods, hybrid models, text-based prediction and reinforcement learning. It then presents the transformative impact of LLMs, highlighting their capabilities in utilizing transfer learning and few-shot learning to understand complex financial information, enhancing sentiment analysis, improving portfolio management, and stock forecasting accuracy. A key novelty of this survey lies in presenting comprehensive analysis of the strengths and weaknesses of LLMs for different financial tasks in addition to exploring how LLMs can be combined with machine learning and reinforcement learning approaches to overcome their limitations in handling unstructured data, improving model explainability, and enhancing generalizability. Finally, this survey identifies existing research gaps and limitations, proposing future research directions aimed at improving prediction accuracy and utilizing both LLMs and predictive models’ capabilities in stock market forecasting. © 2025 Elsevier B.V., All rights reserved.",Feature Engineering; Large Language Models; Llm-based Financial Agents; Machine Learning; Reinforcement Learning; Stock Market Forecasting,Scopus
10.1109/BigData62323.2024.10825946,Stock Price Prediction Using LLM-Based Sentiment Analysis,10.1109/bigdata62323.2024.10825946,"This paper examines the effectiveness of recent large language model-based news sentiment estimation for stock price forecasting with the combination of latest transformer-based prediction models. To achieve a better accuracy in sentiment classification, experiments are designed to compare six different models (GPT 4, Llama 3, Gemma 2, Mistral 7b, FinBERT, VADER) in financial news sentiment classification, and it was found that recent large language models can outperform FinBERT and VADER, which are the most commonly used models in financial sentiment analysis. Based on the experiment results, Llama 3, with relatively stable performance, is chosen to classify the news sentiments of the selected companies. Informer, Transformer, TCN, LSTM, SVR, Random Forest and Naive Forecast are used to predict the stock prices with different sliding window sizes. Experiments with different scenarios are designed to evaluate the prediction ability of news sentiment. Results show that adding news sentiment data can indeed improve the stock price prediction. Informer, one of the state-of-the-art transformer models for long-term prediction tasks, yields the best performances in most cases. Ablation study of Informer suggests that the generative style decoder plays an important role in performance improvement. © 2025 Elsevier B.V., All rights reserved.",Informer; Llm; Sentiment Analysis; Time Series Forecasting; Transformer; Costs; Informer; Language Model; Llm; Model-based Opc; Performance; Sentiment Analysis; Sentiment Classification; Stock Price Prediction; Time Series Forecasting; Transformer; Prediction Models,Scopus
10.1007/978-981-96-6291-3_3,Stock Price Prediction Using Univariate and Multivariate Historical Data with Post-Interpretation via Large Language Models,10.1007/978-981-96-6291-3_3,"In this study, we propose a hybrid approach that utilizes both univariate and multivariate historical data from key variables and related factors across four distinct groups. We developed a hybrid approach combining Volume Features, Valuation Metrics, Technical Indicators, and Market Sentiment for stock price prediction and investigate several state-of-the-art models, including Artificial Neural Networks (ANN), Gated Recurrent Units (GRU), Bidirectional GRU (BI-GRU), and Transformer-based Time Series (TST) models, while experimenting with different lags of inputs to capture intricate temporal patterns in stock price movements. Our experiments, conducted on seven stocks from various sectors, allow us to evaluate the robustness and generalizability of the models across different industries. To enhance interpretability, we employ large language models (LLMs) in the post-prediction phase, which transform the predictive outputs into human-readable narratives explaining the factors driving stock price predictions. Empirical results demonstrate that our approach, incorporating advanced deep learning models like ANN, GRU, BI-GRU, and TST with varying input lags, significantly improves prediction accuracy over traditional methods while providing actionable insights for financial decision-making. © 2025 Elsevier B.V., All rights reserved.",Large Language Models (llm); Multivariate Analysis; Stock Price Prediction; Time Series Forecasting; Univariate Data; Decision Making; Deep Learning; Electronic Trading; Financial Markets; Forecasting; Learning Systems; Neural Networks; Prediction Models; Time Series; Time Series Analysis; Historical Data; Hybrid Approach; Language Model; Large Language Model; Multi Variate Analysis; Neural-networks; Stock Price Prediction; Time Series Forecasting; Univariate; Univariate Data; Multivariant Analysis,Scopus
10.1007/s10489-025-06823-5,Survey on Tabular Data Privacy and Synthetic Data Generation in Industry 4.0,10.1007/s10489-025-06823-5,"Synthetic data is an emerging field that solves the raised need for privacy-preserving data sharing and the lack of real data. One of the most common data types used is tabular data, which is widely used to train machine learning models, especially in the industrial domain for better decision-making and edge case handling, two key points in Industry 4.0. In this paper, we present and evaluate state-of-the-art models for tabular data generation under a proposed taxonomy consisting of statistical models, generative adversarial networks (GANs)-based models, denoising diffusion probabilistic models (DDPMs), and large language models (LLMs). Additionally, we propose a revised evaluation taxonomy consisting of three dimensions, including realism, representativeness, and privacy. The results proved that analyzing models based on multiple metrics from each category could ensure a better understanding of the dataset when used for downstream tasks. Finally, we found that models based on GANs are still a solid option in multiple cases, such as a constrained computational environment. In contrast, models based on LLMs and DDPMs are more promising in terms of realism and representativeness. More research should be invested in overcoming limitations such as numerical data representation and long training times for LLMs. Our survey serves as a study for existing models and newer directions in the field, with guidelines for evaluation that can be applied to industrial and other domains. © 2025 Elsevier B.V., All rights reserved.",Data Privacy; Evaluation Of Synthetic Tabular Data; Industry 4.0; Synthetic Data; Tabular Data Generation; Data Sharing; Decision Making; Learning Systems; Machine Learning; Privacy-preserving Techniques; Adversarial Networks; Data Generation; De-noising; Evaluation Of Synthetic Tabular Data; Language Model; Model-based Opc; Synthetic Data; Tabular Data; Tabular Data Generation; Industry 4.0,Scopus
10.1145/3552326.3587438,Tabi: An Efficient Multi-Level Inference System for Large Language Models,10.1145/3552326.3587438,"Today’s trend of building ever larger language models (LLMs), while pushing the performance of natural language processing, adds significant latency to the inference stage. We observe that due to the diminishing returns of adding parameters to LLMs, a smaller model could make the same prediction as a costly LLM for a majority of queries. Based on this observation, we design Tabi, an inference system with a multi-level inference engine that serves queries using small models and optional LLMs for demanding applications. Tabi is optimized for discriminative models (i.e., not generative LLMs) in a serving framework. Tabi uses the calibrated confidence score to decide whether to return the accurate results of small models extremely fast or re-route them to LLMs. For re-routed queries, it uses attention-based word pruning and weighted ensemble techniques to offset the system overhead and accuracy loss. We implement and evaluate Tabi with multiple tasks and models. Our result shows that Tabi achieves 21%-40% average latency reduction (with comparable tail latency) over the state-of-the-art while meeting LLM-grade high accuracy targets. © 2023 Elsevier B.V., All rights reserved.",Attention-based Transformer; Machine Learning Inference; Computational Linguistics; Learning Algorithms; Natural Language Processing Systems; Attention-based Transformer; Inference Stages; Inference Systems; Language Model; Language Processing; Machine Learning Inference; Machine-learning; Multilevels; Natural Languages; Performance; Machine Learning,Scopus
2-s2.0-85137364174,TeamMX at PoliticEs 2022: Analysis of Feature Sets in Spanish Author Profiling for Political Ideology,,"Natural Language Processing (NLP) is evolving more and more every day and it is becoming a very powerful tool, especially when it works in combination with Machine Learning algorithms, as it is making ventures into areas in which it was not well known, such as automatic programming systems based on the GPT-3 model, the market or sales prediction, even, the risk detection in banking systems on the basis of written exchanges between branch managers or directors of the same bank. The so-called short texts, comments/reviews made on social networks like Twitter, Facebook or Youtube, are becoming relevant in several domains. The corpus provided by the IberLEF 2022 Task - PoliticEs was used for extract political ideology information, it was focused on the identification of the gender, the profession, and the political spectrum from a binary (Left, Right) and multi-class perspective (Left, Right, Moderate-Left and Moderate-Right). Eight methods are proposed, six of them didn't have the expected results, but contributed to the two best ones. We implemented a customized stopwords study for our research in collaboration with experiments such as Best unique words per category, Set-based study, Transition point and others to extract the features, then Random Forest, SVM and Neural Network algorithms with default parameters and the Scikit learn tool were used to identify the categories. Obtaining a Macro F1 value of 0.7984 and the highest value achieved was 0.8270 in the category of Profession. © 2022 Elsevier B.V., All rights reserved.",Author Profiling; Authorship Analysis; Authorship Attribution; Linguistic Features; Natural Language Processing; Decision Trees; Linguistics; Machine Learning; Natural Language Processing Systems; Sales; Social Networking (online); Author Profiling; Authorship Analysis; Authorship Attribution; Features Sets; Language Processing; Linguistic Features; Machine Learning Algorithms; Natural Language Processing; Natural Languages; Political Ideologies; Learning Algorithms,Scopus
10.1016/j.arthro.2024.06.021,The Large Language Model ChatGPT-4 Exhibits Excellent Triage Capabilities and Diagnostic Performance for Patients Presenting With Various Causes of Knee Pain,10.1016/j.arthro.2024.06.021,"Purpose: To provide a proof-of-concept analysis of the appropriateness and performance of ChatGPT-4 to triage, synthesize differential diagnoses, and generate treatment plans concerning common presentations of knee pain. Methods: Twenty knee complaints warranting triage and expanded scenarios were input into ChatGPT-4, with memory cleared prior to each new input to mitigate bias. For the 10 triage complaints, ChatGPT-4 was asked to generate a differential diagnosis that was graded for accuracy and suitability in comparison to a differential created by 2 orthopaedic sports medicine physicians. For the 10 clinical scenarios, ChatGPT-4 was prompted to provide treatment guidance for the patient, which was again graded. To test the higher-order capabilities of ChatGPT-4, further inquiry into these specific management recommendations was performed and graded. Results: All ChatGPT-4 diagnoses were deemed appropriate within the spectrum of potential pathologies on a differential. The top diagnosis on the differential was identical between surgeons and ChatGPT-4 for 70% of scenarios, and the top diagnosis provided by the surgeon appeared as either the first or second diagnosis in 90% of scenarios. Overall, 16 of 30 diagnoses (53.3%) in the differential were identical. When provided with 10 expanded vignettes with a single diagnosis, the accuracy of ChatGPT-4 increased to 100%, with the suitability of management graded as appropriate in 90% of cases. Specific information pertaining to conservative management, surgical approaches, and related treatments was appropriate and accurate in 100% of cases. Conclusions: ChatGPT-4 provided clinically reasonable diagnoses to triage patient complaints of knee pain due to various underlying conditions that were generally consistent with differentials provided by sports medicine physicians. Diagnostic performance was enhanced when providing additional information, allowing ChatGPT-4 to reach high predictive accuracy for recommendations concerning management and treatment options. However, ChatGPT-4 may show clinically important error rates for diagnosis depending on prompting strategy and information provided; therefore, further refinements are necessary prior to implementation into clinical workflows. Clinical Relevance: Although ChatGPT-4 is increasingly being used by patients for health information, the potential for ChatGPT-4 to serve as a clinical support tool is unclear. In this study, we found that ChatGPT-4 was frequently able to diagnose and triage knee complaints appropriately as rated by sports medicine surgeons, suggesting that it may eventually be a useful clinical support tool. © 2025 Elsevier B.V., All rights reserved.","Article; Chatgpt; Clinical Significance; Concept Analysis; Conservative Treatment; Differential Diagnosis; Human; Knee Pain; Large Language Model; Patient Triage; Proof Of Concept; Surgeon; Vignette; Workflow; Arthralgia; Diagnosis; Etiology; Female; Generative Artificial Intelligence; Knee Joint; Male; Procedures; Arthralgia; Diagnosis, Differential; Female; Generative Artificial Intelligence; Humans; Knee Joint; Large Language Models; Male; Proof Of Concept Study; Triage",Scopus
10.1002/9781394308286,The Predictive Edge: Outsmart the Market using Generative AI and ChatGPT in Financial Forecasting,10.1002/9781394308286,"Use ChatGPT to improve your analysis of stock markets and securities In The Predictive Edge: Outsmart the Market Using Generative AI and ChatGPT in Financial Forecasting, renowned AI and finance researcher Dr. Alejandro Lopez-Lira delivers an engaging and insightful new take on how to use large language models (LLMs) like ChatGPT to find new investment opportunities and make better trading decisions. In the book, you’ll learn how to interpret the outputs of LLMs to craft sounder trading strategies and incorporate market sentiment into your analyses of individual securities. In addition to a complete and accessible explanation of how ChatGPT and other LLMs work, you’ll find: Discussions of future trends in artificial intelligence and finance Strategies for implementing new and soon-to-come AI tools into your investing strategies and processes Techniques for analyzing market sentiment using ChatGPT and other AI tools A can’t-miss playbook for taking advantage of the full potential of the latest AI advancements, The Predictive Edge is a fully to-date and exciting exploration of the intersection of tech and finance. It will earn a place on the bookshelves of individual and professional investors everywhere. © 2024 Elsevier B.V., All rights reserved.",,Scopus
10.1109/ARSO60199.2024.10557816,The Robot's Understanding of Classification Concepts Based on Large Language Model,10.1109/arso60199.2024.10557816,"Cognition of classification concepts empowers robots to enhance their planning and task execution capabilities. By incorporating classification cognition of the environment and tasks, robots can make more informed decisions and improve their autonomy. In various domains, specific concept classification is crucial, such as disease categorization in medical diagnosis, transaction classification in the financial field, etc. Therefore, this article conducts an in-depth exploration of the cognition and understanding of abstract classification concepts (object color, object type, object size, etc.), and builds a programmed classification structure based on a large language model (PCS-LLM) to improve the robot's ability to classify the different categories. First, through the programmed definition of interactive objects and executable action libraries in the scene, constraints are added to the prediction of general LLM, thereby improving the executability of predicted actions. Secondly, for abstract classification concepts, typical tasks and decomposed actions performed by robots are programmatically defined as an example of LLM generating new task decomposed actions. Finally, in mixed scenarios, the robot's understanding and decomposition of any concept combination task is verified based on the UR5 robotic arm. Experimental results show that the robot can achieve corresponding classification operations when faced with tasks involving any combination of concepts. © 2024 Elsevier B.V., All rights reserved.",Classification Concept Cognition; Large Language Model (llm); Mixed Scene; Programmatic Defined Structure; Robot Skill Generalization; Abstracting; Computational Linguistics; Computer Aided Diagnosis; Classification Concept Cognition; Concept-based; Generalisation; Language Model; Large Language Model; Mixed Scene; Programmatic Defined Structure; Programmatics; Robot Skill Generalization; Robot Skills; Robot Programming,Scopus
10.1145/3718491.3718663,The Role of AI in Financial Forecasting: ChatGPT’s Potential and Challenges,10.1145/3718491.3718663,"The outlook for the future of artificial intelligence (AI) in the financial sector, especially in financial forecasting, the challenges and implications. The dynamics of AI technology, including deep learning, reinforcement learning, and integration with blockchAIn and the Internet of Things, also highlight the continued improvement in data processing capabilities. Explore how AI is reshaping financial services with precisely tAIlored services that can more precisely meet the diverse needs of individual investors. The integration of AI challenges regulatory and ethical issues in the financial sector, as well as the implications for data privacy protection. Analyze the limitations of current AI technology in financial forecasting and its potential impact on the future financial industry landscape, including changes in the job market, the emergence of new financial institutions, and user interface innovations. Emphasizing the importance of increasing investor understanding and awareness of AI and looking ahead to future trends in AI tools for user experience to drive wider adoption of AI in financial decision making. The huge potential, challenges, and future directions of AI in the financial sector highlight the critical role of AI technology in driving transformation and innovation in the financial sector. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence; Chatgpt; Financial Forecasting; Multimodal Language Model; Risklabs; Differential Privacy; Artificial Intelligence Technologies; Chatgpt; Financial Forecasting; Financial Sectors; Language Model; Learning Reinforcements; Multi-modal; Multimodal Language Model; Reinforcement Learnings; Risklab; Decentralized Finance,Scopus
10.1016/S2589-7500(24)00097-9,The diagnostic and triage accuracy of the GPT-3 artificial intelligence model: an observational study,10.1016/s2589-7500(24)00097-9,"Background: Artificial intelligence (AI) applications in health care have been effective in many areas of medicine, but they are often trained for a single task using labelled data, making deployment and generalisability challenging. How well a general-purpose AI language model performs diagnosis and triage relative to physicians and laypeople is not well understood. Methods: We compared the predictive accuracy of Generative Pre-trained Transformer 3 (GPT-3)'s diagnostic and triage ability for 48 validated synthetic case vignettes (<50 words; sixth-grade reading level or below) of both common (eg, viral illness) and severe (eg, heart attack) conditions to a nationally representative sample of 5000 lay people from the USA who could use the internet to find the correct options and 21 practising physicians at Harvard Medical School. There were 12 vignettes for each of four triage categories: emergent, within one day, within 1 week, and self-care. The correct diagnosis and triage category (ie, ground truth) for each vignette was determined by two general internists at Harvard Medical School. For each vignette, human respondents and GPT-3 were prompted to list diagnoses in order of likelihood, and the vignette was marked as correct if the ground-truth diagnosis was in the top three of the listed diagnoses. For triage accuracy, we examined whether the human respondents’ and GPT-3's selected triage was exactly correct according to the four triage categories, or matched a dichotomised triage variable (emergent or within 1 day vs within 1 week or self-care). We estimated GPT-3's diagnostic and triage confidence on a given vignette using a modified bootstrap resampling procedure, and examined how well calibrated GPT-3's confidence was by computing calibration curves and Brier scores. We also performed subgroup analysis by case acuity, and an error analysis for triage advice to characterise how its advice might affect patients using this tool to decide if they should seek medical care immediately. Findings: Among all cases, GPT-3 replied with the correct diagnosis in its top three for 88% (42/48, 95% CI 75–94) of cases, compared with 54% (2700/5000, 53–55) for lay individuals (p<0.0001) and 96% (637/666, 94–97) for physicians (p=0·012). GPT-3 triaged 70% correct (34/48, 57–82) versus 74% (3706/5000, 73–75; p=0.60) for lay individuals and 91% (608/666, 89–93%; p<0.0001) for physicians. As measured by the Brier score, GPT-3 confidence in its top prediction was reasonably well calibrated for diagnosis (Brier score=0·18) and triage (Brier score=0·22). We observed an inverse relationship between case acuity and GPT-3 accuracy (p<0·0001) with a fitted trend line of –8·33% decrease in accuracy for every level of increase in case acuity. For triage error analysis, GPT-3 deprioritised truly emergent cases in seven instances. Interpretation: A general-purpose AI language model without any content-specific training could perform diagnosis at levels close to, but below, physicians and better than lay individuals. We found that GPT-3's performance was inferior to physicians for triage, sometimes by a large margin, and its performance was closer to that of lay individuals. Although the diagnostic performance of GPT-3 was comparable to physicians, it was significantly better than a typical person using a search engine. Funding: The National Heart, Lung, and Blood Institute. © 2024 Elsevier B.V., All rights reserved.",Artificial Intelligence; Diseases; Ground Truth; Harvard Medical School; Intelligence Models; Labeled Data; Language Model; Observational Study; Performance; Predictive Accuracy; Reading Level; Self-care; Diagnosis; Analytical Error; Article; Bootstrapping; Calibration; Confidence Interval; Controlled Study; Diagnostic Accuracy; Diagnostic Test Accuracy Study; Diagnostic Value; General Practitioner; Generative Pretrained Transformer; Heart Infarction; Human; Internet; Internist; Layperson; Major Clinical Study; Medical School; Observational Study; Patient Triage; Prediction; Reading; Search Engine; Self Care; Trend Study; United States; Validation Process; Vignette; Virus Infection; Adult; Artificial Intelligence; Female; Male; Middle Aged; Procedures; Adult; Artificial Intelligence; Female; Humans; Male; Middle Aged; Triage,Scopus
10.12816/0051162,The estimation and the fulfi llment scenarios of human resources of sharia banking in Indonesia,10.12816/0051162,"This study aims to test the role of Sharia in improving employees' performance. There are some variables of Sharia such as banking estimation and followership based on talent management. Besides, the consequence variables are knowledge sharing and employees' performance. The rapid growth of Sharia Banking faces problems of minimum human resources in quantity and quality. In 2020, the need human resources of Sharia banking will at least reach 179,646 consisting of 165,274 employees in low Sharia quality as executors, and 14,374 employees in middle to high Sharia quality for the banking managerial and leader positions. In three short term scenarios, Sharia banking absorbs 50% of Sharia human resource supply which is considered as a realistic scenario until 2018. In long-term period, ideal condition can be reached if the first scenario can be executed so that the balance of supply and demand will happen. This means Sharia banking industry will be more competitive and growing fast. © 2020 Elsevier B.V., All rights reserved.",Employee Engagement; Human Capital; Human Resource Management; Management Development; Performance Management,Scopus
10.1016/j.techsoc.2025.103035,The impact of financial technology on corporate total factor productivity: from the perspectives of competitive strategy and corporate innovation capability,10.1016/j.techsoc.2025.103035,"This study examines the impact of FinTech development on Chinese corporate total factor productivity (TFP) using a logical framework of "" FinTech development-competitive strategy and corporate innovation-corporate TFP"". Analyzing data from Chinese A-share listed companies in Shanghai and Shenzhen (2011–2021) through web crawler text analysis and employing fixed effect and mediating effect models, we find that FinTech effectively enhances TFP by promoting differentiation strategy. Despite inhibiting cost leadership due to an adaptation dilemma, FinTech still contributes positively to overall TFP. FinTech development also correlates with increased innovation, further elevating corporate TFP. Extending the analysis to consider corporate nature, geographical location, and policy implementation reveals nuanced impacts, with non-state-owned corporations and central region corporate experiencing more significant TFP benefits from FinTech. Robustness checks using GMM, instrumental variable, LLM and Hausman-Taylor estimation validate these findings. © 2025 Elsevier B.V., All rights reserved.",Competitive Strategy; Corporate Innovation; Corporate Tfp; Financial Technology; Competition; Competitive Strategy; Corporate Innovation; Corporate Total Factor Productivity; Corporates; Financial Technology; Fixed Effects; Innovation Capability; Logical Frameworks; Text Analysis; Total Factor Productivity; Fintech; Competition (economics); Competitiveness; Corporate Strategy; Financial System; Industrial Investment; Innovation; Strategic Approach; Total Factor Productivity; China; Guangdong; Shanghai; Shenzhen,Scopus
10.1016/j.chphi.2020.100005,Theoretical prediction of decomposition temperature of typical heat-resistant explosives,10.1016/j.chphi.2020.100005,"With the approaching exhaustion of shallow-ground gas and oil, heat-resistant explosives are in highly demand in oil/gas ultra-deep and ocean well development. However, the prediction method on decomposition temperatures of heat-resistant explosives is still yet to be determined. In this paper, based on the decomposition reaction of “trinitrotoluene mechanism”, the influences of the enthalpy changes, entropies, Gibbs free energies, conversion temperatures and resonance energies on corresponding decomposition temperatures were studied. A theoretical relation was proposed after a genetic-algorithm optimization. Based on this relation, the decomposition temperatures of LLM-105 derivatives and its isomers were discussed. The results showed that the proposed relation coincided well with the decomposition temperatures of typical aromatic explosives. Furthermore, the decomposition temperatures of LLM-105, LLM-105I2 and ANPZ,TANPyO were predicted high and can be taken as new heat-resistant explosive candidates. © 2022 Elsevier B.V., All rights reserved.",Conversion Temperature; Decomposition Temperature; Genetic Functional Approximation; Heat-resistant Explosive; Standard Entropy,Scopus
WOS:000690319500001,Thermal decomposition mechanisms of LLM-105/HTPB plastic-bonded explosive: ReaxFF-lg molecular dynamics simulations,10.1080/07370652.2021.1968071,"Thermal decomposition is not only related to the explosive's ignition and detonation performances but also to their sensitivity to the surrounding environment. Based on the molecular dynamics method, the interaction and the thermal decomposition mechanism of plastic-bonded explosives (PBXs) composed by 2,6-diamino-3,5-dinitropyrazine-1-oxidated (LLM-105) and hydroxyl-terminated polybutadiene (HTPB) were investigated using the reactive force field molecular dynamics (ReaxFF-lg MD) simulations at 2000 K-4000 K. The interaction between the explosive and the binder effectively stabilizes the explosive crystal and reduces its sensitivity. The predicted activation energy of LLM-105/HTPB is 39.86 kcal/mol, more than that of the pure LLM-105 (36.57 kcal/mol), which reduces the initial decomposition reaction rate of LLM-105 after adding HTPB binder. Products distribution in thermal decomposition shows that the main initial decomposition mechanisms of LLM-105 include the C-NO2, C-NH2 bonds breaking, nitroso rearranging, NH2 elimination and H atom transferring. The H ions from the HTPB dehydrogenation delay the chemical bond breaking to inhibit the decomposition of LLM-105, and the reversible H transfer reactions provide an energy buffering area to reduce sensitivity.",plastic-bonded explosives; ReaxFF-lg molecular dynamics; decomposition mechanism; interaction,WoS
10.1609/aaai.v39i17.33989,"TimeCAP: Learning to Contextualize, Augment, and Predict Time Series Events with Large Language Model Agents",10.1609/aaai.v39i17.33989,"Time series data is essential in various applications, including climate modeling, healthcare monitoring, and financial analytics. Understanding the contextual information associated with real-world time series data is often essential for accurate and reliable event predictions. In this paper, we introduce TimeCAP, a time-series processing framework that creatively employs Large Language Models (LLMs) as contextualizers of time series data, extending their typical usage as predictors. TimeCAP incorporates two independent LLM agents: one generates a textual summary capturing the context of the time series, while the other uses this enriched summary to make more informed predictions. In addition, TimeCAP employs a multi-modal encoder that synergizes with the LLM agents, enhancing predictive performance through mutual augmentation of inputs with in-context examples. Experimental results on real-world datasets demonstrate that TimeCAP outperforms state-of-the-art methods for time series event prediction, including those utilizing LLMs as predictors, achieving an average improvement of 28.75% in F1 score. © 2025 Elsevier B.V., All rights reserved.",Contextual Information; Contextualize; Event Prediction; Healthcare Monitoring; Language Model; Model Agents; Real-world Time Series; Time Series Processing; Time-series Data; Time-series Events,Scopus
WOS:001126489800001,Timeshifting strategies for carbon-efficient long-running large language model training,10.1007/s11334-023-00546-x,"Language models play a vital role in various natural language processing tasks, but their training can be computationally intensive and lead to significant carbon emissions. In this study, we explore the effectiveness of timeshifting strategies to mitigate the environmental impact of long-running large language models (LLMs). We develop a simulation tool that estimates carbon emissions for LLMs, enabling developers to make informed decisions prior to running their workloads. By leveraging historical carbon intensity data from WattTime, we investigate the potential benefits and limitations of timeshifting in different locations, considering diverse energy profiles. Our findings demonstrate that timeshifting can substantially reduce emissions, but it is highly dependent on the region's carbon intensity and energy mix. We present insights into the trade-offs between emissions reduction and workload runtime, acknowledging the need for further advancements in carbon-aware computing practices. Our research contributes to the growing field of sustainable computing and encourages developers to adopt environmentally conscious strategies in language model training.",Large language model (LLM); Energy consumption; Carbon-awareness; Timeshifting,WoS
10.1016/j.ab.2025.115882,TransCNN: A novel architecture combining transformer and TextCNN for detecting N4-acetylcytidine sites in human mRNA,10.1016/j.ab.2025.115882,"N4-acetylcytidine (ac4C), a pivotal post-transcriptional RNA modification, is central to understanding transcriptional regulation and diverse biological processes. As a key determinant of RNA structural stability and functional regulation, ac4C has been strongly associated with multiple human diseases. We can obtain a better understanding of regulation mechanism of gene expression by identifying ac4C sites rapidly and precisely. However, existing predictive approaches are constrained by limitations in feature representation and sequence context modeling, necessitating the development of advanced methodologies. In this study, we introduce a novel architecture named TransCNN that integrates transformer and Text convolutional neural network (TextCNN) to predict ac4C sites. TransCNN demonstrates superior performance compared to existing models on both 10-fold cross-validation and independent dataset with the accuracy of 83.27 % and 82.89 %, respectively. The enhanced performance of TransCNN is attributed to the transformer's ability to extract adaptive features and TextCNN's capability to form both narrow and broad connections within the sequence. This study aims to contribute significantly to the field by advancing the understanding and prediction of RNA modifications. The datasets and code used in this study are available at https://github.com/liukai23157/TransCNN. © 2025 Elsevier B.V., All rights reserved.","N4-acetylcytidine; Natural Language Processing; Text Convolutional Neural Network; Transformer; Cytidine; Cytidine; N-acetylcytidine; Rna, Messenger; Convolutional Neural Networks; Convolutional Neural Network; Language Processing; N4-acetylcytidine; Natural Language Processing; Natural Languages; Novel Architecture; Performance; Post-transcriptional; Text Convolutional Neural Network; Transformer; Transcription; Cytidine Derivative; Messenger Rna; N4 Acetylcytidine; Nucleotide; Unclassified Drug; Cytidine; N-acetylcytidine; 10 Fold Cross Validation; Article; Binary Classification; Comparative Study; Computer Model; Controlled Study; Convolutional Neural Network; Cross Validation; Data Accuracy; Deep Learning; Dimensionality Reduction; Embedding; Entropy; False Negative Result; False Positive Result; Feature Extraction; Gene Expression Regulation; Human; Rna Sequence; Sensitivity And Specificity; Text Convolutional Neural Network; Transformer Convolutional Neural Network; Artificial Neural Network; Chemistry; Genetics; Metabolism; Rna Processing; Cytidine; Humans; Neural Networks, Computer; Rna Processing, Post-transcriptional; Rna, Messenger",Scopus
10.1109/ICICI62254.2024.00068,Transforming Healthcare: Unified Medical Identification and AI-Enabled Treatment Advancements,10.1109/icici62254.2024.00068,"In the dynamic realm of healthcare, accurate disease prediction stands as a cornerstone for proactive management and personalized care. Through the introduction of Unified Medical Identification (UMI), users can easily connect to their medical records anywhere at any time. With advanced features like rural language support via video-based disease prediction integrated with the Large Language Model, the system generates a broader bracket of users. An ensembled machine learning model is used for general disease prediction. Integrating the CatBoost Classifier with the LangChain framework, the model accurately detects chronic diseases keeping UMI as the central source of the user's medical data. Furthermore, an AI chatbot aids in the enhancement of user engagement offering targeted treatment options and suitable lifestyle recommendations resulting in better health outcomes. With a remarkable 92.8% accuracy in general disease prediction and a whopping 95.8% accuracy in chronic disease prediction, this revolutionary tool not only reforms the healthcare delivery mode but also guarantees that it is well accepted and widely available across dissimilar communities. © 2024 Elsevier B.V., All rights reserved.",Ensembled Machine Learning Model; Langchain; Large Language Model; Unified Medical Identification; Adversarial Machine Learning; Federated Learning; Prediction Models; Unified Modeling Language; Central Source; Chronic Disease; Ensembled Machine Learning Model; Langchain; Language Model; Large Language Model; Machine Learning Models; Medical Record; Proactive Management; Unified Medical Identification; Electronic Health Record,Scopus
10.57017/jaes.v20.1(87).02,Understanding the Market Trends: A Hybrid Approach to Stock Price Prediction Using RNNs and Transformer-Based Sentiment Analysis,10.57017/jaes.v20.1(87).02,"Stock price prediction is a critical yet challenging task in financial markets due to the complexity and volatility of asset movements. This paper presents a hybrid approach that combines Recurrent Neural Networks (RNN), particularly Long Short-Term Memory (LSTM) models, for time-series prediction with Transformer-based text analysis to capture sentiment from financial news. The study focuses on predicting Apple Inc.'s (AAPL) stock price, using three years of historical data alongside news sentiment analysis. The LSTM model captures temporal dependencies in the stock prices, while the Transformer model extracts relevant features from unstructured textual data, offering insights into market sentiment and external events. The results demonstrate that integrating sentiment data with stock price predictions significantly improves model accuracy, as reflected by a reduction in mean squared error (MSE) compared to models based solely on price data. This hybrid model offers a more holistic approach to financial forecasting, combining quantitative and qualitative data for enhanced prediction. The paper contributes to the field of machine learning in finance by highlighting the benefits of hybrid modelling approaches, and it opens avenues for future research on broader applications in other asset classes and more diverse data sources. © 2025 Elsevier B.V., All rights reserved.",Financial Forecasting; Lstm; Rnn; Sentiment Analysis; Stock Price Prediction; Transformers,Scopus
10.1016/j.ejor.2025.04.032,Unleashing the power of text for credit default prediction: Comparing human-written and generative AI-refined texts,10.1016/j.ejor.2025.04.032,"This study explores the integration of a representative large language model, ChatGPT, into lending decision-making with a focus on credit default prediction. Specifically, we use ChatGPT to analyse and interpret loan assessments written by loan officers and generate refined versions of these texts. Our comparative analysis reveals significant differences between generative artificial intelligence (AI)-refined and human-written texts in terms of text length, semantic similarity, and linguistic representations. Using deep learning techniques, we show that incorporating unstructured text data, particularly ChatGPT-refined texts, alongside conventional structured data significantly enhances credit default predictions. Furthermore, we demonstrate how the contents of both human-written and ChatGPT-refined assessments contribute to the models’ prediction and show that the effect of essential words is highly context-dependent. Moreover, we find that ChatGPT's analysis of borrower delinquency contributes the most to improving predictive accuracy. We also evaluate the business impact of the models based on human-written and ChatGPT-refined texts, and find that, in most cases, the latter yields higher profitability than the former. This study provides valuable insights into the transformative potential of generative AI in financial services. © 2025 Elsevier B.V., All rights reserved.",Credit Risk; Generative Ai; Large Language Model; Or In Banking; Text Mining; Decision Making; Comparative Analyzes; Credit Risks; Decisions Makings; Generative Artificial Intelligence; Language Model; Large Language Model; Or In Banking; Power; Text-mining; Written Texts; Deep Learning,Scopus
10.2196/70535,Unveiling the Potential of Large Language Models in Transforming Chronic Disease Management: Mixed Methods Systematic Review,10.2196/70535,"Background: Chronic diseases are a major global health burden, accounting for nearly three-quarters of the deaths worldwide. Large language models (LLMs) are advanced artificial intelligence systems with transformative potential to optimize chronic disease management; however, robust evidence is lacking. Objective: This review aims to synthesize evidence on the feasibility, opportunities, and challenges of LLMs across the disease management spectrum, from prevention to screening, diagnosis, treatment, and long-term care. Methods: Following the PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analysis) guidelines, 11 databases (Cochrane Central Register of Controlled Trials, CINAHL, Embase, IEEE Xplore, MEDLINE via Ovid, ProQuest Health & Medicine Collection, ScienceDirect, Scopus, Web of Science Core Collection, China National Knowledge Internet, and SinoMed) were searched on April 17, 2024. Intervention and simulation studies that examined LLMs in the management of chronic diseases were included. The methodological quality of the included studies was evaluated using a rating rubric designed for simulation-based research and the risk of bias in nonrandomized studies of interventions tool for quasi-experimental studies. Narrative analysis with descriptive figures was used to synthesize the study findings. Random-effects meta-analyses were conducted to assess the pooled effect estimates of the feasibility of LLMs in chronic disease management. Results: A total of 20 studies examined general-purpose (n=17) and retrieval-augmented generation-enhanced LLMs (n=3) for the management of chronic diseases, including cancer, cardiovascular diseases, and metabolic disorders. LLMs demonstrated feasibility across the chronic disease management spectrum by generating relevant, comprehensible, and accurate health recommendations (pooled accurate rate 71%, 95% CI 0.59-0.83; I2=88.32%) with retrieval-augmented generation-enhanced LLMs having higher accuracy rates compared to general-purpose LLMs (odds ratio 2.89, 95% CI 1.83-4.58; I2=54.45%). LLMs facilitated equitable information access; increased patient awareness regarding ailments, preventive measures, and treatment options; and promoted self-management behaviors in lifestyle modification and symptom coping. Additionally, LLMs facilitate compassionate emotional support, social connections, and health care resources to improve the health outcomes of chronic diseases. However, LLMs face challenges in addressing privacy, language, and cultural issues; undertaking advanced tasks, including diagnosis, medication, and comorbidity management; and generating personalized regimens with real-time adjustments and multiple modalities. Conclusions: LLMs have demonstrated the potential to transform chronic disease management at the individual, social, and health care levels; however, their direct application in clinical settings is still in its infancy. A multifaceted approach that incorporates robust data security, domain-specific model fine-tuning, multimodal data integration, and wearables is crucial for the evolution of LLMs into invaluable adjuncts for health care professionals to transform chronic disease management. © 2025 Elsevier B.V., All rights reserved.",Artificial Intelligence; Chronic Disease; Health Management; Large Language Model; Systematic Review; Adult; Article; Asthma; Awareness; Cardiovascular Disease; Chatgpt; Chronic Disease; Chronic Disease Management; Cohort Analysis; Colorectal Cancer; Comorbidity; Coping; Cultural Factor; Diabetes Mellitus; Drug Dependence; Emotional Support; Feasibility Study; Glaucoma; Health Care Utilization; Health Outcome; Health Promotion; Hearing Impairment; Human; Kidney Failure; Knowledge; Language; Large Language Model; Lifestyle Modification; Liver Cirrhosis; Long Term Care; Male; Malignant Neoplasm; Mental Disease; Meta Analysis; Metabolic Disorder; Multidisciplinary Team; Musculoskeletal Disease; Non Communicable Disease; Osteoarthritis; Personalized Medicine; Physical Activity; Privacy; Prostate Hypertrophy; Quasi Experimental Study; Respiratory Tract Disease; Retrieval Augmented Generation; Self Care; Social Connectedness; Social Support; Systematic Review; Artificial Intelligence; Disease Management; Therapy; Artificial Intelligence; Chronic Disease; Disease Management; Humans; Language; Large Language Models,Scopus
10.1007/s11156-025-01437-x,Using Generative AI to predict the weather impact on future stock returns,10.1007/s11156-025-01437-x,"This study explores the use of Generative AI, specifically OpenAI’s ChatGPT, for forecasting the impacts of severe weather events on stock returns. Employing prompts that assess textual weather descriptions, ChatGPT, a powerful generative AI large language model (LLM), provides predictions incorporated into econometric models. Results show that when ChatGPT forecasts negative stock impacts from storms, larger, more profitable firms with lower leverage and higher liquidity experience lower subsequent returns, suggesting investor underreaction to weather risk. ChatGPT’s predictive abilities are stronger during favorable economic conditions like uptrends, low volatility, and robust employment growth, implying investor underreaction amid bullish sentiment. © 2025 Elsevier B.V., All rights reserved.",Chatgpt; Generative Ai; Large Language Model (llm); Stock Returns; Weather Risk,Scopus
10.3905/jpm.2025.1.710,Using Large Language Models to Estimate Novel Risk: Impact on Volatility,10.3905/jpm.2025.1.710,"This article presents an integrated framework to estimate hard to measure (novel) financial risk for volatility forecasting. Recognizing the limitations of traditional models—which often overlook emerging “novel risks”—the article leverages advanced large language models (LLMs) to extract and quantify key risk factors, including ESG, geopolitical, and supply chain disruption risks, from corporate disclosures. These LLM-derived risk scores are then combined with conventional financial indicators such as leverage, beta, and short interest, and incorporated into a long short-term memory (LSTM) neural network to predict firm-specific (idiosyncratic) volatility. Empirical analysis, conducted on over 18,000 regulatory filings spanning 2015 to 2024, demonstrates that the integrated model significantly improves volatility forecasting, as evidenced by enhanced R2 values and reduced mean squared error. Additionally, feature importance analyses confirm the pivotal role of novel risk measures. Overall, the findings underscore the benefits of merging unstructured and hard to quantify data with quantitative models to offer a more nuanced approach to estimation of novel financial risk. © 2025 Elsevier B.V., All rights reserved.",,Scopus
10.1177/10534512241235896,Utilizing Text-Generative AI for Creating Oral Reading Fluency Probes,10.1177/10534512241235896,"Oral reading fluency probes are essential for reading assessment, intervention, and progress monitoring. Due to the limited options for choosing oral reading fluency probes, it is important to utilize all available resources such as generative artificial intelligence (AI) like ChatGPT to create oral reading fluency probes. The purpose of this article is to describe how to use AI through ChatGPT to create customizable reading passages comparable with that of oral reading fluency probes. Using readability estimates, the ChatGPT-generated passages can be tailored to suit for specific grade levels, similar to how current publishers design oral reading fluency probes for the market. The implication of ChatGPT-generated passages is that researchers and practitioners alike could use ChatGPT to be able to create a seemingly unlimited amount of reading passages tailored to the skill level and interests of the learner for intervention material and potentially assessment material, while reducing cost and time investment. © 2024 Elsevier B.V., All rights reserved.",Academic Intervention; Artificial Intelligence; Oral Reading Fluency,Scopus
10.1109/ACCESS.2025.3532995,Zero-Shot Classification of Art with Large Language Models,10.1109/access.2025.3532995,"Art has become an important new investment vehicle. Thus, interest is growing in art price prediction as a tool for assessing the returns and risks of art investments. Both traditional statistical methods and machine learning methods have been used to predict art prices. However, both methods incur substantial human costs for data preprocessing for the construction of prediction models, necessitating a reduction in the workload. In this study, we propose the zero-shot classification method to perform automatic annotation in data processing for art price prediction by leveraging large language models (LLMs). The proposed method can perform annotation without new training data. Thus, it minimizes human costs. Our experiments demonstrated that the 4-bit quantized Llama-3 70B model, which can run on a local server, achieved the most accurate (over 0.9) automatic annotation of different art forms using LLMs, performing slightly better than the GPT-4o model from OpenAI. These results are practical for data preprocessing and comparable with the results of previous machine learning methods. © 2025 Elsevier B.V., All rights reserved.",Art; Auction Price; Chatgpt; Classification; Data Preprocessing; Gemma; Large Language Model; Llama; Llm; Machine Learning; Zero-shot Learning; Adversarial Machine Learning; Contrastive Learning; Network Security; Prediction Models; Risk Assessment; Art; Auction Price; Chatgpt; Data Preprocessing; Gemma; Language Model; Large Language Model; Llama; Machine-learning; Zero-shot Learning,Scopus
10.1109/ITC-CSCC66376.2025.11137629,Zero-Shot Time Series Forecasting of the Online Gig Economy Using the CHRONOS Pretrained Language Model,10.1109/itc-cscc66376.2025.11137629,"This study introduces a zero-shot forecasting method for the online gig economy applying the CHRONOS pre-trained language model to the online labor index (OLI) data. The paper concentrates on forecasting demand for six freelance job categories using historical weekly data from January 2017 to August 2024, without retraining the model specifically for these tasks. Although the one-step-ahead (weekly) forecasts demonstrate highly satisfactory accuracy, with a mean absolute scaled error below 1 for most freelance occupations, there is a decline in performance for multi-step forecasts (at least one month ahead), shown by increased errors. This result underscores the importance of pre-trained models in learning semantic information that can be easily transferred to other applications, especially in zero-shot setups. This paper makes a substantial contribution to the field by using OLI to enhance our understanding of predicting the demand for freelance jobs based on structural patterns in economic indicators. This is crucial given the volatility and distinct behaviors of the online gig economy. Furthermore, it illustrates the potential of using pre-trained transformer-based language models in time series forecasting, even with limited historical data, proving that advanced pre-trained models can still produce valuable predictions. © 2025 Elsevier B.V., All rights reserved.",Online Gig Economy; Online Labor Index; Pre-trained Language Model; Zero-shot Time Series Forecasting; Employment; Forecasting; Time Series; Forecasting Demand; Forecasting Methods; Language Model; Multisteps; Online Gig Economy; Online Labor Index; Performance; Pre-trained Language Model; Time Series Forecasting; Zero-shot Time Series Forecasting; Semantics,Scopus
10.18653/v1/2021.semeval-1.87,cs60075 team2 at SemEval-2021 Task 1: Lexical Complexity Prediction using Transformer-based Language Models pre-trained on various text corpora,10.18653/v1/2021.semeval-1.87,"This paper describes the performance of the team cs60075 team2 at SemEval 2021 Task 1 - Lexical Complexity Prediction. The main contribution of this paper is to fine-tune transformer-based language models pre-trained on several text corpora, some being general (E.g., Wikipedia, BooksCorpus), some being the corpora from which the CompLex Dataset was extracted, and others being from other specific domains such as Finance, Law, etc. We perform ablation studies on selecting the transformer models and how their individual complexity scores are aggregated to get the resulting complexity scores. Our method1 achieves a best Pearson Correlation of 0.784 in sub-task 1 (single word) and 0.836 in sub-task 2 (multiple word expressions). © 2025 Elsevier B.V., All rights reserved.",Correlation Methods; Semantics; Complex Datasets; Language Model; Pearson Correlation; Performance; Single Words; Subtask; Text Corpora; Transformer Modeling; Wikipedia; Computational Linguistics,Scopus
